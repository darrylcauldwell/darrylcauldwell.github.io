---
layout: post
title:  "Introduction To Kubernetes Cluster Networking with Antrea"
date:   2020-01-14 20:22:56 +0100
tags:
    - Docker
    - DevOps
    - Software Defined Networking
permalink: antrea/
---

<img src="/images/antrea-logo.png">

## What is Antrea

Antrea is a Kubernetes networking solution intended to be Kubernetes native. It operates at Layer3/4 to provide networking and security services for a Kubernetes cluster, leveraging Open vSwitch as the networking data plane.

It it an open source project hosted on GitHub [here](https://github.com/vmware-tanzu/antrea).

The current features of Antrea are,

* IPv4 overlay network for a Kubernetes cluster
* Kubernetes Network Policies implementation
* Octant UI plugin for monitoring Antrea components

Antrea implements the overlay network datapath using Open vSwitch (OVS). Other Container Network Interface (CNI) plugins such as Flannel and Calico use iptables, iptables can generate a large CPU load when number of rules are scaled. In a Kubernetes environment that uses complex Network Policies and a large number of Services, it is expected that iptables-based CNI will not perform as expected. By using OVS Antrea aims to avoid the performance problems which can be caused by iptables. Using OVS also introduces other standard network management capabilities such NetFlow, sFlow, IP Flow Information Export (IPFIX) and Remote SPAN (RSPAN).

The overlay encapsulation uses Virtual Extensible LAN (VXLAN) by default, although Generic Network Virtualization Encapsulation (GENEVE), Generic Routing Encapsulation (GRE) or Stateless Transport Tunneling (STT) encapsulation can be configured.

## Antrea Architecture

Antrea is composed of several components. The Antrea Conroller monitor the addition and deletion of Kubernetes objects using the API. Each Kubernetes Node has OVS installed, the Antrea Agent runs on each Kubernetes Node and configures OVS. The Antrea CNI passes CNI commands from Kubelet to Antrea Agent. 

<img src="/images/antrea-architecture.png">

Antrea presently has capability for maintaining the Overlay network. It is worth noting that in the present release the kube-proxy is the standard Kubernetes implementation. The standard kube-proxy  uses iptables for Service load balancing. It is planned in a future release this will be replaced for one which uses OVS for load balancing.

Antrea provides a plug-in for Octant, and you can easily check the status of Antrea from Octant.m Octant is a dashboard that visualizes the status of Kubernetes Cluster.

## Deploy Basic Kubernetes Cluster

Create three Ubuntu 19.10 VMs with 2x CPU, 4GB RAM and 50GB vHDD, use Netplan to configure NIC to static IP addressing like.

```
cat /etc/netplan/01-netcfg.yaml 

network:
  version: 2
  renderer: networkd
  ethernets:
    ens160:
      addresses: 
      - 192.168.1.27/24
      gateway4: 192.168.1.254
      nameservers:
          search:
          - darrylcauldwell.com
          addresses: 
          - 192.168.1.10
```

Antrea requires Kubernetes 1.16 or later. Install Docker, Open vSwitch and Kubernetes by running the following.

```
sudo apt update -y
sudo apt upgrade -y
sudo apt install docker.io python apt-transport-https openvswitch-switch -y
sudo gpasswd -a $USER docker
sudo systemctl start docker
sudo systemctl enable docker
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
sudo apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"
sudo apt-get update
sudo swapoff -a 
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
sudo apt-get install -y kubelet=1.16.4-00 kubeadm=1.16.4-00 kubectl=1.16.4-00
```

To form the overlay network Antrea requires an IP address block to allocate IP addresses (--pod-network-cidr). We can configure the IP address block and bootstrap the cluster by running the following:

```
sudo kubeadm init --pod-network-cidr=172.16.0.0/16
```

In order to easily run kubectl as a user we need to copy the cluster configuration to the user profile by running the following.

```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

If we now get the cluster token from the master we can use this to add the two worker Nodes to the cluster.

```
sudo kubeadm join 192.168.1.27:6443 --token 4bp2f0.ppfjk7mfee89j2kd \
>     --discovery-token-ca-cert-hash sha256:5e79610f28840c15be46895340c4a5535b9a0d003741ed657961891a05987ccd 
```

All the Antrea components are containerized and can be installed using the Kubernetes deployment manifest. We can apply the default Antrea manifest supplied in GitHub repository which will deploy all necessary components.

```
kubectl apply -f https://raw.githubusercontent.com/vmware-tanzu/antrea/master/build/yamls/antrea.yml
```

The Antrea components are deployed to the kube-system Namespace. The components include Pods, Daemonset and ConfigMap

```
kubectl config set-context --current --namespace kube-system
kubectl get pods
kubectl get daemonset
kubectl get configmap
```

The ConfigMap defines antrea-agent.conf this is used by Daemonset on each Node to configure Open vSwitch for use with CNI we can see some key information like. 

```
kubectl describe configmap antrea-config-tm7bht9mg6

Integration Bridge name (default: br-int)
DataPath type (default: system)
Interface name to communicate with host (default: gw0)
Tunnel (Encapsulation) type (default: vxlan)
MTU value (default: 1450)
Service CIDR (default 10.96.0.0/12)
```

With the Antrea CNI now deployed successfully we can now connect to cluster and check the state of the nodes, do this by running the following on the Master node.

```
kubectl get nodes
```

##Â Exploring The Open vSwitch Overlay Network

With Antrea in place we can look at exploring the overlay network it has put in place.  We create a Kubernetes Namespace and deploy an simple stateless application like [guestbook](https://kubernetes.io/docs/tutorials/stateless-application/guestbook/).

```
kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
kubectl config set-context --current --namespace=development
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-deployment.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-service.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-deployment.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-service.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-deployment.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-service.yaml
```

When running we can view the Pods IP addressing.

```
kubectl get pods --output wide

NAME                            READY   STATUS    RESTARTS   AGE     IP           NODE              NOMINATED NODE   READINESS GATES
frontend-6cb7f8bd65-25qv4       1/1     Running   0          5m47s   172.16.2.4   antrea-worker-2   <none>           <none>
frontend-6cb7f8bd65-mn5jk       1/1     Running   0          5m47s   172.16.1.5   antrea-worker-1   <none>           <none>
frontend-6cb7f8bd65-nr9zk       1/1     Running   0          5m47s   172.16.2.5   antrea-worker-2   <none>           <none>
redis-master-7db7f6579f-4wdnj   1/1     Running   0          5m51s   172.16.2.2   antrea-worker-2   <none>           <none>
redis-slave-7664787fbc-5xrvj    1/1     Running   0          5m49s   172.16.1.4   antrea-worker-1   <none>           <none>
redis-slave-7664787fbc-nj6m5    1/1     Running   0          5m49s   172.16.2.3   antrea-worker-2   <none>           <none>
```

We can see that IPs from 172.16.1.0/24 are issued to Pods running on Node antrea-worker-1 and 172.16.2.0/24 are issued to Pods running on Node antrea-worker-2. To facilitate communications between Pods the antrea-agent configures flows on Open vSwitch on each Node. If we connect to a Antrea Agent container we can see that an OVS bridge is created called br-int and the bridge has a vxlan tunnel port called tun0 and a gateway port called gw0. The internal port gw0 is allocated the role of gateway of the Node's subnet and is allocated the first IP address in subnet.  

```
kubectl config set-context --current --namespace=kube-system
kubectl get pods --selector=component=antrea-agent --output wide

NAME                 READY   STATUS    RESTARTS   AGE   IP             NODE              NOMINATED NODE   READINESS GATES
antrea-agent-czksb   2/2     Running   0          14m   192.168.1.28   antrea-worker-1   <none>           <none>
antrea-agent-gwkmr   2/2     Running   0          14m   192.168.1.27   antrea-master     <none>           <none>
antrea-agent-x9xjk   2/2     Running   0          14m   192.168.1.29   antrea-worker-2   <none>           <none>

kubectl exec -it antrea-agent-czksb -c antrea-ovs bash

ovs-vsctl list-br
ovs-vsctl show

3cf39eec-f64c-49ed-ad86-48f203b215a4
    Bridge br-int
        Port "tun0"
            Interface "tun0"
                type: vxlan
                options: {key=flow, remote_ip=flow}
        Port "coredns--886217"
            Interface "coredns--886217"
        Port "redis-sl-b32512"
            Interface "redis-sl-b32512"
        Port "coredns--3d5851"
            Interface "coredns--3d5851"
        Port "gw0"
            Interface "gw0"
                type: internal
        Port "frontend-1ee3a9"
            Interface "frontend-1ee3a9"
    ovs_version: "2.11.1"

ovs-ofctl dump-flows br-int | grep 172.16.2

 cookie=0xf70e020000000000, duration=820.733s, table=20, n_packets=1, n_bytes=42, idle_age=585, priority=200,arp,arp_tpa=172.16.2.1,arp_op=1 actions=move:NXM_OF_ETH_SRC[]->NXM_OF_ETH_DST[],mod_dl_src:aa:bb:cc:dd:ee:ff,load:0x2->NXM_OF_ARP_OP[],move:NXM_NX_ARP_SHA[]->NXM_NX_ARP_THA[],load:0xaabbccddeeff->NXM_NX_ARP_SHA[],move:NXM_OF_ARP_SPA[]->NXM_OF_ARP_TPA[],load:0xac100201->NXM_OF_ARP_SPA[],IN_PORT
 cookie=0xf70e020000000000, duration=820.733s, table=70, n_packets=649, n_bytes=63996, idle_age=0, priority=200,ip,nw_dst=172.16.2.0/24 actions=dec_ttl,mod_dl_src:06:d1:bb:d3:bc:fa,mod_dl_dst:aa:bb:cc:dd:ee:ff,load:0x1->NXM_NX_REG1[],load:0x1->NXM_NX_REG0[16],load:0xc0a8011d->NXM_NX_TUN_IPV4_DST[],resubmit(,105)
```

We can look to test the IP routing and connectivity between Pods on same Node and also between Nodes,  

```
kubectl config set-context --current --namespace=development
kubectl get pods --output wide | grep frontend

frontend-6cb7f8bd65-25qv4       1/1     Running   0          12m   172.16.2.4   antrea-worker-2   <none>           <none>
frontend-6cb7f8bd65-mn5jk       1/1     Running   0          12m   172.16.1.5   antrea-worker-1   <none>           <none>
frontend-6cb7f8bd65-nr9zk       1/1     Running   0          12m   172.16.2.5   antrea-worker-2   <none>           <none>

kubectl exec -it frontend-6cb7f8bd65-25qv4 -- ping -c 1 172.16.2.5
kubectl exec -it frontend-6cb7f8bd65-25qv4 -- ping -c 1 172.16.1.5
```

## Network Policy

Antrea implements NetworkPolicy using OVS Flows. Flows are organized in tables, and they are applied on each node by the Antrea agent the ingress rule table is flow table 90.

Before applying a network policy, flow table 90 on a given node is almost empty:

$ kubectl -n kube-system exec -it antrea-agent-kx9s7 ovs-ofctl dump-flows br-int | grep table=90
cookie=0x0, duration=261738.430s, table=90, n_packets=36327088, n_bytes=597418196451, priority=210,ct_state=-new+est,ip actions=resubmit(,110)
cookie=0x0, duration=261738.793s, table=90, n_packets=89, n_bytes=7527, priority=80,ip actions=resubmit(,100)

After applying a network policy with an ingress rule, we can see new entries in the flow table. Because these are ingress rules, Antrea installs the flows on the nodes that are hosting the pods selected by the network policy.

$ kubectl -n kube-system exec -it antrea-agent-kx9s7 ovs-ofctl dump-flows br-int | grep table=90
cookie=0x0, duration=262007.710s, table=90, n_packets=36327088, n_bytes=597418196451, priority=210,ct_state=-new+est,ip actions=resubmit(,110)
cookie=0x0, duration=162.476s, table=90, n_packets=0, n_bytes=0, priority=200,ip,nw_src=192.168.2.1 actions=conjunction(2,1/2)
cookie=0x0, duration=0.277s, table=90, n_packets=0, n_bytes=0, priority=200,ip,nw_src=192.168.2.31 actions=conjunction(2,1/2)
cookie=0x0, duration=162.432s, table=90, n_packets=0, n_bytes=0, priority=200,ip,reg1=0x1e actions=conjunction(2,2/2)
cookie=0x0, duration=162.507s, table=90, n_packets=0, n_bytes=0, priority=190,conj_id=2,ip actions=resubmit(,110)
cookie=0x0, duration=262008.073s, table=90, n_packets=89, n_bytes=7527, priority=80,ip actions=resubmit(,100)

Antrea installs flows that have conjunction actions. In the output above, the third line is an action that says âAllow Ingress when the source IP address is 192.168.2.31â. This is the first half of the conjunction, as indicated by conjunction(2,1/2). The second half of the conjunction is on the fourth line, which matches the flow with the targeted pod's MAC address using registers (reg1=0x1e). This is all fairly new to me, so it is pretty hand-wavy at the moment. I plan to dig into this more in a follow-up post.

## Service Resources
As well as Pod deployments the guestbook application installation also creates Service resources. A Service is an abstraction which defines a logical set of Pods. Our application runs three frontend pods and two Redis slaves the Service provides virtual IP. We can see the ClusterIP are issued by default from 10.96.0.0/12. 

```
kubectl get services

NAME           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
frontend       NodePort    10.108.123.79   <none>        80:32120/TCP   14m
redis-master   ClusterIP   10.101.24.211   <none>        6379/TCP       14m
redis-slave    ClusterIP   10.97.137.174   <none>        6379/TCP       14m

kubectl cluster-info dump | grep -m 1 service-cluster-ip-range

    "--service-cluster-ip-range=10.96.0.0/12",
```

The [kube-proxy](https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/) is responsible for implementing the virtual IP for Services.  If we look in more detail at the Service resource we can see the three internal IP addresses behind it.

```
kubectl describe service frontend

Name:                     frontend
Namespace:                development
Labels:                   app=guestbook
                          tier=frontend
Annotations:              kubectl.kubernetes.io/last-applied-configuration:
                            {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app":"guestbook","tier":"frontend"},"name":"frontend","namespa...
Selector:                 app=guestbook,tier=frontend
Type:                     NodePort
IP:                       10.108.123.79
Port:                     <unset>  80/TCP
TargetPort:               80/TCP
NodePort:                 <unset>  32120/TCP
Endpoints:                172.16.1.5:80,172.16.2.4:80,172.16.2.5:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   <none>
```