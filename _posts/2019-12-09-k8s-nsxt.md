---
layout: post
title:  "Introduction To Kubernetes Cluster Networking with NSX-T"
date:   2019-12-24 20:20:56 +0100
tags:
    - Docker
    - DevOps
    - Software Defined Networking
permalink: k8s-nsxt/
---
When developing a cloud native application using Docker containers she soon needs to understand how Docker containers communicate. In previous [post](http://darrylcauldwell.com/basic-container-networking/) I looked at how Docker containers communicate on a single host. When the developer wants to scaleout capacity of the hosting across multiple hosts or increase abailability she might look at deploying this on a Kubernetes cluster. The move from single Docker host to multiple hosts managed as Kubernetes Cluster introduces changes to the container networking model. The four distinct networking problems a Kubernetes Cluster needs to address:

* Highly-coupled container-to-container communications
* Pod-to-Pod communications
* Pod-to-Service communications
* External-to-Service communications: this is covered by services

## What Is A Kubernetes Pod

A Docker container is great for deploying a single atomic unit of software. This model can become a bit cumbersome when you want to run multiple pieces of software together. You often see this when developers create Docker images that use [supervisord](https://docs.docker.com/config/containers/multi-service_container/) as an entrypoint to start and manage multiple processes. Many have found that it is instead more useful to deploy those applications in groups of containers that are partially isolated and partially share an environment. It is possible to configure Docker to control the level of sharing between groups of containers by creating a parent container and manage the lifetime of those containers, however this is administratively complex. Kubernetes provides an abstraction called Pods for just this use case.

A Kubernetes Pod implements a 'pause' container as the managing parent container, the Pod also contains one or more of  application containers. The 'pause' container serves as the basis of Linux namespace sharing in the Pod the other containers are starterd within that namespace. Sharing a namespace includes sharing network stack and other resources such as volumes. Sharing a network namespace means containers within a Pod share an IP address and all containers within a Pod can all reach each other’s ports on localhost.

<img src="/images/k8s-nsxt-pod.png">

## What Is A Kubernetes Service

Typically a Kubernetes Deployment is used to When a Kubernetes Pod is deployed Pods. A Deployment describes a desired state it can create and destroy Pods dynamically. While each Pod gets its own IP address, the set of Pods running can change. A Kubernetes Service (sometimes called a micro-service) is an abstraction which defines a logical set of Pods. Kubernetes natively provides Service discovery to Pods, it also offers a method of exposing Services via network port or load balancer to external systems.

## What Is Container Networking Interface (CNI)

Container-centric infrastructure needs a network and this network must be dynamic. The Container Networking Interface is container networking [specification](https://github.com/containernetworking/cni/blob/master/SPEC.md) it’s goal is to create a generic plugin-based dynamic networking solution for containers.

## What Is NSX-T Container Plugin (NCP)

The NSX-T Container Plug-in (NCP) provides an integration between NSX-T and container orchestrators such as Kubernetes. The main component of NCP runs in a container and communicates with NSX Manager and with the Kubernetes control plane. NCP monitors changes to containers and other resources and manages networking resources such as logical ports, switches, routers, and security groups for the containers bycalling the NSX API.

NSX-T bring advanced features which can enrich Kubernetes cluster networking [including](https://blogs.vmware.com/networkvirtualization/2017/03/kubecon-2017.html/):

* Fine-grained traffic control and monitoring
* Fine-grained security policy (firewall rules)
* Automated creation of network topology
* Integration with enterprise networking

## NSX-T Container Plug-in Release 2.5

I looked at the [NCP initially for integration with OpenShift](http://darrylcauldwell.com/nsx-openshift/) in mid-2018. I am revisiting this now to refresh my understanding and explore some of the new features introduced with the [2.5 release](https://blogs.vmware.com/networkvirtualization/2019/09/nsx-t-2-5-what-is-new-for-kubernetes.html/) including:

### NSX-T Policy API Object Support

Prior to the 2.5 release all NSX objects which the NCP interacted with had to be created via the Advanced Networking & Security tab in the UI or the old imperative APIs. The imperative API was harder than it could have been to control so with NSX 2.4 VMware introduced a new intent-based Policy API and corresponding Simplified UI. The NCP now supports either the imperative or the intent-based API,  to use the intent-based API a new parameter in the NCP configmap (ncp.ini) policy_nsxapi needs to be set to True.

### Simplified Installation With NSX NCP Bootstrap

Another change I am interested in exploring is the simplified installation. In the past, an admin had to login to every k8s node and perform multiple steps to bootstrap it. She had to install the NSX CNI Plug-in and OpenVSwitch, to create OVS bridge and to add one vNic to the bridge. The 2.5 release introduces a second DaemonSet nsx-ncp-bootstrap Pod this now handles the deployment and lifecycle management of these components and we don’t need to login to every node. This should make it easier to scale out a cluster with additional nodes.

## Lab Environment

To explore Kubernetes networking and the NCP I am using my homelab. My homelab has a very simple physical network namely a single subnet (192.168.1.0/24) which routes northbound to internet. Connected to the physical network are three Intel NUCs each has two 1G NIC an onboard and an additional USB3. They run vSphere 6.7 Update 3 and have the onboard NIC configure as a vSphere Standard Switch hosting Management and vSAN VMkernels and the USB3 NIC is unused. They are formed into a VSAN cluster, have an Active Directory VM which also acts as DNS server and NTP source and a vCenter appliance running in the cluster.

For this research I installed NSX-T 2.5.1 and configured all hosts as Transport Nodes in a Transport Zone named overlayTransport. A single medium sized Edge is also deployed with a Tier-0 configured with High-Availability mode Active-Passive. [The the router must be created in active-standby mode is specified in the NCP install guide](The router must be created in active-standby mode](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/com.vmware.nsxt.ncp_kubernetes.doc/GUID-49D7630F-AEB1-4443-BDE0-8788E4415647.html). A medium sized Edge is required to enable deployment of Kubernetes Service resources of type LoadBalancer. The Edge is also configured as a Transport Nodes and a member of Transport Zone named overlayTransport it is also added a member of Transport Zone named vlanTransportZone. The Edge has configured a Tier-0 Logical Router this will be used to connect the Kubernetes nodes to external networks and perform Source NAT. The Tier-0 router has a interface configured on physical network IP 192.168.1.17 and the physical router has a static route configured for 172.16.0.0/16 which directs to the Tier-0.

## Lab Kubernetes IP Address Space

A Kubernetes Cluster requires all Pods on a node to communicate with all Pods on all nodes in the cluster without NAT these are refered to as CluterIP. To support this a range of IP addresses must be defined to be issued to Pods and Services within a cluster. Even though the range is used for both Pods and Services, it is called the Pod address range. The last /20 of the Pod address range is used for Services. A /20 range has 212 = 4096 addresses. So 4096 addresses are used for Services, and the rest of the range is used for Pods.

The address range I will be using to issue ClusterIP for this lab cluster is 10.0.0.0/16.

As well as internal communicatins using ClusterIP some parts of your application may need to be exposed as a Service to be accessible on an externally routable IP address. There are two methods for exposing Service onto an externally routable IP address, NodePort and LoadBalancer. Source NAT is used for translating private ClusterIP address to a public routable address.

The external address range I will be using to issue ExternalIP for this lab cluster is 172.16.0.0/16. This is routable from physical 192.168.1.0/24 network.

## NCP - NSX Object Identification

There can be many objects deployed within NSX-T, the NCP needs to understand which of these objects to interact with. The NSX objects which the NCP needs to interact with include:

* Overlay transport zone
* Tier-0 logical router
* Logical switch to connect the node VMs
* IP Block for internal ClusterIP addressing
* IP Pool for ExternalIP addressing

The NCP configuration is stored in ncp.ini file. It is possible to put the UUID of NSX objects in the ncp.ini but this an administrative pain. The mapping of which NSX objects for NCP to interact with is better achieved by applying tags to the appropriate NSX objects.

An NSX-T instance can support multiple Kubernetes clusters to ensure correct object mapping a cluster name is used. The cluster name is specified in NCP configuration and the appopriate NSX objects must have tag of same name applied.

For this lab environment I am configuring with cluster name 'pandora'.

### NSX Tier-0 and Overlay Transport Zone

The Tier-0 Logical Router and Overlay Transport Zone reqiore a single tag applied like this:

| Tag            | Scope         |
| -------------- |:-------------:|
| pandora        | ncp/cluster   |

<img src="/images/k8s-nsxt-tier0.png">

<img src="/images/k8s-nsxt-overlay.png">

### NSX IP Block for internal ClusterIP

NSX-T has an inbuilt capability for IP Management, in which we can allocate blocks of IP Addresses and create IP Pools.

The NCP requires an IP Block for issuing internal ClusterIP. Create the 10.0.0.0/16 IP address block named 'k8s-1-internal' with tags applied like this:

| Tag            | Scope         |
| -------------- |:-------------:|
| pandora        | ncp/no_snat   |
| pandora        | ncp/cluster   |

<img src="/images/k8s-nsxt-internal-block.png">

## NSX IP Pool for ExternalIP

The NCP requires an IP Pool for issuing internal ExternalIP. First create an IP Block named k8s-1-external with CIDR 172.16.0.0/16. This IP Block isn't accessed directly by NCP so does not need any tags.

<img src="/images/k8s-nsxt-external-block.png">

Once the IP Block is in place create an IP Pool named 'k8s-1-loadbalancer' which has a subnet issued from IP Block 'k8s-1-external' which is sized at 128 the IP Pool.  The External IP Pool can be shared but should at least have tag applied like this:

| Tag            | Scope         |
| -------------- |:-------------:|
| true           | ncp/external  |

<img src="/images/k8s-nsxt-external-pool.png">

## Node Logical Switch

Network connectivity to the containers running in Kubernetes is provided by a NSX-T logical switch segment which is often referred to as the node logical switch. For this create a new segment called 'node-logical-switch' within the 'overlayTransportZone' connected to no gateway.

## Kubernetes Master VM

Create a Ubuntu 18.04 VM with 2x CPU, 4GB RAM and 50GB vHDD named 'k8s-master'. The VM should have two vNIC one which is used to communicate with NSX API and which will host Kubernetes API. The second connected to the node logical switch which will have the Open vSwitch (OVS) bridge configured to give connectivity to the Pods. In my lab first connected to 'VM Network' enumerates as ens160 and the scond connected to 'node-logical-switch' enumerates as ens192.

```
cat /etc/netplan/01-netcfg.yaml 

network:
  version: 2
  renderer: networkd
  ethernets:
    ens160:
      addresses: 
      - 192.168.1.27/24
      gateway4: 192.168.1.254
      nameservers:
          search:
          - darrylcauldwell.com
          addresses: 
          - 192.168.1.10
    ens192: {}
```

In order the NCP know which vNIC to configure the VM vNIC connected 'node-logical-switch' creates a segment port object in NSX-T this port must have tag applied like this:

| Tag            | Scope         |
| -------------- |:-------------:|
| k8s-master     | ncp/node_name |
| pandora        | ncp/cluster   |

<img src="/images/k8s-nsxt-segment-port-tags.png">

## Install and Configure Kubernetes

Docker and Kubernetes require installation do this by running the following.

```
sudo apt update
sudo apt upgrade -y
sudo apt install docker.io python apt-transport-https -y
sudo gpasswd -a $USER docker
sudo systemctl start docker
sudo systemctl enable docker
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
sudo apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"
sudo apt-get update
sudo swapoff -a 
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
sudo apt-get install -y kubelet=1.16.4-00 kubeadm=1.16.4-00 kubectl=1.16.4-00
```

Initialize the Kubernetes cluster by running the following.

```
sudo kubeadm init
```

In order to easily run kubectl as a user we need to copy the cluster configuration to the user profile, do this by running the following.

```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

We can now connect to cluster and check the state of the nodes, do this by running the following on the Master node.

```
sudo kubectl get nodes

NAME           STATUS     ROLES    AGE     VERSION
k8s-master     NotReady   master   10m     v1.14.8
```

The status of each Node will show 'NotReady',  we can get more details of why it is in this state by running the following on the Master node.

```
sudo kubectl describe nodes k8s-master | grep Conditions A9

Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 16 Dec 2019 18:47:40 +0000   Mon, 16 Dec 2019 18:38:10 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 16 Dec 2019 18:47:40 +0000   Mon, 16 Dec 2019 18:38:10 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 16 Dec 2019 18:47:40 +0000   Mon, 16 Dec 2019 18:38:10 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            False   Mon, 16 Dec 2019 18:47:40 +0000   Mon, 16 Dec 2019 18:38:10 +0000   KubeletNotReady              runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Addresses:
  InternalIP:  192.168.1.27
  Hostname:    k8s-master
```

We can see this reason for the error is 'NetworkPluginNotReady' that the Container Networking Interface (CNI) plugin is missing. 

## Install and Configure NSX Container Plug-in (NCP)

The NSX Container Plug-in (NCP) provides integration between NSX-T and Kubernetes, it is a containerised application which manages communicates between NSX Manager and the Kubernetes control plane. The NSX Container Plug-in (NCP) application runs in Kubernetes it is supplied as .zip download. The configuration of the NCP applications is maintained in a Kubernetes manifest file.

The NCP application ships a container image file we could deploy this to a container registry but here we will just upload the image file to VM and import the image to the local docker repository. The default container image name specified in the manifest is nsx-ncp so we can apply that as tag on the image.

```
sudo docker load -i /home/ubuntu/nsx-ncp-ubuntu-2.5.1.15287458.tar
sudo docker image tag registry.local/2.5.1.15287458/nsx-ncp-ubuntu nsx-ncp
sudo docker images | grep ncp
```

Prior to the 2.5 release there were multiple  manifest files each targetting a specific area of configuration.  These are now merged into a single manifest file with multiple sections with multiple resource specifications the sections can be identified by the separator `---`. The resources are created in the order they appear in the file.

| Resource Kind            | Resource Name                        |Comments                                           |
| ------------------------ |:------------------------------------:|:-------------------------------------------------:|
| CustomResourceDefinition | nsxerrors.nsx.vmware.com             |                                                   |
| CustomResourceDefinition | nsxlocks.nsx.vmware.com              |                                                   |
| Namespace                | nsx-system                           |                                                   |
| ServiceAccount           | ncp-svc-account                      |                                                   |
| ClusterRole              | ncp-cluster-role                     |                                                   |
| ClusterRole              | ncp-patch-role                       |                                                   |
| ClusterRoleBinding       | ncp-cluster-role-binding             |                                                   |
| ClusterRoleBinding       | ncp-patch-role-binding               |                                                   |
| ServiceAccount           | nsx-node-agent-svc-account           |                                                   |
| ClusterRole              | nsx-node-agent-cluster-role          |                                                   |
| ClusterRoleBinding       | nsx-node-agent-cluster-role-binding  |                                                   |
| ConfigMap                | nsx-ncp-config                       | Must update Kubernetes API and NSX API parameters |
| Deployment               | nsx-ncp                              |                                                   |
| ConfigMap                | nsx-node-agent-config                | Must update Kubernetes API and NSX API parameters |
| DaemonSet                | nsx-ncp-bootstrap                    |                                                   |
| DaemonSet                | nsx-node-agent                       |                                                   |

For this lab I will use mostly default configuration from the supplied manifest file template. The settings I change from what is in template are the following.

```
[nsx_v3]
nsx_api_managers = 192.168.1.14
nsx_api_user = admin
nsx_api_password = VMware1!
insecure = True
policy_nsxapi = True

[coe]
cluster = pandora

[k8s]
apiserver_host_ip = 192.168.1.27
apiserver_host_port = 6443

[nsx_kube_proxy]
ovs_uplink_port = ens192
```

Once the manifest file is updated and docker image in local registry we can apply the NCP manifest. Applying the manifest takes a couple of minutes while as it creates various Pods we can watch their creation to view progress.

```
kubectl apply --filename /home/ubuntu/ncp-ubuntu.yaml
kubectl get pods --output wide --namespace nsx-system --watch

NAME                       READY   STATUS     RESTARTS   AGE   IP             NODE         NOMINATED NODE   READINESS GATES
nsx-ncp-6978b9cb69-prrcq   0/1     Pending    0          1s    <none>         <none>       <none>           <none>
nsx-ncp-bootstrap-mdpxk    0/1     Init:0/1   0          1s    192.168.1.27   k8s-master   <none>           <none>
nsx-node-agent-dvzpq       0/3     Blocked    0          1s    192.168.1.27   k8s-master   <none>           <none>
nsx-ncp-bootstrap-mdpxk    0/1     Init:0/1   0          2s    192.168.1.27   k8s-master   <none>           <none>
nsx-ncp-bootstrap-mdpxk    0/1     PodInitializing   0          75s   192.168.1.27   k8s-master   <none>           <none>
nsx-ncp-bootstrap-mdpxk    1/1     Running           0          76s   192.168.1.27   k8s-master   <none>           <none>
nsx-node-agent-dvzpq       0/3     ContainerCreating   0          88s   192.168.1.27   k8s-master   <none>           <none>
nsx-node-agent-dvzpq       3/3     Running             0          90s   192.168.1.27   k8s-master   <none>           <none>
nsx-ncp-6978b9cb69-prrcq   0/1     Pending             0          93s   <none>         k8s-master   <none>           <none>
nsx-ncp-6978b9cb69-prrcq   0/1     ContainerCreating   0          93s   192.168.1.27   k8s-master   <none>           <none>
nsx-ncp-6978b9cb69-prrcq   1/1     Running             0          95s   192.168.1.27   k8s-master   <none>           <none>
```

If all has gone well we can take a look at the objects created within the namespace and then check the CNI on Node is good.

```
kubectl get all --namespace nsx-system

NAME                           READY   STATUS    RESTARTS   AGE
pod/nsx-ncp-6978b9cb69-prrcq   1/1     Running   0          2m14s
pod/nsx-ncp-bootstrap-mdpxk    1/1     Running   0          2m14s
pod/nsx-node-agent-dvzpq       3/3     Running   0          2m14s
NAME                               DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/nsx-ncp-bootstrap   1         1         1       1            1           <none>          2m14s
daemonset.apps/nsx-node-agent      1         1         1       1            1           <none>          2m14s
NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nsx-ncp   1/1     1            1           2m15s
NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/nsx-ncp-6978b9cb69   1         1         1       2m14s

kubectl describe nodes k8s-master
NAME         STATUS   ROLES    AGE   VERSION
k8s-master   Ready    master   3m   v1.16.4
```

Now its running we can check health of the NCP, the NCP has a [CLI](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.5/ncp-kubernetes/GUID-EA8E6CEE-36F4-423C-AD1E-DD6421A5FB1C.html) where we can run various commands including health checks.

```
kubectl exec -it nsx-ncp-6978b9cb69-prrcq --namespace nsx-system -- /bin/bash 
nsxcli
get ncp-k8s-api-server status
get ncp-nsx status
exit
```

## NSX Policy UI Object Creation

With NSX integration when we deploy a Kubernetes namespace the NCP creates a corresponding segment, IP Pool (from internet 10.0.0.0/8 range), subnet and Tier-1 router.  If you open NSX Manager and view one of the object categories which should have objects created. Then create two Kubernetes namespaces one called development and one called production.

```
kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
kubectl create -f https://k8s.io/examples/admin/namespace-prod.json
```

If you refresh view in NSX Manager you will see the new objects appear in the Policy UI.

<img src="/images/k8s-nsxt-tier1s.png">

We can then remove these two namespaces the NCP removes the corresponding segment, IP Pool (from internet 10.0.0.0/8 range), subnet and Tier-1 router.

```
kubectl delete -f https://k8s.io/examples/admin/namespace-dev.json
kubectl delete -f https://k8s.io/examples/admin/namespace-prod.json
```

## NCP Bootstrap Scaleout Cluster
We started this lab with a single node cluster to look at the nsx-ncp-bootstrap Daemonset. When Nodes are added to the cluster this should install and configure the Node with NCP.

Create a second VM with same hardware configuration as k8s-master but name this k8s-worker-1 and give IP 192.168.1.28.  Ensure the NSX segment port attached to  VM is tagged correctly. Ensure the NCP docker image is uploaded, imported to local docker registry and has tag applied.

To add the additional Node to the cluster first step is to create a token on master, use this output of this on the worker node. The nsx-ncp-bootstap and nsx-node-agent are DaemonSets this ensures that all Nodes run a copy of a Pod. When we add the worker node to the cluster we can see these two Pods being initialized on it. We can see the node start in NotReady state due to missing CNI and when nsx-ncp-bootstap has done its work the node ends in Ready state.

```
kubeadm token create --print-join-command
kubectl get nodes

NAME           STATUS     ROLES    AGE   VERSION
k8s-master     Ready      master   17h   v1.16.4
k8s-worker-1   NotReady   <none>   4s    v1.16.4

kubectl get pods -o wide --namespace nsx-system

NAME                       READY   STATUS     RESTARTS   AGE   IP             NODE           NOMINATED NODE   READINESS GATES
nsx-ncp-6978b9cb69-prrcq   1/1     Running    10         16h   192.168.1.27   k8s-master     <none>           <none>
nsx-ncp-bootstrap-2l4ml    0/1     Init:0/1   0          73s   192.168.1.28   k8s-worker-1   <none>           <none>
nsx-ncp-bootstrap-mdpxk    1/1     Running    2          16h   192.168.1.27   k8s-master     <none>           <none>
nsx-node-agent-dvzpq       3/3     Running    8          16h   192.168.1.27   k8s-master     <none>           <none>
nsx-node-agent-ksj8j       0/3     Blocked    0          73s   192.168.1.28   k8s-worker-1   <none>           <none>

kubectl get nodes

NAME           STATUS   ROLES    AGE     VERSION
k8s-master     Ready    master   17h     v1.16.4
k8s-worker-1   Ready    <none>   6m24s   v1.16.4
```

## IP Assignments
When a Pod is deployed it can be exposed as a Service, the service 

If we deploy an simple stateless application example like [guestbook](https://kubernetes.io/docs/tutorials/stateless-application/guestbook/). When we configures the lab we created NSX managed IP Block 10.0.0.0/16. When created the development namespace got allocated a /24 subnet from this block. If we view the Pods get IP in correct range. 

```
kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
kubectl config set-context --current --namespace=development
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-deployment.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-service.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-deployment.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-service.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-deployment.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-service.yaml
kubectl get pods

NAME                            READY   STATUS    RESTARTS   AGE
frontend-6cb7f8bd65-4zkxx       1/1     Running   0          59m
frontend-6cb7f8bd65-tl8fs       1/1     Running   0          59m
frontend-6cb7f8bd65-w5k8n       1/1     Running   0          59m
redis-master-7db7f6579f-rxhvj   1/1     Running   0          55m
redis-slave-7664787fbc-gq2fx    1/1     Running   0          55m
redis-slave-7664787fbc-x22dm    1/1     Running   0          55m

kubectl describe pod frontend-6cb7f8bd65-4zkxx | grep IP:

IP:           10.0.5.4
  IP:           10.0.5.4
```

When I deployed the application I also created Service resources. A Service is an abstraction which defines a logical set of Pods. Our application runs three frontend pods and two Redis slaves the Service provides virtual IP. The kube-proxy is responsible for implementing the virtual IP for Services. We can see the ClusterIP are not from the IP Block NSX manages these are managed by Kubernetes and the default range is 10.96.0.0/12 ( 10.96.0.1 - 10.111.255.254 ). 

```
kubectl get services

NAME           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
frontend       NodePort    10.98.146.143    <none>        80:32466/TCP   54m
redis-master   ClusterIP   10.97.245.142    <none>        6379/TCP       54m
redis-slave    ClusterIP   10.111.115.189   <none>        6379/TCP       54m

kubectl cluster-info dump | grep -m 1 service-cluster-ip-range
     "--service-cluster-ip-range=10.96.0.0/12",
```

We can then remove the namespace and all objects within.

```
kubectl delete -f https://k8s.io/examples/admin/namespace-dev.json
```

## NSX Loadbalancer

NSX provides an load balancer capability to Kubernetes we can use this by creating service resource with type LoadBalancer.  If we create a simple replicaset of five pods and expose this we can see it gets issued with IP Address from the IP Pool tagged with ncp/external = True. We can also see this in NSX Loadbalancer configuration.

```
kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
kubectl config set-context --current --namespace=development
kubectl apply -f https://k8s.io/examples/service/load-balancer-example.yaml
kubectl get replicasets
kubectl expose deployment hello-world --type=LoadBalancer --name=hello-world-nsx-lb
kubectl get services hello-world-nsx-lb

NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
hello-world-nsx-lb   LoadBalancer   10.110.64.115   172.16.0.13   8080:32091/TCP   7s
```

We can then remove the namespace and all objects within.

```
kubectl delete -f https://k8s.io/examples/admin/namespace-dev.json
```

## Kubernetes Pods Micro-Segmentation

One of the great usecases for NSX with vSphere has been the is the Distributed Firewall which protects VM workload at the Hypervisor layer. When we extend NSX into the Kubernetes container we also extend the Distributed Firewall capability. We can create firewall rules which contain members of groups, the groups can be dynamically populated by use of NSX tags. Kubernetes objects can have labels attached a label attached to a Pod is reflected in NSX as a tag on the segement port of the Pod.

A simple test might be to deploy a two tier app where frontend can talk to backend but frontend cannot talk to other frontend. If we create a NSX group called Web with membership criteria Segement Port, Tag, Equals web Scope secgroup. We then create a NSX DFW rule with Web as source and destination and action of drop.

With this in place we can ping test from one frontend pod to another frontend and backend and see this works.  We can then apply the label to the three frontend web Pods so they become members of the NSX group and are affected by the firewall rule.  With these in place we can retest ping from one frontend pod to another frontend and see that this is now blocked.

```
kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
kubectl config set-context --current --namespace=development
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-deployment.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-deployment.yaml
kubectl get pods -o wide

NAME                            READY   STATUS    RESTARTS   AGE   IP         NODE           NOMINATED NODE   READINESS GATES
frontend-6cb7f8bd65-4mctt       1/1     Running   0          13m   10.0.6.4   k8s-worker-1   <none>           <none>
frontend-6cb7f8bd65-8wkhr       1/1     Running   0          13m   10.0.6.2   k8s-worker-1   <none>           <none>
frontend-6cb7f8bd65-rtgc9       1/1     Running   0          13m   10.0.6.3   k8s-worker-1   <none>           <none>
redis-master-7db7f6579f-zlx26   1/1     Running   0          3s    10.0.6.5   k8s-worker-1   <none>           <none>

kubectl exec -it frontend-6cb7f8bd65-4mctt ping 10.0.6.2
PING 10.0.6.2 (10.0.6.2): 56 data bytes
64 bytes from 10.0.6.2: icmp_seq=0 ttl=64 time=0.083 ms

kubectl exec -it frontend-6cb7f8bd65-4mctt ping 10.0.6.5
PING 10.0.6.5 (10.0.6.5): 56 data bytes
64 bytes from 10.0.6.5: icmp_seq=0 ttl=64 time=3.191 ms

kubectl label pod frontend-6cb7f8bd65-4mctt secgroup=web
kubectl label pod frontend-6cb7f8bd65-8wkhr secgroup=web
kubectl label pod frontend-6cb7f8bd65-rtgc9 secgroup=web

kubectl exec -it frontend-6cb7f8bd65-4mctt ping 10.0.6.2
PING 10.0.6.2 (10.0.6.2): 56 data bytes
^C--- 10.0.6.2 ping statistics ---
2 packets transmitted, 0 packets received, 100% packet loss
command terminated with exit code 1

kubectl exec -it frontend-6cb7f8bd65-4mctt ping 10.0.6.5
PING 10.0.6.5 (10.0.6.5): 56 data bytes
64 bytes from 10.0.6.5: icmp_seq=0 ttl=64 time=5.672 ms
```

We can then remove the namespace and all objects within.

```
kubectl delete -f https://k8s.io/examples/admin/namespace-dev.json
```