---
layout: post
title:  "Introduction To Kubernetes Cluster Networking with NSX-T"
date:   2019-12-24 20:20:56 +0100
tags:
    - Docker
    - DevOps
    - Software Defined Networking
permalink: k8s-nsxt/
---
When developing a cloud native application using Docker containers you may want to host this on Kubernetes cluster. The move from Docker host to Kubernetes Cluster introduces changes to the networking model. [Kubernetes Cluster networking](https://kubernetes.io/docs/concepts/cluster-administration/networking/) model is extensible and products like NSX-T Data Center can be integrated using the NSX-T Container Plugin (NCP). Products like NSX-T bring advanced features which can enrich Kubernetes cluster networking.

## What Is A Kubernetes Pod
A Kubernetes Pod consists of one or more containers that are collocated on the same host, and are configured to share a network stack and other resources such as volumes. Kubernetes implements this by creating a special 'pause' container for each pod whose only purpose is to provide a network interface for the other containers.

<img src="/images/k8s-nsxt-pod.png">

As such every Pod gets its own IP address. Pods can be treated much like VMs or physical hosts from the perspectives of port allocation, naming, service discovery, load balancing, application configuration, and migration.

## Planning Kubernetes IP Address Space
Kubernetes Cluster networking requires enables a capability for all Pods on a node to communicate with all Pods on all nodes in the cluster without NAT and agents on a node (e.g. system daemons, kubelet) to communicate with all Pods on that node. To support this a range of IP addresses must be defined to be issued to Pods and Services within a cluster. Even though the range is used for both Pods and Services, it is called the Pod address range. The last /20 of the Pod address range is used for Services. A /20 range has 212 = 4096 addresses. So 4096 addresses are used for Services, and the rest of the range is used for Pods.

The Pod address range I will be using to issue ClusterIP for this lab cluster is 10.0.0.0/16 this is a block from within the RFC1918 private address space 10.0.0.0/8. 

As well as internal communicatins using ClusterIP some parts of your application may need to be exposed as a Service onto an externally routable IP address. There are two methods for exposing Service onto an externally routable IP address, NodePort and LoadBalancer.  Source NAT is used for translating private ClusterIP address to a public routable address.

The External address range I will be using to issue ExternalIP for this lab cluster is from within the RFC1918 private address space 172.16.0.0/16. 

## vSphere and NSX-T Lab
The lab I am using runs vSphere 6.7 Update 3 lab with three ESXi, a Active Directory VM which also acts as DNS server and NTP source and a vCenter appliance.

NSX-T 2.5.1 is installed with a single medium sized Edge deployed (Active-Passive). If you want to deploy Loadbalancer function it is important a medium Edge is used and it uses Active-Passive HA mode. All ESXi and the Edge are transport nodes in a Transport Zone named overlayTransport and the Edge is also a member of Transport Zone named vlanTransportZone. The Edge has configured a Tier-0 Logical Router this will be used to connect the Kubernetes nodes to external networks and perform Source NAT. The physical network has a single VLAN and devices have IPv4 addresses on the 192.168.1.0/24 network.

On the physical network all traffic routes northbound to internet except 172.16.0.0/16 which has static route to 192.168.1.17 which is the address of interface on the Tier-0 Logical Router. The Kubernetes external address range is part of this so these addresses will be routable from the physical network.

## NSX-T and NCP 2.5.1
One of the reasons I revisited NSX-T and Kubernetes integration just now was due to the [2.5 release which introduced some changes](https://blogs.vmware.com/networkvirtualization/2019/09/nsx-t-2-5-what-is-new-for-kubernetes.html/).

Prior to the 2.5 release all NSX objects which the NCP interacted with had to be created via the Advanced Networking & Security tab in the UI or the old imperative APIs. The imperative API was harder than it could have been to control so with NSX 2.4 VMware introduced a new intent-based Policy API and corresponding Simplified UI. The NCP now supports either the imperative or the intent-based API,  to use the intent-based API a new parameter in the NCP configmap (ncp.ini) policy_nsxapi needs to be set to True.

Another change I am interested in exploring is the simplified installation. In the past, an admin had to login to every k8s node and perform multiple steps to bootstrap it. She had to install the NSX CNI Plug-in and OpenVSwitch, to create OVS bridge and to add one vNic to the bridge. The 2.5 release introduces a second DaemonSet nsx-ncp-bootstrap Pod this now handles the deployment and lifecycle management of these components and we donâ€™t need to login to every node. This should make it easier to scale out a cluster with additional nodes.

## NSX Object Tagging
There can be many objects deployed within NSX-T, the NSX Container Plugin (NCP) needs to understand which of these objects to interact with. The NSX objects which the NSX Container Plugin (NCP) needs to interact with include the overlay transport zone, the tier-0 logical router, the logical switch to connect the node VMs, the IP Block for Pods IP addressing and an IP Block for external IP addressing. The NSX Container Plugin (NCP) configuration is stored in ncp.ini file. It is possible to put the UUID of NSX objects in the ncp.ini but this great for ongoing maintenance. The mapping of which NSX objects should be used is better achieved by applying tags to the correct NSX objects.

## Configure Tier-0 and Overlay
In order for the NSX Container Plugin to recognize the Tier-0 Logical Router and overlay transport zone as being for our lab Kubernetes cluster we need to add a tag for ncp/cluster.

| Tag            | Scope         |
| -------------- |:-------------:|
| <cluster_name> | ncp/cluster   |

For this environment I am configuring with cluster name 'pandora' so the Tier-0 Logical Router should have tag applied like this:

| Tag            | Scope         |
| -------------- |:-------------:|
| pandora        | ncp/cluster   |

<img src="/images/k8s-nsxt-tier0.png">

The overlay transport zone should have tag applied like this:

| Tag            | Scope         |
| -------------- |:-------------:|
| pandora        | ncp/cluster   |

<img src="/images/k8s-nsxt-overlay.png">

## Configure IP Block and IP Pool
NSX-T has an inbuilt capability for IP Management, in this we can allocate the blocks of IP Addresses we defined earlier.

In order for the NSX Container Plugin to recognize these as being the IP blocks to use for our lab Kubernetes cluster we need to add a tag for ncp/cluster. To recognize an IP Block to issue ClusterIP for the cluster in we also need to add a tag for ncp/no_snat.

| Tag            | Scope         |
| -------------- |:-------------:|
| <cluster_name> | ncp/cluster   |
| <cluster_name> | ncp/no_snat   |

For this environment I am configuring with cluster name 'pandora' so the IP Blocks for ClusterIP it should be tagged like this:

| Tag            | Scope         |
| -------------- |:-------------:|
| pandora        | ncp/no_snat   |
| pandora        | ncp/cluster   |

Create the 10.0.0.0/16 IP address block named k8s-1-internal with correct tags applied.

<img src="/images/k8s-nsxt-internal-block.png">

When Kubernetes services need to be accessed externally they require an IP Address the NCP can issue these from an NSX IP Pool. Create an IP Block named k8s-1-external with CIDR 172.16.0.0/16. Create an IP Pool named k8s-1-loadbalancer which has a subnet from IP Block k8s-1-external which is sized at 128. In order for the NSX Container Plugin to recognize an IP Pool to issue ExternalIP it should be tagged like this:

| Tag            | Scope         |
| -------------- |:-------------:|
| true           | ncp/external  |

<img src="/images/k8s-nsxt-external-block.png">

## Create NSX Logical Switch
Network connectivity to the containers running in Kubernetes is provided by a NSX-T logical switch segment which is often referred to as the node logical switch. For this I created a new segment called 'node-logical-switch' within the 'overlayTransportZone' connected to no gateway.

In order for the NSX Container Plugin to recognize this Logical Switch for the cluster we also need to add a tag for ncp/cluster. For this environment I am configuring with cluster name 'pandora' so the Logical Switch is tagged like this:

| Tag            | Scope         |
| -------------- |:-------------:|
| pandora        | ncp/cluster   |

<img src="/images/k8s-nsxt-logical-switch.png">

## Create Kubernetes Cluster VM
For the purposes of this we will need a very basic Kubernetes cluster,  create a Ubuntu 18.04 VMs with 2x CPU, 4GB RAM and 50GB vHDD named k8s-master.

The VM should have two vNIC one which is used to communicate with NSX API and which will host Kubernetes API. The second connected to the node logical switch which will have the Open vSwitch (OVS) bridge configured to give connectivity to the Pods. In my lab first connected to 'VM Network' enumerates as ens160 and the scond connected to 'node-logical-switch' enumerates as ens192.

```
cat /etc/netplan/01-netcfg.yaml 

network:
  version: 2
  renderer: networkd
  ethernets:
    ens160:
      addresses: 
      - 192.168.1.27/24
      gateway4: 192.168.1.254
      nameservers:
          search:
          - darrylcauldwell.com
          addresses: 
          - 192.168.1.10
    ens192: {}
```

## Tag Kubernetes VM Logical Switch Port
The Kubernetes VM connection to 'node-logical-switch' creates a segment ports object in NSX-T. The NSX-T Container Plug-in (NCP) needs to understand this mapping. To form this relationship two tags should be applied to each segment port:

| Tag            | Scope         |
| -------------- |:-------------:|
| <node_name>    | ncp/node_name |
| <cluster_name> | ncp/cluster   |

For this environment I am configuring with cluster name 'pandora' so the segment port attached to Kubernetes master VM is tagged like this:

| Tag            | Scope         |
| -------------- |:-------------:|
| k8s-master     | ncp/node_name |
| pandora        | ncp/cluster   |

<img src="/images/k8s-nsxt-segment-port-tags.png">

## Install and Configure Kubernetes

Docker and Kubernetes require installation do this by running the following.

```
sudo apt update
sudo apt upgrade -y
sudo apt install docker.io python apt-transport-https -y
sudo gpasswd -a $USER docker
sudo systemctl start docker
sudo systemctl enable docker
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
sudo apt-add-repository "deb http://apt.kubernetes.io/ kubernetes-xenial main"
sudo apt-get update
sudo swapoff -a 
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
sudo apt-get install -y kubelet=1.16.4-00 kubeadm=1.16.4-00 kubectl=1.16.4-00
```

Initialize the Kubernetes cluster by running the following.

```
sudo kubeadm init
```

In order to easily run kubectl as a user we need to copy the cluster configuration to the user profile, do this by running the following.

```
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

We can now connect to cluster and check the state of the nodes, do this by running the following on the Master node.

```
sudo kubectl get nodes

NAME           STATUS     ROLES    AGE     VERSION
k8s-master     NotReady   master   10m     v1.14.8
```

The status of each Node will show 'NotReady',  we can get more details of why it is in this state by running the following on the Master node.

```
sudo kubectl describe nodes k8s-master | grep Conditions A9

Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 16 Dec 2019 18:47:40 +0000   Mon, 16 Dec 2019 18:38:10 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 16 Dec 2019 18:47:40 +0000   Mon, 16 Dec 2019 18:38:10 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 16 Dec 2019 18:47:40 +0000   Mon, 16 Dec 2019 18:38:10 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            False   Mon, 16 Dec 2019 18:47:40 +0000   Mon, 16 Dec 2019 18:38:10 +0000   KubeletNotReady              runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:docker: network plugin is not ready: cni config uninitialized
Addresses:
  InternalIP:  192.168.1.27
  Hostname:    k8s-master
```

We can see this reason for the error is 'NetworkPluginNotReady' that the Container Networking Interface (CNI) plugin is missing. 

## Install and Configure NSX Container Plug-in (NCP)

The NSX Container Plug-in (NCP) provides integration between NSX-T and Kubernetes, it is a containerised application which manages communicates between NSX Manager and the Kubernetes control plane. As well as the NCP there is a NSX DaemonSet comprised of two containers, the NSX Node Agent and the NSX Kube-Proxy. The NSX Node Agent is a container image which manages the container network interface. The NSX Kube-Proxy is a container image which replaces the native distributed east-west load balancer in Kubernetes with the NSX load-balancer based on Open vSwitch (OVS). The NSX Container Network Interface (CNI) is a plug-in which runs on each Kubernetes node. It provides a connection between the container interface to the node Open vSwitch, and programs the node Open vSwitch to tag and forward container traffic.

<img src="/images/nsx-k8s.jpeg">

The NSX Container Plug-in (NCP) application runs in Kubernetes it is supplied as .zip download. The configuration of Kubernetes applications is maintained in a manifest file. 

Kubernetes applications also require container image files. The NSX Container Plug-in (NCP) ships with three Docker container images one each for Photon, RHEL and Ubuntu (Kubernetes\nsx-ncp-ubuntu-2.5.0.14628220.tar). We could deploy this to a local container registry like Harbor but for simple lab just import the image to the local repository on all Kubernetes nodes (Master and Worker). Extract from zip and copy the container image to all Kubernetes nodes (Master and Worker). The default container image name specified in the manifest is nsx-ncp so we can apply that as tag on the image.

```
sudo docker load -i /home/ubuntu/nsx-ncp-ubuntu-2.5.1.15287458.tar
sudo docker image tag registry.local/2.5.1.15287458/nsx-ncp-ubuntu nsx-ncp
sudo docker images | grep ncp
```

The NSX Container Plug-in (NCP) ships with two Kubernetes manifest file yaml templates one each for RHEL and Ubuntu (Kubernetes\ncp-ubuntu.yaml). Prior to the 2.5 release there were multiple  manifest files each targetting a specific resource specification.  These are now merged into a single manifest file with multiple sections with multiple resource specifications the sections can be identified by the separator `---`. The resources are created in the order they appear in the file. 

For this lab we will use mostly default configuration from the supplied manifest file template it does however indicate some mandatory updates to ConfigMap resourcesto reflect the environment.

| Resource Kind            | Resource Name                        |Comments                                           |
| ------------------------ |:------------------------------------:|:-------------------------------------------------:|
| CustomResourceDefinition | nsxerrors.nsx.vmware.com             |                                                   |
| CustomResourceDefinition | nsxlocks.nsx.vmware.com              |                                                   |
| Namespace                | nsx-system                           |                                                   |
| ServiceAccount           | ncp-svc-account                      |                                                   |
| ClusterRole              | ncp-cluster-role                     |                                                   |
| ClusterRole              | ncp-patch-role                       |                                                   |
| ClusterRoleBinding       | ncp-cluster-role-binding             |                                                   |
| ClusterRoleBinding       | ncp-patch-role-binding               |                                                   |
| ServiceAccount           | nsx-node-agent-svc-account           |                                                   |
| ClusterRole              | nsx-node-agent-cluster-role          |                                                   |
| ClusterRoleBinding       | nsx-node-agent-cluster-role-binding  |                                                   |
| ConfigMap                | nsx-ncp-config                       | Must update Kubernetes API and NSX API parameters |
| Deployment               | nsx-ncp                              |                                                   |
| ConfigMap                | nsx-node-agent-config                | Must update Kubernetes API and NSX API parameters |
| DaemonSet                | nsx-ncp-bootstrap                    |                                                   |
| DaemonSet                | nsx-node-agent                       |                                                   |

Based on comments in yaml I uncommenting or added  the following parameters,  the values here are appropriate for the lab I am using.

```
[nsx_v3]
nsx_api_managers = 192.168.1.14
nsx_api_user = admin
nsx_api_password = VMware1!
insecure = True
policy_nsxapi = True

[coe]
cluster = pandora

[k8s]
apiserver_host_ip = 192.168.1.27
apiserver_host_port = 6443

[nsx_kube_proxy]
ovs_uplink_port = ens192
```

Once the ncp-ubuntu.yaml is updated and docker image in all local registries we can apply the NCP manifest. Applying the manifest takes 5-6 minutes while as it creates various Pods we can watch their creation to view progress.

```
kubectl apply --filename /home/ubuntu/ncp-ubuntu.yaml
kubectl get pods --output wide --namespace nsx-system --watch

NAME                       READY   STATUS     RESTARTS   AGE   IP             NODE         NOMINATED NODE   READINESS GATES
nsx-ncp-6978b9cb69-prrcq   0/1     Pending    0          1s    <none>         <none>       <none>           <none>
nsx-ncp-bootstrap-mdpxk    0/1     Init:0/1   0          1s    192.168.1.27   k8s-master   <none>           <none>
nsx-node-agent-dvzpq       0/3     Blocked    0          1s    192.168.1.27   k8s-master   <none>           <none>
nsx-ncp-bootstrap-mdpxk    0/1     Init:0/1   0          2s    192.168.1.27   k8s-master   <none>           <none>
nsx-ncp-bootstrap-mdpxk    0/1     PodInitializing   0          75s   192.168.1.27   k8s-master   <none>           <none>
nsx-ncp-bootstrap-mdpxk    1/1     Running           0          76s   192.168.1.27   k8s-master   <none>           <none>
nsx-node-agent-dvzpq       0/3     ContainerCreating   0          88s   192.168.1.27   k8s-master   <none>           <none>
nsx-node-agent-dvzpq       3/3     Running             0          90s   192.168.1.27   k8s-master   <none>           <none>
nsx-ncp-6978b9cb69-prrcq   0/1     Pending             0          93s   <none>         k8s-master   <none>           <none>
nsx-ncp-6978b9cb69-prrcq   0/1     ContainerCreating   0          93s   192.168.1.27   k8s-master   <none>           <none>
nsx-ncp-6978b9cb69-prrcq   1/1     Running             0          95s   192.168.1.27   k8s-master   <none>           <none>
```

If all has gone well we can take a look at the objects created within the namespace and then check the CNI on Node is good.

```
kubectl get all --namespace nsx-system

NAME                           READY   STATUS    RESTARTS   AGE
pod/nsx-ncp-6978b9cb69-prrcq   1/1     Running   0          2m14s
pod/nsx-ncp-bootstrap-mdpxk    1/1     Running   0          2m14s
pod/nsx-node-agent-dvzpq       3/3     Running   0          2m14s
NAME                               DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/nsx-ncp-bootstrap   1         1         1       1            1           <none>          2m14s
daemonset.apps/nsx-node-agent      1         1         1       1            1           <none>          2m14s
NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/nsx-ncp   1/1     1            1           2m15s
NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/nsx-ncp-6978b9cb69   1         1         1       2m14s

kubectl describe nodes k8s-master
NAME         STATUS   ROLES    AGE   VERSION
k8s-master   Ready    master   3m   v1.16.4
```

Now its running we can check health of the NCP, to do this connect to the NCP Pod we find nsxcli where we can run various commands including some health checks.

```
kubectl exec -it nsx-ncp-6978b9cb69-prrcq --namespace nsx-system -- /bin/bash 
nsxcli
get ncp-k8s-api-server status
get ncp-nsx status
exit
```

## NSX Policy UI Object Creation

With NSX integration when we deploy a Kubernetes namespace the NCP creates a corresponding segment, IP Pool (from internet 10.0.0.0/8 range), subnet and Tier-1 router.  If you open NSX Manager and view one of the object categories which should have objects created. Then create two Kubernetes namespaces one called development and one called production.

```
kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
kubectl create -f https://k8s.io/examples/admin/namespace-prod.json
```

If you refresh view in NSX Manager you will see the new objects appear in the Policy UI.

<img src="/images/k8s-nsxt-tier1s.png">

We can then remove these two namespaces the NCP removes the corresponding segment, IP Pool (from internet 10.0.0.0/8 range), subnet and Tier-1 router.

```
kubectl delete -f https://k8s.io/examples/admin/namespace-dev.json
kubectl delete -f https://k8s.io/examples/admin/namespace-prod.json
```

## NCP Bootstrap Scaleout Cluster
We started this lab with a single node cluster to look at the nsx-ncp-bootstrap Daemonset. When Nodes are added to the cluster this should install and configure the Node with NCP.

Create a second VM with same hardware configuration as k8s-master but name this k8s-worker-1 and give IP 192.168.1.28.  Ensure the NSX segment port attached to  VM is tagged correctly. Ensure the NCP docker image is uploaded, imported to local docker registry and has tag applied.

To add the additional Node to the cluster first step is to create a token on master, use this output of this on the worker node. The nsx-ncp-bootstap and nsx-node-agent are DaemonSets this ensures that all Nodes run a copy of a Pod. When we add the worker node to the cluster we can see these two Pods being initialized on it. We can see the node start in NotReady state due to missing CNI and when nsx-ncp-bootstap has done its work the node ends in Ready state.

```
kubeadm token create --print-join-command
kubectl get nodes

NAME           STATUS     ROLES    AGE   VERSION
k8s-master     Ready      master   17h   v1.16.4
k8s-worker-1   NotReady   <none>   4s    v1.16.4

kubectl get pods -o wide --namespace nsx-system

NAME                       READY   STATUS     RESTARTS   AGE   IP             NODE           NOMINATED NODE   READINESS GATES
nsx-ncp-6978b9cb69-prrcq   1/1     Running    10         16h   192.168.1.27   k8s-master     <none>           <none>
nsx-ncp-bootstrap-2l4ml    0/1     Init:0/1   0          73s   192.168.1.28   k8s-worker-1   <none>           <none>
nsx-ncp-bootstrap-mdpxk    1/1     Running    2          16h   192.168.1.27   k8s-master     <none>           <none>
nsx-node-agent-dvzpq       3/3     Running    8          16h   192.168.1.27   k8s-master     <none>           <none>
nsx-node-agent-ksj8j       0/3     Blocked    0          73s   192.168.1.28   k8s-worker-1   <none>           <none>

kubectl get nodes

NAME           STATUS   ROLES    AGE     VERSION
k8s-master     Ready    master   17h     v1.16.4
k8s-worker-1   Ready    <none>   6m24s   v1.16.4
```

## IP Assignments
When a Pod is deployed it can be exposed as a Service, the service 

If we deploy an simple stateless application example like [guestbook](https://kubernetes.io/docs/tutorials/stateless-application/guestbook/). When we configures the lab we created NSX managed IP Block 10.0.0.0/16. When created the development namespace got allocated a /24 subnet from this block. If we view the Pods get IP in correct range. 

```
kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
kubectl config set-context --current --namespace=development
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-deployment.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-service.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-deployment.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-service.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-deployment.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-service.yaml
kubectl get pods

NAME                            READY   STATUS    RESTARTS   AGE
frontend-6cb7f8bd65-4zkxx       1/1     Running   0          59m
frontend-6cb7f8bd65-tl8fs       1/1     Running   0          59m
frontend-6cb7f8bd65-w5k8n       1/1     Running   0          59m
redis-master-7db7f6579f-rxhvj   1/1     Running   0          55m
redis-slave-7664787fbc-gq2fx    1/1     Running   0          55m
redis-slave-7664787fbc-x22dm    1/1     Running   0          55m

kubectl describe pod frontend-6cb7f8bd65-4zkxx | grep IP:

IP:           10.0.5.4
  IP:           10.0.5.4
```

When I deployed the application I also created Service resources. A Service is an abstraction which defines a logical set of Pods. Our application runs three frontend pods and two Redis slaves the Service provides virtual IP. The kube-proxy is responsible for implementing the virtual IP for Services. We can see the ClusterIP are not from the IP Block NSX manages these are managed by Kubernetes and the default range is 10.96.0.0/12 ( 10.96.0.1 - 10.111.255.254 ). 

```
kubectl get services

NAME           TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
frontend       NodePort    10.98.146.143    <none>        80:32466/TCP   54m
redis-master   ClusterIP   10.97.245.142    <none>        6379/TCP       54m
redis-slave    ClusterIP   10.111.115.189   <none>        6379/TCP       54m

kubectl cluster-info dump | grep -m 1 service-cluster-ip-range
     "--service-cluster-ip-range=10.96.0.0/12",
```

We can then remove the namespace and all objects within.

```
kubectl delete -f https://k8s.io/examples/admin/namespace-dev.json
```

## NSX Loadbalancer

NSX provides an load balancer capability to Kubernetes we can use this by creating service resource with type LoadBalancer.  If we create a simple replicaset of five pods and expose this we can see it gets issued with IP Address from the IP Pool tagged with ncp/external = True. We can also see this in NSX Loadbalancer configuration.

```
kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
kubectl config set-context --current --namespace=development
kubectl apply -f https://k8s.io/examples/service/load-balancer-example.yaml
kubectl get replicasets
kubectl expose deployment hello-world --type=LoadBalancer --name=hello-world-nsx-lb
kubectl get services hello-world-nsx-lb

NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
hello-world-nsx-lb   LoadBalancer   10.110.64.115   172.16.0.13   8080:32091/TCP   7s
```

We can then remove the namespace and all objects within.

```
kubectl delete -f https://k8s.io/examples/admin/namespace-dev.json
```

## Kubernetes Pods Micro-Segmentation

One of the great usecases for NSX with vSphere has been the is the Distributed Firewall which protects VM workload at the Hypervisor layer. When we extend NSX into the Kubernetes container we also extend the Distributed Firewall capability. We can create firewall rules which contain members of groups, the groups can be dynamically populated by use of NSX tags. Kubernetes objects can have labels attached a label attached to a Pod is reflected in NSX as a tag on the segement port of the Pod.

A simple test might be to deploy a two tier app where frontend can talk to backend but frontend cannot talk to other frontend. If we create a NSX group called Web with membership criteria Segement Port, Tag, Equals web Scope secgroup. We then create a NSX DFW rule with Web as source and destination and action of drop. We can ping test before applying label,  apply labels and then ping test after.


```
kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
kubectl config set-context --current --namespace=development
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-deployment.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-deployment.yaml
kubectl get pods -o wide

NAME                            READY   STATUS    RESTARTS   AGE   IP         NODE           NOMINATED NODE   READINESS GATES
frontend-6cb7f8bd65-4mctt       1/1     Running   0          13m   10.0.6.4   k8s-worker-1   <none>           <none>
frontend-6cb7f8bd65-8wkhr       1/1     Running   0          13m   10.0.6.2   k8s-worker-1   <none>           <none>
frontend-6cb7f8bd65-rtgc9       1/1     Running   0          13m   10.0.6.3   k8s-worker-1   <none>           <none>
redis-master-7db7f6579f-zlx26   1/1     Running   0          3s    10.0.6.5   k8s-worker-1   <none>           <none>

kubectl exec -it frontend-6cb7f8bd65-4mctt ping 10.0.6.2
PING 10.0.6.2 (10.0.6.2): 56 data bytes
64 bytes from 10.0.6.2: icmp_seq=0 ttl=64 time=0.083 ms

kubectl exec -it frontend-6cb7f8bd65-4mctt ping 10.0.6.5
PING 10.0.6.5 (10.0.6.5): 56 data bytes
64 bytes from 10.0.6.5: icmp_seq=0 ttl=64 time=3.191 ms

kubectl label pod frontend-6cb7f8bd65-4mctt secgroup=web
kubectl label pod frontend-6cb7f8bd65-8wkhr secgroup=web
kubectl label pod frontend-6cb7f8bd65-rtgc9 secgroup=web

kubectl exec -it frontend-6cb7f8bd65-4mctt ping 10.0.6.2
PING 10.0.6.2 (10.0.6.2): 56 data bytes
^C--- 10.0.6.2 ping statistics ---
2 packets transmitted, 0 packets received, 100% packet loss
command terminated with exit code 1

kubectl exec -it frontend-6cb7f8bd65-4mctt ping 10.0.6.5
PING 10.0.6.5 (10.0.6.5): 56 data bytes
64 bytes from 10.0.6.5: icmp_seq=0 ttl=64 time=5.672 ms
```

We can then remove the namespace and all objects within.

```
kubectl delete -f https://k8s.io/examples/admin/namespace-dev.json
```