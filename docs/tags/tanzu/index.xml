<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tanzu on </title>
    <link>https://darrylcauldwell.github.io/tags/tanzu/</link>
    <description>Recent content in tanzu on </description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 10 Dec 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://darrylcauldwell.github.io/tags/tanzu/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Managing Clusters Using Tanzu Mission Control</title>
      <link>https://darrylcauldwell.github.io/post/tanzu-tmc/</link>
      <pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/tanzu-tmc/</guid>
      <description>
        
          &lt;p&gt;Third in a series of posts which build on each other looking Tanzu.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://darrylcauldwell.github.io/post/tanzu-basic-nsx&#34;&gt;Deploying Tanzu for vSphere with NSX-T&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://darrylcauldwell.github.io/post/tanzu-tkc&#34;&gt;Managing Tanzu Kubernetes Clusters Using ClusterAPI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://darrylcauldwell.github.io/post/tanzu-tmc&#34;&gt;Managing Tanzu Kubernetes Clusters Using Misson Control&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I am moving next to look at Tanzu Mission Control. The Mission Control inventory has a hierachy, first I&amp;rsquo;ll create a group to place the clusters in. While adding the group add some metadata to group including a label so other users know who to contact with any queries.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-group.png&#34; alt=&#34;Tanzu Mission Control Cluster Group&#34;&gt;&lt;/p&gt;
&lt;p&gt;With the group in place we can look at attaching an existing cluster to TMC.  Following the wizard the first step is to enter some metadata for the cluster being attached.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-meta.png&#34; alt=&#34;Tanzu Mission Control Cluster Metadata&#34;&gt;&lt;/p&gt;
&lt;p&gt;anzu Misson Control does not need to connect inwards to the cluster to be managed.  Instead agents are deployed and configured on the cluster which initiate outbound connection to Tanzu Misson Control. Outbound internet communications are sometimes via proxy, if required add proxy details next.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-proxy.png&#34; alt=&#34;Tanzu Mission Control Cluster Proxy&#34;&gt;&lt;/p&gt;
&lt;p&gt;When the deployed agents initiate communications with TMC they require to advertise who they are and be associated with the right objects. The agent configuration is supplied via UI as YAML. A URL is supplied to the configuration on internet. The administrator of this cluster is Hermione so I connect with her credentials and apply the yaml file to deploy and configure the agents.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl vsphere login --server&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;172.16.1.2 --tanzu-kubernetes-cluster-name hogwarts-cluster01 --tanzu-kubernetes-cluster-namespace hogwarts --vsphere-username hermione@vsphere.local --insecure-skip-tls-verify

kubectl create -f &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://tanzuemea.tmc.cloud.vmware.com/installer?id=f1d706f38d6f3ad834314a941831b190e18508468ade1adf2e105a822c8c0b49&amp;amp;source=attach&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-kubectl.png&#34; alt=&#34;Tanzu Mission Control Cluster kubectl&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once the agent has be deployed can move back to TMC. The agents take a couple of minutes to initialize and gather data but in very short order can then view the cluster and extension health in TMC.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-health.png&#34; alt=&#34;Tanzu Mission Control Cluster Health&#34;&gt;&lt;/p&gt;
&lt;p&gt;To unlock more power of TMC we can register the TKG management cluster. The process is similar to attaching subordinate cluster but is slightly different as before we start with adding some metadata.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-sup-meta.png&#34; alt=&#34;Tanzu Mission Control Supervisor Cluster Metadata&#34;&gt;&lt;/p&gt;
&lt;p&gt;Again here I am not using proxy so just skip through next page.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-sup-proxy.png&#34; alt=&#34;Tanzu Mission Control Supervisor Cluster Proxy&#34;&gt;&lt;/p&gt;
&lt;p&gt;Rather than being presented with a yaml file to apply with kubectl cmd here the wizard outputs a URL link which contains config.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-sup-URL.png&#34; alt=&#34;Tanzu Mission Control Supervisor Cluster Config URL&#34;&gt;&lt;/p&gt;
&lt;p&gt;We then have to create environmentally specific yaml file which includes namespace and the supplied URL. This needs to be ran in the context of the cluster itself so we switch to that and then get the namespace name prefixed with svc-tmc.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl vsphere login --server&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;172.16.1.2 --vsphere-username administrator@vsphere.local --insecure-skip-tls-verify
Logged in successfully.
You have access to the following contexts:
   172.16.1.2
   hogsmead
   hogwarts
If the context you wish to use is not in this list, you may need to try
logging in again later, or contact your cluster administrator.
To change context, use &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;kubectl config use-context &amp;lt;workload name&amp;gt;&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;

kubectl config use-context 172.16.1.2
Switched to context &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;172.16.1.2&amp;#34;&lt;/span&gt;.

kubectl get ns | grep svc-tmc
NAME                                        STATUS   AGE
svc-tmc-c34                                 Active   11d
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With the namespace name and URL we can populate the yaml file to apply agent and config.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;installers.tmc.cloud.vmware.com/v1alpha1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;AgentInstall&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;tmc-agent-installer-config&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;svc-tmc-c34 &lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;operation&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;INSTALL&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;registrationLink&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://tanzuemea.tmc.cloud.vmware.com/installer?id=0f25b71394ac623867f64138ed9d27bb0db5e8d0436e874e4f65ff179e7be807&amp;amp;source=registration&amp;amp;type=tkgs&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can then apply the configuration using kubectl and the agents get created with correct configuration.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl create -f tmc-registration.yaml
agentinstall.installers.tmc.cloud.vmware.com/tmc-agent-installer-config created
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-sup-kubectl.png&#34; alt=&#34;Tanzu Mission Control Supervisor Cluster kubectl&#34;&gt;&lt;/p&gt;
&lt;p&gt;After a couple of minutes when the agents have started and have reported back we can get a view of supervisor cluster health.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-sup-health.png&#34; alt=&#34;Tanzu Mission Control Supervisor Cluster Health&#34;&gt;&lt;/p&gt;
&lt;p&gt;With the supervisor cluster registered we can start to some interesting things from TMC like provision a new cluster. To recap at this stage my deployment has two namespaces configured. Only Hogwarts has a workload cluster deployed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hogwarts
&lt;ul&gt;
&lt;li&gt;hogwarts-cluster-01&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Hogsmead&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Starting the TMC wizard to create cluster first prompt is for the provider. This is analagous to namespace so we&amp;rsquo;ll look at creating cluster under Hogsmead.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-prov-provider.png&#34; alt=&#34;Tanzu Mission Control Cluster Provising Provider&#34;&gt;&lt;/p&gt;
&lt;p&gt;Then add some cluster metadata.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-prov-meta.png&#34; alt=&#34;Tanzu Mission Control Cluster Provising Metadata&#34;&gt;&lt;/p&gt;
&lt;p&gt;Can then pick a version of Kubernetes which the cluster will run, the Pod and Service networking configuration and storage class.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-prov-config.png&#34; alt=&#34;Tanzu Mission Control Cluster Provising Config&#34;&gt;&lt;/p&gt;
&lt;p&gt;Followed by choosing the VM class, hardware specifcation of the cluster nodes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-prov-vmclass.png&#34; alt=&#34;Tanzu Mission Control Cluster Provising VM Class&#34;&gt;&lt;/p&gt;
&lt;p&gt;Finally the node pool configuration.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-prov-nodepool.png&#34; alt=&#34;Tanzu Mission Control Cluster Provising Node Pool&#34;&gt;&lt;/p&gt;
&lt;p&gt;It takes a few minutes to deploy the required VMs and configure the Kubernetes cluster.  You can get a view on how far it is along by looking in vCenter.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-prov-vcenter.png&#34; alt=&#34;Tanzu Mission Control Cluster Provising vCenter&#34;&gt;&lt;/p&gt;
&lt;p&gt;When deploying cluster from TMC the agents and configuration to register the cluster as with TMC is included.  After a few minutes when VMs are built and configured the cluster shows in UI.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-prov-registered.png&#34; alt=&#34;Tanzu Mission Control Cluster Provising Registered&#34;&gt;&lt;/p&gt;
&lt;p&gt;As well as provisioning TMC can also help with day two operations. When deploying the Hogsmead cluster I selected an v1.20.12 version of k8s. Upgrading to latest can be initiated very easily. Within the cluster view just hit Upgrade and select from drop down of all newer versions the version you need.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-lcm.png&#34; alt=&#34;Tanzu Mission Control Cluster Lifecycle Management&#34;&gt;&lt;/p&gt;
&lt;p&gt;A couple of minutes later and can see its now running v1.21.6.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-lcm-version.png&#34; alt=&#34;Tanzu Mission Control Cluster Lifecycle Management Complete&#34;&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Managing Tanzu Kubernetes Clusters Using ClusterAPI</title>
      <link>https://darrylcauldwell.github.io/post/tanzu-tkc/</link>
      <pubDate>Fri, 03 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/tanzu-tkc/</guid>
      <description>
        
          &lt;p&gt;Second in a series of posts which build on each other looking Tanzu.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://darrylcauldwell.github.io/post/tanzu-basic-nsx&#34;&gt;Deploying Tanzu for vSphere with NSX-T&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://darrylcauldwell.github.io/post/tanzu-tkc&#34;&gt;Managing Tanzu Kubernetes Clusters Using ClusterAPI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://darrylcauldwell.github.io/post/tanzu-tmc&#34;&gt;Managing Tanzu Kubernetes Clusters Using Misson Control&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Clusters are created within organisational construct known as a namespace. The namespace configuration can be used to map user/groups to role/permission it uses the same users/groups available as vSphere Identity source, these can be aligned to one of three roles view, edit and owner. The namespace configuration also defines which vSphere storage policies can be aligned to namespace and the amount of available CPU, Memory and Storage can be limited. I have a single cluster prepared with the following Namespace configuration:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Namespace&lt;/th&gt;
&lt;th&gt;User&lt;/th&gt;
&lt;th&gt;Role&lt;/th&gt;
&lt;th&gt;Storage Policy&lt;/th&gt;
&lt;th&gt;Limit&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Hogwarts&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;mailto:hermione@vsphere.local&#34;&gt;hermione@vsphere.local&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Owner&lt;/td&gt;
&lt;td&gt;vSAN Default Storage Policy&lt;/td&gt;
&lt;td&gt;Unlimited&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hogwarts&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;mailto:harry@vsphere.local&#34;&gt;harry@vsphere.local&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Edit&lt;/td&gt;
&lt;td&gt;vSAN Default Storage Policy&lt;/td&gt;
&lt;td&gt;Unlimited&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hogwarts&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;mailto:ron@vsphere.local&#34;&gt;ron@vsphere.local&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;View&lt;/td&gt;
&lt;td&gt;vSAN Default Storage Policy&lt;/td&gt;
&lt;td&gt;Unlimited&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hogsmead&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;mailto:hermione@vsphere.local&#34;&gt;hermione@vsphere.local&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Edit&lt;/td&gt;
&lt;td&gt;vSAN Default Storage Policy&lt;/td&gt;
&lt;td&gt;Unlimited&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hogsmead&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;mailto:harry@vsphere.local&#34;&gt;harry@vsphere.local&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Owner&lt;/td&gt;
&lt;td&gt;vSAN Default Storage Policy&lt;/td&gt;
&lt;td&gt;Unlimited&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hogsmead&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;mailto:ron@vsphere.local&#34;&gt;ron@vsphere.local&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;View&lt;/td&gt;
&lt;td&gt;vSAN Default Storage Policy&lt;/td&gt;
&lt;td&gt;Unlimited&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;With these in place I can attempt to connect as Hermione and check access to Hippogriff.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;set KUBECTL_VSPHERE_PASSWORD&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;VMware1!
kubectl vsphere login --server&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;172.16.1.2 --vsphere-username hermione@vsphere.local --insecure-skip-tls-verify 

Logged in successfully.
You have access to the following contexts:
   172.16.1.2
   hogsmead
   hogwarts
If the context you wish to use is not in this list, you may need to try
logging in again later, or contact your cluster administrator.
To change context, use &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;kubectl config use-context &amp;lt;workload name&amp;gt;&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;

kubectl config use-context hogwarts
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To deploy a cluster you need to specify certain attributes such as which image and Kubernetes version to deploy and the dimensions of the nodes. The node images are stored in vSphere Content Library and some standard ones are available via a publiclu content library subscription. The VM hardware specification dimensions of node are defined as virtual machine classes. The VM storage location are defined as storage classes. The names of available images, vmclasses and storageclasses available to be deployed from supervisor can be easily checked.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl get tanzukubernetesreleases
NAME                                VERSION                          READY   COMPATIBLE   CREATED   UPDATES AVAILABLE
v1.16.12---vmware.1-tkg.1.da7afe7   1.16.12+vmware.1-tkg.1.da7afe7   True    True         6m28s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.17.17+vmware.1-tkg.1.d44d45a 1.16.14+vmware.1-tkg.1.ada4837&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
v1.16.14---vmware.1-tkg.1.ada4837   1.16.14+vmware.1-tkg.1.ada4837   True    True         6m25s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.17.17+vmware.1-tkg.1.d44d45a&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
v1.16.8---vmware.1-tkg.3.60d2ffd    1.16.8+vmware.1-tkg.3.60d2ffd    False   False        6m33s
v1.17.11---vmware.1-tkg.1.15f1e18   1.17.11+vmware.1-tkg.1.15f1e18   True    True         6m29s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.18.19+vmware.1-tkg.1.17af790 1.17.17+vmware.1-tkg.1.d44d45a&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
v1.17.11---vmware.1-tkg.2.ad3d374   1.17.11+vmware.1-tkg.2.ad3d374   True    True         6m28s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.18.19+vmware.1-tkg.1.17af790 1.17.17+vmware.1-tkg.1.d44d45a&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
v1.17.13---vmware.1-tkg.2.2c133ed   1.17.13+vmware.1-tkg.2.2c133ed   True    True         6m30s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.18.19+vmware.1-tkg.1.17af790 1.17.17+vmware.1-tkg.1.d44d45a&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
v1.17.17---vmware.1-tkg.1.d44d45a   1.17.17+vmware.1-tkg.1.d44d45a   True    True         6m27s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.18.19+vmware.1-tkg.1.17af790&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
v1.17.7---vmware.1-tkg.1.154236c    1.17.7+vmware.1-tkg.1.154236c    True    True         6m24s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.18.19+vmware.1-tkg.1.17af790 1.17.17+vmware.1-tkg.1.d44d45a&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
v1.17.8---vmware.1-tkg.1.5417466    1.17.8+vmware.1-tkg.1.5417466    True    True         6m28s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.18.19+vmware.1-tkg.1.17af790 1.17.17+vmware.1-tkg.1.d44d45a&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
v1.18.10---vmware.1-tkg.1.3a6cd48   1.18.10+vmware.1-tkg.1.3a6cd48   True    True         6m30s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.19.14+vmware.1-tkg.1.8753786 1.18.19+vmware.1-tkg.1.17af790&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
v1.18.15---vmware.1-tkg.1.600e412   1.18.15+vmware.1-tkg.1.600e412   True    True         6m27s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.19.14+vmware.1-tkg.1.8753786 1.18.19+vmware.1-tkg.1.17af790&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
v1.18.15---vmware.1-tkg.2.ebf6117   1.18.15+vmware.1-tkg.2.ebf6117   True    True         6m29s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.19.14+vmware.1-tkg.1.8753786 1.18.19+vmware.1-tkg.1.17af790&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
v1.18.19---vmware.1-tkg.1.17af790   1.18.19+vmware.1-tkg.1.17af790   True    True         6m27s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.19.14+vmware.1-tkg.1.8753786&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
v1.18.5---vmware.1-tkg.1.c40d30d    1.18.5+vmware.1-tkg.1.c40d30d    True    True         6m30s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.19.14+vmware.1-tkg.1.8753786 1.18.19+vmware.1-tkg.1.17af790&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
v1.19.11---vmware.1-tkg.1.9d9b236   1.19.11+vmware.1-tkg.1.9d9b236   True    True         6m33s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.20.9+vmware.1-tkg.1.a4cee5b 1.19.14+vmware.1-tkg.1.8753786&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
v1.19.14---vmware.1-tkg.1.8753786   1.19.14+vmware.1-tkg.1.8753786   True    True         6m29s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.20.9+vmware.1-tkg.1.a4cee5b&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
v1.19.7---vmware.1-tkg.1.fc82c41    1.19.7+vmware.1-tkg.1.fc82c41    True    True         6m26s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.20.9+vmware.1-tkg.1.a4cee5b 1.19.14+vmware.1-tkg.1.8753786&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
v1.19.7---vmware.1-tkg.2.f52f85a    1.19.7+vmware.1-tkg.2.f52f85a    True    True         6m26s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.20.9+vmware.1-tkg.1.a4cee5b 1.19.14+vmware.1-tkg.1.8753786&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
v1.20.2---vmware.1-tkg.1.1d4f79a    1.20.2+vmware.1-tkg.1.1d4f79a    True    True         6m26s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.21.2+vmware.1-tkg.1.ee25d55 1.20.9+vmware.1-tkg.1.a4cee5b&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
v1.20.2---vmware.1-tkg.2.3e10706    1.20.2+vmware.1-tkg.2.3e10706    True    True         6m33s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.20.9+vmware.1-tkg.1.a4cee5b&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
v1.20.7---vmware.1-tkg.1.7fb9067    1.20.7+vmware.1-tkg.1.7fb9067    True    True         6m24s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.21.2+vmware.1-tkg.1.ee25d55 1.20.9+vmware.1-tkg.1.a4cee5b&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
v1.20.8---vmware.1-tkg.2            1.20.8+vmware.1-tkg.2            True    True         6m25s
v1.20.9---vmware.1-tkg.1.a4cee5b    1.20.9+vmware.1-tkg.1.a4cee5b    True    True         6m31s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.21.2+vmware.1-tkg.1.ee25d55&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
v1.21.2---vmware.1-tkg.1.ee25d55    1.21.2+vmware.1-tkg.1.ee25d55    True    True         6m31s

kubectl get vmclass
NAME                  CPU   MEMORY   AGE
best-effort-2xlarge   &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;     64Gi     80s
best-effort-4xlarge   &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;    128Gi    80s
best-effort-8xlarge   &lt;span style=&#34;color:#ae81ff&#34;&gt;32&lt;/span&gt;    128Gi    80s
best-effort-large     &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;     16Gi     80s
best-effort-medium    &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;     8Gi      78s
best-effort-small     &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;     4Gi      80s
best-effort-xlarge    &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;     32Gi     80s
best-effort-xsmall    &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;     2Gi      77s
guaranteed-2xlarge    &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;     64Gi     79s
guaranteed-4xlarge    &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;    128Gi    74s

kubectl get storageclass -n hogwarts
NAME                          PROVISIONER              RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
vsan-default-storage-policy   csi.vsphere.vmware.com   Delete          Immediate           true                   19m
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Using the output from previous commands we can copy paste the appropriate storage class, vm class and Kuberneters version values from what is available in the namespace into a yaml file definition for the cluster.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;run.tanzu.vmware.com/v1alpha2&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TanzuKubernetesCluster&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hogwarts-cluster01&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hogwarts&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;topology&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;controlPlane&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;vmClass&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;best-effort-small&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;storageClass&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;vsan-default-storage-policy&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;tkr&lt;/span&gt;:  
        &lt;span style=&#34;color:#f92672&#34;&gt;reference&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1.21.2---vmware.1-tkg.1.ee25d55&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;nodePools&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;forbidden-forest&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;vmClass&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;best-effort-small&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;storageClass&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;vsan-default-storage-policy&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;tkr&lt;/span&gt;:  
        &lt;span style=&#34;color:#f92672&#34;&gt;reference&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1.21.2---vmware.1-tkg.1.ee25d55&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;settings&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;network&lt;/span&gt;:
        &lt;span style=&#34;color:#f92672&#34;&gt;cni&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;antrea&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;services&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;cidrBlocks&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;10.96.0.0/12&amp;#34;&lt;/span&gt;]
        &lt;span style=&#34;color:#f92672&#34;&gt;pods&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;cidrBlocks&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;172.16.128.0/17&amp;#34;&lt;/span&gt;]
        &lt;span style=&#34;color:#f92672&#34;&gt;serviceDomain&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hogwarts.local&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With all of this in place we can deploy the cluster,  we can check it has been deployed correctly.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f hogwarts-cls1.yml
tanzukubernetescluster.run.tanzu.vmware.com/hogwarts-cluster01 created

kubectl get TanzuKubernetesCluster
NAMESPACE   NAME                 CONTROL PLANE   WORKER   TKR NAME                           AGE   READY   TKR COMPATIBLE   UPDATES AVAILABLE
hogwarts    hogwarts-cluster01   &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;               &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;        v1.21.2---vmware.1-tkg.1.ee25d55   32m   True    True

kubectl get virtualmachines
NAME                                                         POWERSTATE   AGE
hogwarts-cluster01-control-plane-9tljd                       poweredOn    35m
hogwarts-cluster01-control-plane-cn89v                       poweredOn    37m
hogwarts-cluster01-control-plane-wfnjg                       poweredOn    41m
hogwarts-cluster01-forbidden-forest-hdqms-7b9bf6bb44-fqw2d   poweredOn    38m
hogwarts-cluster01-forbidden-forest-hdqms-7b9bf6bb44-n2r5s   poweredOn    38m
hogwarts-cluster01-forbidden-forest-hdqms-7b9bf6bb44-vbnnj   poweredOn    38m

kubectl get service
NAME                                               TYPE           CLUSTER-IP    EXTERNAL-IP   PORT&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;S&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;          AGE
service/hogwarts-cluster01-control-plane-service   LoadBalancer   10.96.0.231   172.16.1.3    6443:32176/TCP   44m
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If I swing over to take a look what has happened in NSX-T a second segment has been added to the T1 router which was created during the supervisor cluster enablement. The segment has connected the three control plane and three node pool VMs. An NSX Load Balancer is deployed to represent the Kubernetes API service resource.  There are also NAT Rules to represent cluster Egress via 172.16.2.2 IP address. All of the created NSX entities have the namespace name in this case &amp;lsquo;hogwarts&amp;rsquo; in their description they are really easy to identify via search.&lt;/p&gt;
&lt;p&gt;So all looks good and can try and connect to the subordinate cluster and interact with it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl vsphere login --server&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;172.16.1.2 --tanzu-kubernetes-cluster-name hogwarts-cluster01 --tanzu-kubernetes-cluster-namespace hogwarts --vsphere-username hermione@vsphere.local --insecure-skip-tls-verify

Logged in successfully.
You have access to the following contexts:
   172.16.1.2
   hogsmead
   hogwarts
   hogwarts-cluster01
If the context you wish to use is not in this list, you may need to try logging in again later, or contact your cluster administrator.
To change context, use &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;kubectl config use-context &amp;lt;workload name&amp;gt;&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;

kubectl config use-context hogwarts-cluster01

kubectl get nodes
NAME                                                         STATUS   ROLES                  AGE   VERSION
hogwarts-cluster01-control-plane-9tljd                       Ready    control-plane,master   58m   v1.21.2+vmware.1
hogwarts-cluster01-control-plane-cn89v                       Ready    control-plane,master   61m   v1.21.2+vmware.1
hogwarts-cluster01-control-plane-wfnjg                       Ready    control-plane,master   64m   v1.21.2+vmware.1
hogwarts-cluster01-forbidden-forest-hdqms-7b9bf6bb44-fqw2d   Ready    &amp;lt;none&amp;gt;                 62m   v1.21.2+vmware.1
hogwarts-cluster01-forbidden-forest-hdqms-7b9bf6bb44-n2r5s   Ready    &amp;lt;none&amp;gt;                 62m   v1.21.2+vmware.1
hogwarts-cluster01-forbidden-forest-hdqms-7b9bf6bb44-vbnnj   Ready    &amp;lt;none&amp;gt;                 62m   v1.21.2+vmware.1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next step is to deploy first Pod into the cluster, but this got stuck ContainerCreating and Pod seems to show an Antrea socket error.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl run -i --tty busybox --image&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;quay.io/quay/busybox --restart&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Never -- sh

kubectl get pods
NAME      READY   STATUS              RESTARTS   AGE
busybox   0/1     ContainerCreating   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          3m12s

kubectl describe pod busybox
Events:
  Type     Reason                  Age                From               Message
  ----     ------                  ----               ----               -------
  Normal   Scheduled               3m55s              default-scheduler  Successfully assigned default/busybox to hogwarts-cluster01-forbidden-forest-8g2s2-66d6bfdd-8wzbr
  Warning  FailedCreatePodSandBox  3m52s              kubelet            Failed to create pod sandbox: rpc error: code &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Unknown desc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; failed to setup network &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; sandbox &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;354f63b2c94a0fb5a88c9611adb3008c7dd564deb157a39ebeca78f1b717316c&amp;#34;&lt;/span&gt;: rpc error: code &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Unavailable desc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; connection error: desc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;transport: Error while dialing dial unix /var/run/antrea/cni.sock: connect: connection refused&amp;#34;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So first thing to look at I guess is Antrea Pods and can see one is broken.  Checking logs and it looks like OVS timed out on startup.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl get pods -A
NAMESPACE                      NAME                                                                       READY   STATUS              RESTARTS   AGE
default                        busybox                                                                    0/1     ContainerCreating   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          12m
kube-system                    antrea-agent-8pf5t                                                         0/2     CrashLoopBackOff    &lt;span style=&#34;color:#ae81ff&#34;&gt;26&lt;/span&gt;         96m
kube-system                    antrea-agent-bv9rq                                                         2/2     Running             &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;          102m
kube-system                    antrea-agent-cth2b                                                         2/2     Running             &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          102m
kube-system                    antrea-agent-p6qfq                                                         2/2     Running             &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;          96m
kube-system                    antrea-agent-wh8k4                                                         2/2     Running             &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          91m
kube-system                    antrea-agent-wsrtg                                                         2/2     Running             &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          99m

kubectl logs antrea-agent-8pf5t -n kube-system antrea-ovs
&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1;34m2021-12-03T20:08:00Z &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;0;32mINFO &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;0;36mantrea-ovs&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;0m&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;: Starting ovsdb-server
Starting ovsdb-server.
2021-12-03T20:09:05Z|00002|timeval|WARN|Unreasonably long 10026ms poll interval &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;7883ms user, 82ms system&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
2021-12-03T20:09:05Z|00003|timeval|WARN|faults: &lt;span style=&#34;color:#ae81ff&#34;&gt;57&lt;/span&gt; minor, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; major
2021-12-03T20:09:05Z|00004|timeval|WARN|context switches: &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; voluntary, &lt;span style=&#34;color:#ae81ff&#34;&gt;9309&lt;/span&gt; involuntary
2021-12-03T20:09:17Z|00002|timeval|WARN|Unreasonably long 8797ms poll interval &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;7858ms user, 89ms system&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
2021-12-03T20:09:17Z|00003|timeval|WARN|faults: &lt;span style=&#34;color:#ae81ff&#34;&gt;60&lt;/span&gt; minor, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; major
2021-12-03T20:09:17Z|00004|timeval|WARN|context switches: &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; voluntary, &lt;span style=&#34;color:#ae81ff&#34;&gt;9264&lt;/span&gt; involuntary
Configuring Open vSwitch system IDs.
/usr/share/openvswitch/scripts/ovs-ctl: line 42: hostname: command not found
Enabling remote OVSDB managers.
&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1;34m2021-12-03T20:09:19Z &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;0;32mINFO &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;0;36mantrea-ovs&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;0m&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;: Stopping OVS before quit
&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1;34m2021-12-03T20:09:19Z &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;0;32mINFO &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;0;36mantrea-ovs&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;0m&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;: Stopping OVS
ovs-vswitchd is not running.
2021-12-03T20:09:21Z|00001|fatal_signal|WARN|terminating with signal &lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;Alarm clock&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As this was a timeout and that the cluster is very easy to remove and redeploy I tried this and with exact same configuration this issue resolved. And we can finally run our pod and run some commands!&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl config use-context hogwarts
kubectl delete -f hogwarts-cls1b.yml
kubectl apply -f hogwarts-cls1b.yml

kubectl vsphere login --server&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;172.16.1.2 --tanzu-kubernetes-cluster-name hogwarts-cluster01 --tanzu-kubernetes-cluster-namespace hogwarts --vsphere-username hermione@vsphere.local --insecure-skip-tls-verify
Logged in successfully.
You have access to the following contexts:
   172.16.1.2
   hogsmead
   hogwarts
   hogwarts-cluster01
If the context you wish to use is not in this list, you may need to try logging in again later, or contact your cluster administrator.
To change context, use &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;kubectl config use-context &amp;lt;workload name&amp;gt;&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;

kubectl config use-context hogwarts-cluster01

kubectl run -i --tty busybox --image&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;quay.io/quay/busybox --restart&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Never -- sh
If you don&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&amp;#39;&lt;/span&gt;t see a command prompt, try pressing enter.
/ &lt;span style=&#34;color:#75715e&#34;&gt;# nslookup bbc.co.uk&lt;/span&gt;
Server:         10.96.0.10
Address:        10.96.0.10:53

Non-authoritative answer:
Name:	bbc.co.uk
Address: 151.101.128.81
Name:	bbc.co.uk
Address: 151.101.0.81
Name:	bbc.co.uk
Address: 151.101.192.81
Name:	bbc.co.uk
Address: 151.101.64.81

/ &lt;span style=&#34;color:#75715e&#34;&gt;#&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
        
      </description>
    </item>
    
    <item>
      <title>Deploying Tanzu for vSphere with NSX-T</title>
      <link>https://darrylcauldwell.github.io/post/tanzu-basic-nsx/</link>
      <pubDate>Mon, 15 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/tanzu-basic-nsx/</guid>
      <description>
        
          &lt;p&gt;First in a series of posts which build on each other looking Tanzu.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://darrylcauldwell.github.io/post/tanzu-basic-nsx&#34;&gt;Deploying Tanzu for vSphere with NSX-T&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://darrylcauldwell.github.io/post/tanzu-tkc&#34;&gt;Managing Tanzu Kubernetes Clusters Using ClusterAPI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://darrylcauldwell.github.io/post/tanzu-tmc&#34;&gt;Managing Tanzu Kubernetes Clusters Using Misson Control&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;supervisor-cluster-bootstrap&#34;&gt;Supervisor Cluster Bootstrap&lt;/h2&gt;
&lt;p&gt;The enablement of Workload Control Plane triggers the deployment of three VMs which initially have single vNIC eth0 connected to a PortGroup which can route to vCenter and NSX. This appears to configure the first VM as bootstrap Kubernetes Node on which are deployed multiple Pods. The NSX Container Plug-in (NCP) Pod performs integration of Kubernetes and NSX-T.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl get pods -n vmware-system-nsx
NAME                       READY   STATUS    RESTARTS   AGE
nsx-ncp-7c6578d9fd-xj6jk   1/1     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;          25h

During the bootstrap of the Supervisor Cluster the requires NSX objects are created by the NCP Pod.  The NCP Pod is configured with a ConfigMap which includes details of how to connect to the NSX API.

&lt;span style=&#34;color:#e6db74&#34;&gt;```&lt;/span&gt;bash
kubectl get pods -A | grep ncp
vmware-system-nsx                           nsx-ncp-7c6578d9fd-xdqvl                                          1/1     Running     &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;          2d22h

kubectl describe pod nsx-ncp-7c6578d9fd-xdqvl -n vmware-system-nsx | grep config
ConfigMapName:       nsx-ncp-config

kubectl describe ConfigMap nsx-ncp-config -n vmware-system-nsx | grep manager
nsx_api_managers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 192.168.10.28:443
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;routing-domains&#34;&gt;Routing Domains&lt;/h2&gt;
&lt;p&gt;The Workload Control Plane configures various components on discreet network segments. The Management, Ingress and Egress networks all need to be part of a common routing. In this deployment the Managemenet network operates within a VLAN defined on the physical network. The Workload Control Plane deployment creates the Ingress and Egress networks as NSX-T logical segments. To faciliate the correct sharing of routes the NSX-T Tier-0 router is configured as a BGP Autonomous System which peers with the physical router, both advertise and share routes.&lt;/p&gt;
&lt;h2 id=&#34;no-nat-topology&#34;&gt;No-NAT Topology&lt;/h2&gt;
&lt;p&gt;It is possible [from vCenter Server 7.0 Update 3] to deploy a No-NAT (routed) topology which allows routing outside of the cluster network, more info on No-NAT &lt;a href=&#34;https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-2BC5CC9D-7396-4700-A698-3C97A882AE23.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;nat-topology&#34;&gt;NAT Topology&lt;/h2&gt;
&lt;p&gt;In this deployment I deployed the default NAT (routed) topology where the Kubernetes POD and Service are within the same routing domain.  Communications to the wider network is facilitated by combination of NAT, Load Balancer Ingress and Egress.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-basic-nsx-nat.drawio.png&#34; alt=&#34;Tanzu Basic with NSX-T NAT&#34;&gt;&lt;/p&gt;
&lt;p&gt;A NSX-T logical segment is created for ther Kubernetes Cluster network ( 10.244.0.0 /20 ) which all the Nodes and Pods will attach.&lt;/p&gt;
&lt;p&gt;NSX-T Service Load Balancers get created which correspond to the Ingress resources:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Pod&lt;/th&gt;
&lt;th&gt;Ingress IP Allocation&lt;/th&gt;
&lt;th&gt;Port&lt;/th&gt;
&lt;th&gt;Protocol&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;vsphere-csi-controller&lt;/td&gt;
&lt;td&gt;172.16.1.1&lt;/td&gt;
&lt;td&gt;2112&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vsphere-csi-controller&lt;/td&gt;
&lt;td&gt;172.16.1.1&lt;/td&gt;
&lt;td&gt;2113&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-apiserver&lt;/td&gt;
&lt;td&gt;172.16.1.2&lt;/td&gt;
&lt;td&gt;6443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-apiserver&lt;/td&gt;
&lt;td&gt;172.16.1.2&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;NSX-T Distributed Load Balancers get created which correspond to the ClusterIP service resources:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Pod&lt;/th&gt;
&lt;th&gt;Ingress IP Allocation&lt;/th&gt;
&lt;th&gt;Port&lt;/th&gt;
&lt;th&gt;Protocol&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;kubernetes&lt;/td&gt;
&lt;td&gt;10.96.0.1&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-dns&lt;/td&gt;
&lt;td&gt;10.96.0.10&lt;/td&gt;
&lt;td&gt;53&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-dns&lt;/td&gt;
&lt;td&gt;10.96.0.10&lt;/td&gt;
&lt;td&gt;53&lt;/td&gt;
&lt;td&gt;L4 UDP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-dns&lt;/td&gt;
&lt;td&gt;10.96.0.10&lt;/td&gt;
&lt;td&gt;9153&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tkg-vmware-system-tkg-webhook-service&lt;/td&gt;
&lt;td&gt;10.96.0.57&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vmop-webhook-service&lt;/td&gt;
&lt;td&gt;10.96.0.70&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;capw-capw-controller-manager-metrics-service&lt;/td&gt;
&lt;td&gt;10.96.0.72&lt;/td&gt;
&lt;td&gt;9846&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;capw-capw-controller-webhook-service&lt;/td&gt;
&lt;td&gt;10.96.0.168&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-apiserver-lb-svc&lt;/td&gt;
&lt;td&gt;10.96.0.186&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-apiserver-lb-svc&lt;/td&gt;
&lt;td&gt;10.96.0.186&lt;/td&gt;
&lt;td&gt;6443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tkg-vmware-system-tkg-controller-manager-metrics-service&lt;/td&gt;
&lt;td&gt;10.96.0.202&lt;/td&gt;
&lt;td&gt;9847&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vmop-controller-manager-metrics-service&lt;/td&gt;
&lt;td&gt;10.96.0.204&lt;/td&gt;
&lt;td&gt;9848&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;capw-capi-kubeadm-bootstrap-webhook-service&lt;/td&gt;
&lt;td&gt;10.96.1.44&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;capw-capi-controller-manager-metrics-service&lt;/td&gt;
&lt;td&gt;10.96.1.45&lt;/td&gt;
&lt;td&gt;9844&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nsop-vmware-system-nsop-webhook-service&lt;/td&gt;
&lt;td&gt;10.96.1.87&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;capw-capi-kubeadm-control-plane-controller-manager-metrics-service&lt;/td&gt;
&lt;td&gt;10.96.1.95&lt;/td&gt;
&lt;td&gt;9848&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vmware-system-license-operator-webhook-service&lt;/td&gt;
&lt;td&gt;10.96.1.112&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;capw-capi-kubeadm-bootstrap-controller-manager-metrics-service&lt;/td&gt;
&lt;td&gt;10.96.1.136&lt;/td&gt;
&lt;td&gt;9845&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;cert-manager-cert-manager&lt;/td&gt;
&lt;td&gt;10.96.1.149&lt;/td&gt;
&lt;td&gt;9402&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;psp-operator-webhook-service&lt;/td&gt;
&lt;td&gt;10.96.1.150&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;capw-capi-webhook-service&lt;/td&gt;
&lt;td&gt;10.96.1.163&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;docker-registry&lt;/td&gt;
&lt;td&gt;10.96.1.170&lt;/td&gt;
&lt;td&gt;5000&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;apiserver-authproxy&lt;/td&gt;
&lt;td&gt;10.96.1.173&lt;/td&gt;
&lt;td&gt;8443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;csi-vsphere-csi-controller&lt;/td&gt;
&lt;td&gt;10.96.1.208&lt;/td&gt;
&lt;td&gt;2112&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;csi-vsphere-csi-controller&lt;/td&gt;
&lt;td&gt;10.96.1.208&lt;/td&gt;
&lt;td&gt;2113&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;capw-capi-kubeadm-control-plane-webhook-service&lt;/td&gt;
&lt;td&gt;10.96.0.235&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tkg-tkgs-plugin-service&lt;/td&gt;
&lt;td&gt;10.96.1.245&lt;/td&gt;
&lt;td&gt;8099&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;cert-manager-cert-manager-webhook&lt;/td&gt;
&lt;td&gt;10.96.1.252&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The following three NAT rules are put in place to facilitate Egress with NSX-T:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Action&lt;/th&gt;
&lt;th&gt;Source&lt;/th&gt;
&lt;th&gt;Destination&lt;/th&gt;
&lt;th&gt;Translation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SNAT&lt;/td&gt;
&lt;td&gt;Any&lt;/td&gt;
&lt;td&gt;Any&lt;/td&gt;
&lt;td&gt;172.16.2.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No SNAT&lt;/td&gt;
&lt;td&gt;10.244.0.0/20&lt;/td&gt;
&lt;td&gt;10.244.0.0/20&lt;/td&gt;
&lt;td&gt;Any&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No SNAT&lt;/td&gt;
&lt;td&gt;10.244.0.0/20&lt;/td&gt;
&lt;td&gt;172.16.2.0/24&lt;/td&gt;
&lt;td&gt;Any&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;supervisor-cluster-dns-complexities&#34;&gt;Supervisor Cluster DNS Complexities&lt;/h2&gt;
&lt;p&gt;At this stage the VMs have a single NIC eth0 and management DNS resolver.&lt;/p&gt;
&lt;p&gt;Once the NCP has created the required NSX objects the bootstrap node has a 2nd NIC added eth1 which is connected to the Cluster Network segment. This enables configuration to proceed on the bootstrap host after which the 2nd and 3rd Nodes are configured and have 2nd NIC added onnected to the Cluster Network segment.&lt;/p&gt;
&lt;p&gt;I initially had issue where the installation status got stuck in “configuring” state. When I SSH to the first control plane VM I saw various error messages with various components. For example authorization errors were due to the kube-apiserver not being able to resolve the vCenter’s fqdn for SSO.  I had the NCP Pod moving to CrashLoopBackoff status again with what looked like fqdn lookup issues. This was confusing as during the earlier stages of the bootstrap process the bootstrap node had connected to vCenter and NSX using Management DNS resolver.&lt;/p&gt;
&lt;p&gt;When working through the WCP installation UI there are two places to enter DNS server IP address.  The first is a DNS resolver with A and PTR records for the management components.  The Kubernetes CoreDNS performs lookups between Pods and Services within the Cluster. The second DNS entry what CoreDNS falls back to for FQDN lookups external of the Cluster. In my environment I have a single DNS solution which hosts both management and workload zones so the IP address (192.168.10.10) is common.&lt;/p&gt;
&lt;p&gt;When I connect to first control plane VM I noticed I could ping DNS server via its management interface but could not ping DNS via workload interface.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;ifconfig
eth0      Link encap:Ethernet  HWaddr 00:50:56:b3:29:f3
          inet addr:192.168.10.45  Bcast:192.168.10.255  Mask:255.255.255.0
...
eth1      Link encap:Ethernet  HWaddr 04:50:56:00:d0:00
          inet addr:10.244.0.2  Bcast:10.244.0.15  Mask:255.255.255.240
...

ping 192.168.10.10 -I eth0 -c &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
PING 192.168.10.10 &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;192.168.10.10&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; from 192.168.10.45 eth0: 56&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;84&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; bytes of data.
&lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt; bytes from 192.168.10.10: icmp_seq&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; ttl&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt; time&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0.412 ms

--- 192.168.10.10 ping statistics ---
&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; packets transmitted, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; received, 0% packet loss, time 0ms
rtt min/avg/max/mdev &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 0.412/0.412/0.412/0.000 ms

ping 192.168.10.10 -I eth1 -c &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
PING 192.168.10.10 &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;192.168.10.10&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; from 10.244.0.2 eth1: 56&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;84&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; bytes of data.
From 10.244.0.1 icmp_seq&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; Destination Host Unreachable

--- 192.168.10.10 ping statistics ---
&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; packets transmitted, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; received, +1 errors, 100% packet loss, time 0ms
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So here I know I have a routing or Egress issue from workload to management routing domain. I would expect however the VM should still resolve vCenter and NSX-T records on management DNS and bootstrapping should comlete. I checked the network config to ensure that eth0 had correct DNS server IP and the routing table appeared correct.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat /etc/systemd/network/10-eth0.network | grep DNS
DNS &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 192.168.10.10
cat /etc/systemd/network/10-eth1.network | grep DNS
DNS &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 10.244.0.2

route -n
Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.10.254  0.0.0.0         UG    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;      &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; eth0
10.244.0.0      0.0.0.0         255.255.255.240 U     &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;      &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; eth1
192.168.10.0    0.0.0.0         255.255.255.0   U     &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;      &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; eth0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Looking deeper at routing policy redirects traffic to 192.168.10.10 via eth1.  As we know routing from eth1 was to DNS was not possible this causes the DNS issue.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ip rule show
0:      from all lookup local
0:      from all to 192.168.10.10  lookup 200
0:      from all to 172.16.1.0 /24 lookup 200
0:      from all to 10.244.0.0 /20 lookup 200
0:      from 10.244.0.2 lookup 200
0:      from all to 100.64.0.0 /16 lookup 200
0:      from all to 10.96.0.0 /23 lookup 200
32766:  from all lookup main
32767:  from all lookup default

ip route show table 200
default via 10.244.0.1 dev eth1 proto static
10.244.0.0/28 dev eth1 proto static scope link
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Interestingly if I remove Workload Management and then redeploy specifing Management DNS 192.168.10.10 but chaning Workload DNS to alternate IP but still on unroutable address 192.168.10.11. The deployment does not get stuck at the same place and completes successfully.  Looking at routing policy having unique DNS IP addressing does not force traffic to 192.168.10.10 via eth1.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;ip rule show
0:      from all lookup local
0:      from all to 172.16.1.0 /24 lookup &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;
0:      from all to 10.244.0.0 /20 lookup &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;
0:      from all to 10.96.0.0 /23 lookup &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;
0:      from 10.244.0.2 lookup &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;
0:      from all to 100.64.0.0 /16 lookup &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;
0:      from all to 192.168.10.11  lookup &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;
32766:  from all lookup main
32767:  from all lookup default

ip route show table &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;
default via 10.244.0.1 dev eth1 proto static
10.244.0.0/28 dev eth1 proto static scope link
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;workload-cluster-dns-traffic&#34;&gt;Workload Cluster DNS Traffic&lt;/h2&gt;
&lt;p&gt;While useful to understand if the workloa cluster CoreDNS cannot route to the configured upstream DNS server it will cause an issue later so the unerlying cause needs resolving.  For me the issue was the DNS server was dual homed and required static route adding for 172.16.0.0/16 to redirect to the router which was BGP peered to the NSX Edge.&lt;/p&gt;
&lt;h2 id=&#34;kubectl-vsphere-login-traffic&#34;&gt;Kubectl vSphere Login Traffic&lt;/h2&gt;
&lt;p&gt;To connect to the Supervisor Cluster we connect to the LoadBalancer VIP representation of Kubernetes API ( 172.16.1.2 ).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;set KUBECTL_VSPHERE_PASSWORD&lt;span style=&#34;color:#f92672&#34;&gt;={&lt;/span&gt;my password&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
kubectl vsphere login --server&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;172.16.1.2 --vsphere-username administrator@vsphere.local --insecure-skip-tls-verify --verbose &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;
kubectl config use-context 172.16.1.2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
        
      </description>
    </item>
    
    <item>
      <title>vSphere with Kubernetes Homelab Build</title>
      <link>https://darrylcauldwell.github.io/post/vsphere-k8s/</link>
      <pubDate>Wed, 29 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/vsphere-k8s/</guid>
      <description>
        
          &lt;p&gt;I&amp;rsquo;d been exploring Project Pacific during its beta in my homelab for a while. When vSphere 7 and NSX-T 3.0 went GA I took chance to rebuild lab and consolidate much of the configuration I&amp;rsquo;d been applying iteratively.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-vsphere.png&#34; alt=&#34;vSphere with Tanzu&#34;&gt;&lt;/p&gt;
&lt;p&gt;My lab hardware specification is a constraint so I&amp;rsquo;ve had to deviate from documentation in a few areas of configuration. During stand up and power up hosts experience CPU and RAM pressure but once everything is running in steady state it is tight but just fits.&lt;/p&gt;
&lt;h2 id=&#34;single-vlan--subnet-lab-network&#34;&gt;Single VLAN / subnet Lab Network&lt;/h2&gt;
&lt;p&gt;My lab has a very simple physical network namely a single subnet (192.168.1.0/24) with DHCP enabled and which has default route to the internet.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Host&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Allocation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ad&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;esx1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;esx2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;esx3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vcenter&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nsx&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;edge&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;t0-uplink&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tep-pool&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.20-24&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;workload control plane&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.30-34&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kubernetes Ingress&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.48-63&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kubernetes Egress&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.64-79&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DHCP&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.64-253&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gateway&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.254&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I look to configure subnets on the NSX overlay network using the other RFC1918 private address range. The physical router does not support dynamic routing protocol so I configure static routes for these two CIDR with next hop as tier-0 uplink IP 192.168.1.17.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;CIDR&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;IP Range&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;10.0.0.0/8&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;10.0.0.0 – 10.255.255.255&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;172.16.0.0/12&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;172.16.0.0 – 172.31.255.255&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;compute-resource-nodes&#34;&gt;Compute Resource Nodes&lt;/h2&gt;
&lt;p&gt;Lab compute resources are provided by three &lt;a href=&#34;https://ark.intel.com/content/www/us/en/ark/products/89190/intel-nuc-kit-nuc6i5syh.html&#34;&gt;Intel NUC6i5SYH&lt;/a&gt; hardware, each provides 2x 1.8Ghz CPU and 32GB RAM. I install ESXi boot partition on USB drives. To create a bootable USB for ESXi on macOS by creating a new VM in VMware Fusion with ESXi ISO attached as CD/DVD. It is only possible to connect USB to a running VM so power on VM and connect USB drive, work through ESXi installation via Fusion console.&lt;/p&gt;
&lt;p&gt;At the end of installation message to disconnect the CD/DVD and reboot VM.  I remain connected for reboot, using remote console navigate Troubleshoot menu to enable ESXi Shall and SSH, then configure networking to reflect homelab IP allocation.&lt;/p&gt;
&lt;h2 id=&#34;ensure-unique-system-uuid-and-mac-address&#34;&gt;Ensure unique System UUID and MAC address&lt;/h2&gt;
&lt;p&gt;Once ESXi has a basic network configuration I power down the VM and move USB to the Intel NUC and power on. During installation of ESXi the System UUID and MAC address are formed from the MAC address of the NIC. When  using Fusion to create multiple ESXi VMs this scenario can lead to duplicatation of System UUID and MAC address. We can configure ESXi to move to physical NIC MAC address and form new ESXi System UUID by running commands like.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;esxcli system settings advanced set -o /Net/FollowHardwareMac -i 1
sed -i &#39;s#/system/uuid.*##&#39; /etc/vmware/esx.conf
/sbin/auto-backup.sh
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;add-external-usb-nic-drivers&#34;&gt;Add external USB NIC drivers&lt;/h2&gt;
&lt;p&gt;The Intel NUC has a single onboard 1GB NIC, I add an external USB NIC to each NUC. ESXi doesn&amp;rsquo;t ship with required drivers for these external USB NIC, for labs these are provides via &lt;a href=&#34;https://flings.vmware.com/usb-network-native-driver-for-esxi&#34;&gt;VMware Flings&lt;/a&gt;. I copy the downloaded driver bundle to the /tmp folder on ESXi via an SFTP client like CyberDuck. Once the bundle is uploaded I following command to install the bundle.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;esxcli software vib install -d /tmp/ESXi700-VMKUSB-NIC-FLING-34491022-component-15873236.zip
esxcli system maintenanceMode set --enable true
esxcli system shutdown reboot --reason &amp;quot;USB NIC driver&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;suppress-warnings&#34;&gt;Suppress Warnings&lt;/h2&gt;
&lt;p&gt;When booting ESXi from USB the system logs and coredumps can not persist to local storage. I also prefer to leave SSH enabled in lab so get warnings. Both of these choices gives me warnings which I prefer to suppress.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim-cmd hostsvc/advopt/update UserVars.SuppressShellWarning long 1
vim-cmd hostsvc/advopt/update UserVars.SuppressCoredumpWarning long 1
vim-cmd hostsvc/advopt/update Syslog.global.logHost string 127.0.0.1
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;form-vsan-cluster&#34;&gt;Form vSAN Cluster&lt;/h2&gt;
&lt;p&gt;Two of the Intel NUC I use each have SSD the third is used to provide compute resources only. The ESXi installation allows Management traffic only on VMkernel port so need to bind vSAN to VMkernel port. In my lab only two devices contribute disk to vSAN the default storage policy for will prevent any VM deployment. To configure this I run the following on each host in cluster.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;esxcli system maintenanceMode set --enable false
esxcli vsan network ip add -i vmk0
esxcli vsan policy setdefault -c cluster -p &amp;quot;((\&amp;quot;hostFailuresToTolerate\&amp;quot; i0)&amp;quot;
esxcli vsan policy setdefault -c vdisk -p &amp;quot;((\&amp;quot;hostFailuresToTolerate\&amp;quot; i0)&amp;quot;
esxcli vsan policy setdefault -c vmnamespace -p &amp;quot;((\&amp;quot;hostFailuresToTolerate\&amp;quot; i0)&amp;quot;
esxcli vsan policy setdefault -c vmswap -p &amp;quot;((\&amp;quot;hostFailuresToTolerate\&amp;quot; i0) (\&amp;quot;forceProvisioning\&amp;quot; i1))&amp;quot;
esxcli vsan policy setdefault -c vmem -p &amp;quot;((\&amp;quot;hostFailuresToTolerate\&amp;quot; i0) (\&amp;quot;forceProvisioning\&amp;quot; i1))&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With the pre-requists in place a new vSAN cluster can be formed on first host using command like.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;esxcli vsan cluster new 
esxcli vsan cluster get | grep &amp;quot;Sub-Cluster UUID&amp;quot;
    Sub-Cluster UUID: 5291783f-77a6-c1dc-2ed8-6cfc200618b1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Add other nodes to the cluster using output of Sub-Cluster Master UUID using command like.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;esxcli vsan cluster join -u 5291783f-77a6-c1dc-2ed8-6cfc200618b1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;At this stage vSAN cluster while nodes are looking to form the quorum cannot be formed. Prior to vSAN 6.6 the cluster can discover members using multicast. Forming cluster without vCenter requires formation of unicast networking from the command line by manually building table &lt;a href=&#34;https://kb.vmware.com/s/article/2150303&#34;&gt;kb2150303&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;active-directory&#34;&gt;Active Directory&lt;/h2&gt;
&lt;p&gt;At this stage I upload Windows Server 2019 ISO to vSAN and use this to deploy a VM which acts as a environment Remote Desktop, File Store, DNS server, NTP source and Active Directory. Before proceeding create DNS A &amp;amp; PTR records for the environment.&lt;/p&gt;
&lt;h2 id=&#34;vcenter-server&#34;&gt;vCenter Server&lt;/h2&gt;
&lt;p&gt;I use the Active Directory jump server to mount the vCenter ISO and use UI installer to deploy a &amp;lsquo;Tiny&amp;rsquo; sized vCenter Server to existing Datastore. This creates a cluster with the move deployed to, at this stage I attach remaining hosts to cluster.&lt;/p&gt;
&lt;p&gt;We can then pass ownership of vSAN cluster membership back to vCenter by running following on each host.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;esxcfg-advcfg -s 0 /VSAN/IgnoreClusterMemberListupdates
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The Workload Control Plane feature requires DRS and HA are required to be enabled on clusters it manages. Ensure these are enabled and HA admission control is disabled to maintain capacity.&lt;/p&gt;
&lt;h2 id=&#34;default-vsan-storage-policy&#34;&gt;Default vSAN Storage Policy&lt;/h2&gt;
&lt;p&gt;During the setup of VSAN cluster we configured the local policies to FTT=0 as I only have 2x hosts providing disk capacity. To deploy VMs and appliances via vCenter similarly need to adjust the &amp;lsquo;vSAN Default Storage Policy&amp;rsquo; to &amp;lsquo;No data redundancy&amp;rsquo;.&lt;/p&gt;
&lt;h2 id=&#34;nsx-installation&#34;&gt;NSX Installation&lt;/h2&gt;
&lt;p&gt;Deploy NSX-T 3.0 appliance sized Small, I need to remove CPU reservation to power on.&lt;/p&gt;
&lt;h2 id=&#34;connect-nsx-and-vcenter&#34;&gt;Connect NSX and vCenter&lt;/h2&gt;
&lt;p&gt;Connect to NSX appliance navigate to System &amp;gt; Fabric &amp;gt; Compute Managers and create bindng to vCenter. In order vCenter Workload Platform services can communicate with NSX ensure Enable Trust option is checked.&lt;/p&gt;
&lt;h2 id=&#34;vsphere-distributed-switch&#34;&gt;vSphere Distributed Switch&lt;/h2&gt;
&lt;p&gt;Prior to release of vSphere 7 and NSX-T 3.0 to install NSX required the creation of a N-VDS host switch on each ESXi host. While segments created on N-VDS where visible in vSphere they were not as rich as VDS. It is a constraint that physical NIC uplinks cannot be assigned to both a VDS and N-VDS.&lt;/p&gt;
&lt;p&gt;It was possible to have hosts with two pNIC which initially get built as VDS and then migrate to N-VDS and remove VDS. When running only with N-VDS the segments show in vCenter as Opaque networks. Not many 3rd party products support Opaque networks, automation became more complex as the network could not be correctly gathered via vCenter. Many production deployments moved to hosts with four pNIC two assigned to N-VDS and two assigned to VDS to hold the VMkernel ports.&lt;/p&gt;
&lt;p&gt;With these latest product versions the VDS and N-VDS capabilities converge to singluar VDS construct for use as host switch on ESXi the N-VDS remains for Edge and non-ESXi Transport Node types. The converged VDS for ESXi improves pNIC design and also makes operational management much easier as there is a singluar place for configuring NIOC, MTU and LLDP configuration.&lt;/p&gt;
&lt;p&gt;The lab hosts have two pNIC I leave onboard to vSphere Standard Switch with VMkernel ports attached. I create a VDS Version 7 configured with 1 uplink,  NIOS enabled but without default port group. The default VDS is created with MTU 1500, to support NSX I increase this to Advanced configuration option for MTU 1700.&lt;/p&gt;
&lt;p&gt;Once VDS is created I add usb0 NIC from all cluster hosts to be assigned to Uplink1.  I do not migrate VMkernel or VM networking to VDS.&lt;/p&gt;
&lt;h2 id=&#34;create-a-tunnel-endpoint-ip-address-pool&#34;&gt;Create a Tunnel Endpoint IP Address Pool&lt;/h2&gt;
&lt;p&gt;System &amp;gt; Networking &amp;gt; IP Management &amp;gt; IP Address Pools &amp;gt; Add IP Address Pool&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:             tep-pool
IP Range:         192.168.1.20-192.168.1.24
CIDR:             192.168.1.0/24
Gateway IP:       192.168.1.254
DNS Server:       192.168.1.10
DNS Suffix:       darrylcauldwell.com
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;configure-esxi-hosts-as-transport-nodes&#34;&gt;Configure ESXi hosts as Transport Nodes&lt;/h2&gt;
&lt;p&gt;To consistently configure ESXi hosts as NSX Transport Nodes I create a Transport Node Profile.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name                           Host Transport Node Profile
Host Switch - Type             VDS
Host Switch - Mode             Standard
Host Switch - Name             vCenter \ DSwitch
Host Switch - Transport Zone   nsx-overlay-transportzone &amp;amp; nsx-vlan-transportzone
Host Switch - Uplink Profile   nsx-default-uplink-hostswitch-profile
Host Switch - IP Assignment    IP Pool - tep-pool
Host Switch - Teaming Policy   uplink1 (active)= Uplink 1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once created using Host Transport Nodes menu select cluster and apply profile. Once completed it is possible to view the NSX components installed on all ESXi hosts using:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;esxcli software vib list | grep nsx
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;configure-nsx-edge&#34;&gt;Configure NSX Edge&lt;/h2&gt;
&lt;p&gt;The NSX Edge provides a Layer 3 routing capability the NSX Container Plugin requires at least a medium sized Edge to deploy a small loadbalancer.&lt;/p&gt;
&lt;p&gt;Deploy a medium sized Edge node&lt;/p&gt;
&lt;p&gt;System &amp;gt; Fabric &amp;gt;  Nodes &amp;gt; Edge Transport Nodes &amp;gt; Add Edge VM&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:                   nsxEdge
FQDN:                   edge
Size:                   Large
Shares:                 Normal
Memory Reservation:     0
IP Assignment:          Static
Management Interface:   VM Network
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Node Switch&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:                       nvds1
Transport Zone:             nsx-overlay-transportzone
Uplink Profile:             nsx-edge-single-nic-uplink-profile
IP Pool:                    tep-pool
DPDK Fastpath Interfaces:   VM Network

Name:                       nvds2
Transport Zone:             nsx-vlan-transportzone
Uplink Profile:             nsx-edge-single-nic-uplink-profile
DPDK Fastpath Interfaces:   VM Network
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;configure-edge-cluster&#34;&gt;Configure Edge Cluster&lt;/h2&gt;
&lt;p&gt;When Edge has fully deployed create Edge Cluster&lt;/p&gt;
&lt;p&gt;System &amp;gt; Fabric &amp;gt; Nodes &amp;gt; Edge Clusters &amp;gt; Add&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:                       edgeCluster
Transport Nodes:            nsxEdge
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;uplink-network-segment&#34;&gt;Uplink Network Segment&lt;/h2&gt;
&lt;p&gt;A network segment is required to to connect Tier0 router uplink to VLAN&lt;/p&gt;
&lt;p&gt;Networking &amp;gt; Segments &amp;gt;  Add Segment&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Segment Name:               edgeUplink
Connectivity:               None
Transport Zone:             nsx-vlan-transportzone
VLAN:                       0
Multicast:                  Disabled
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;tier0-router&#34;&gt;Tier0 Router&lt;/h2&gt;
&lt;p&gt;A logical router is required I create one like:&lt;/p&gt;
&lt;p&gt;Networking &amp;gt; Tier-0 Gateways &amp;gt; Add Gateway &amp;gt; Tier0&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:                       tier0
HA Mode:                    Active-Standby
Failover:                   non preemptive
Edge Cluster:               edgeCluster
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Add Interface&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:                       tier0uplink
Type:                       External
IP Address:                 192.168.1.17/24
Connected To:               edgeUplink
Edge Node:                  nsxEdge
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once interface is added you can test it works as it will now be able to ping.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;ping 192.168.1.17 -c &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;enable-a-cluster-as-workload-control-plane-wcp&#34;&gt;Enable a cluster as Workload Control Plane (WCP)&lt;/h2&gt;
&lt;p&gt;With all of the pre-requists in place we can now use wizard to enable integrated Kubernetes.&lt;/p&gt;
&lt;p&gt;[EDIT 4th May] since initial publication I found another &lt;a href=&#34;https://www.virtuallyghetto.com/2020/04/deploying-a-minimal-vsphere-with-kubernetes-environment.html&#34;&gt;blog post&lt;/a&gt; around deploying minimal lab. By default when enabling Workload Control Plane this deploys a 3x VMs which form the Kubernetes supervisor cluster. This can be reduced to 2x by updating &lt;code&gt;/etc/vmware/wcp/wcpsvc.yaml&lt;/code&gt; on the VCSA and changing the minmasters and maxmasters properties from 3 to 2. Then restarting the wcp service &lt;code&gt;service-control --restart wcp&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Menu &amp;gt; Workload Management &amp;gt; Enable&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Select a Cluster&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If cluster does not show available check reason via GUI and or these vCenter logs for clues.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/var/log/vmware/wcp/wcpsvc.log
/var/log/vmware/wcp/nsxd.log
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Cluster Settings&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;Cluster Size:                  Tiny
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Network&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The management network refers to the control plane needs to communicate with vCenter and NSX.  The workload network will come from the RFC1918 addressable space. Enter network details&lt;/p&gt;
&lt;p&gt;Workload Control Plane Networking&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Network:                        VM Network
Starting IP Address:            192.168.1.30
Subnet Mask:                    255.255.255.0
Gateway:                        192.168.1.254
DNS Server:                     192.168.1.10
NTP Server:                     192.168.1.10
DNS Search Domains:             darrylcauldwell.com
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Workload Network&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vDS:                            DSwitch
Edge Cluster:                   edgeCluster
DNS Server:                     192.168.1.10
Pod CIDRs:                      10.244.0.0/21 (default)
Service CIDRS:                  10.96.0.0/24 (default)
Ingress CIDR:                   192.168.1.48/28
Egress CIDR:                    192.168.1.64/28
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Configure storage to use&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;Control Plane Node              vSAN Default Storage Policy
Ephemeral Disks                 vSAN Default Storage Policy
Image Cache                     vSAN Default Storage Policy
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;Review and confirm&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Once submitted we see in vCenter a resource pool named Namespaces and three virtual appliances named WcpAPIServerAgent each is allocated 2CPU and 8GB RAM.  An installation occurs on the three virtual appliances which,  these attempt to run before appliances deploy it is normal to see install failures during this phase.&lt;/p&gt;
&lt;p&gt;The progress messages available via UI aren&amp;rsquo;t superb its useful to SSH to VCSA and tail the log:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tail -f /var/log/vmware/wcp/wcpsvc.logtail -f /var/log/vmware/wcp/wcpsvc.log
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If all has goes to plan an hour and a half or so later and if all gone to plan.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-wcp.png&#34; alt=&#34;WCP Success&#34;&gt;&lt;/p&gt;
&lt;p&gt;From here I can begin to create and consume native Kubernetes namespaces and resources.&lt;/p&gt;

        
      </description>
    </item>
    
  </channel>
</rss>
