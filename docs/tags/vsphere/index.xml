<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>vsphere on </title>
    <link>https://darrylcauldwell.github.io/tags/vsphere/</link>
    <description>Recent content in vsphere on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Wed, 09 Jul 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://darrylcauldwell.github.io/tags/vsphere/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Automating vSphere Provisioning from Bare Metal to Cluster</title>
      <link>https://darrylcauldwell.github.io/post/ztp/</link>
      <pubDate>Wed, 09 Jul 2025 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/ztp/</guid>
      <description>
        
          &lt;p&gt;Most remote sites don’t come with an IT expert, and noone enjoys clicking through ESXi setup screens. That’s where zero touch provisioning comes in. In this post, I’ll show you how to fully automate ESXi installation using a Kickstart script, use VCF Orchestrator to spin up a site specific cluster and onboard the hosts—no keyboard or mouse required.&lt;/p&gt;
&lt;p&gt;But it’s not just about convenience. Automation brings consistency, reduces human error, and gives you a repeatable, auditable process—especially important if you’re working in regulated environments. Whether you’re scaling edge infrastructure or building out a new region, this approach ensures every host is deployed the same way, every time, without needing skilled hands on-site.&lt;/p&gt;
&lt;h2 id=&#34;http-boot&#34;&gt;HTTP Boot&lt;/h2&gt;
&lt;p&gt;When automating infrastructure provisioning, network booting is a key enabler—and there are a few options to choose from. PXE and iPXE have long been the go-to methods, but they rely on TFTP, which can be finicky to set up and troubleshoot, especially across subnets or in firewalled environments. UEFI HTTP Boot offers a simpler, more reliable alternative. It uses standard UEFI firmware to download bootloaders and installation files directly over HTTP, eliminating the need for a TFTP server entirely.&lt;/p&gt;
&lt;p&gt;Most modern server vendors—including Dell, HPE, Cisco, and others—support UEFI HTTP Boot and provide ways to configure it remotely through their management interfaces such as iDRAC, iLO, and UCS Manager. While the specific setup steps and interface details vary by vendor and firmware version, UEFI HTTP Boot is quickly becoming the standard choice for zero touch provisioning, delivering consistent and reliable network boot capabilities across a wide range of hardware platforms.&lt;/p&gt;
&lt;h3 id=&#34;setting-up-a-http-server-for-esxi-uefi-http-boot&#34;&gt;Setting Up A HTTP Server for ESXi UEFI HTTP Boot&lt;/h3&gt;
&lt;p&gt;To enable UEFI HTTP Boot, you need a web server to serve the ESXi installation files and Kickstart scripts. For this example, I built a simple Ubuntu server configured with a static IP using netplan. Once the Ubuntu server is up, install and start the Apache web server with these commands:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt update
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install apache2 -y
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo systemctl enable apache2
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo systemctl start apache2
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;After the service is running, you can verify it by opening your server’s IP address in a browser — you should see the default Apache welcome page.&lt;/p&gt;
&lt;p&gt;Next, create directories to host the ESXi installer files and Kickstart configurations:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo mkdir -p /var/www/html/esxi9
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo mkdir -p /var/www/html/kickstart
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo chown -R www-data:www-data /var/www/html/esxi9 /var/www/html/kickstart
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo chmod -R &lt;span style=&#34;color:#ae81ff&#34;&gt;755&lt;/span&gt; /var/www/html/esxi9 /var/www/html/kickstart
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With the folders ready, copy your ESXi 9 ISO to the Ubuntu server. To make the installation files accessible over HTTP, extract the ISO contents into the web directory. First, install the necessary tools to extract the ISO:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install genisoimage p7zip-full -y
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then extract the ESXi installer:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo 7z x /home/ubuntu/VMware-VMvisor-Installer-9.0.0.24755229.x86_64.iso -o/var/www/html/esxi9
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;UEFI HTTP Boot expects a file named mboot.efi, but the ESXi ISO provides it as BOOTX64.EFI. To avoid boot failures, copy it to the expected filename:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo cp /var/www/html/esxi9/EFI/BOOT/BOOTX64.EFI /var/www/html/esxi9/mboot.efi
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Your ESXi installer is now hosted and ready to be served over HTTP, laying the foundation for a fully automated, zero touch provisioning workflow.&lt;/p&gt;
&lt;h3 id=&#34;host-specific-provisioning-with-uefi-http-boot&#34;&gt;Host-Specific Provisioning with UEFI HTTP Boot&lt;/h3&gt;
&lt;p&gt;To make your ESXi deployment truly zero touch and environment-aware, it helps to tailor the provisioning flow for each individual host. A simple way to achieve this is by serving a unique boot.cfg file based on the server’s MAC address. This lets you assign static IPs, hostnames, and custom Kickstart scripts—without touching the console.&lt;/p&gt;
&lt;p&gt;This relies on a naming convention used by many bootloaders (also common in PXE environments). The MAC address must be lowercase, use hyphens between octets, and be prefixed with 01-. The 01- prefix represents the hardware type code for Ethernet, as defined by the ARP protocol (where 01 = Ethernet). For example a MAC address like 00:11:22:33:44:55:66 becomes:
01-00-11-22-33-44-55-66&lt;/p&gt;
&lt;p&gt;Create folder:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo mkdir -p /var/www/html/esxi9/01-&amp;lt;MAC address&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo chown -R www-data:www-data /var/www/html/esxi9/01-&amp;lt;MAC address&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo chmod -R &lt;span style=&#34;color:#ae81ff&#34;&gt;755&lt;/span&gt; /var/www/html/esxi9/01-&amp;lt;MAC address&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Inside that folder, place a host-specific boot.cfg that defines the kernel, initrd, and Kickstart settings for that machine. Modify the boot.cfg file, make the following adjustments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Set a prefix: Add prefix=http://&lt;!-- raw HTML omitted --&gt;/esxi9&lt;/li&gt;
&lt;li&gt;Fix module paths: Remove all trailing slashes from kernel= and modules=&lt;/li&gt;
&lt;li&gt;Clean kernel options: Remove cdromBoot from the kernelopt= line&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo vi /var/www/html/esxi9/EFI/01-&amp;lt;MAC address&amp;gt;/BOOT/BOOT.CFG
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;kickstart-script-distributed-version-control&#34;&gt;Kickstart Script Distributed Version Control&lt;/h2&gt;
&lt;p&gt;To bring change control and transparency to my provisioning workflow, I’ve stored all my host-specific Kickstart files in a private GitHub repository. Instead of embedding the full script in the boot.cfg, I reference the raw URL of the Kickstart file, making it easy to audit, update, and roll back changes over time.&lt;/p&gt;
&lt;p&gt;In each host’s boot.cfg, I point to the correct raw GitHub URL:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kernelopt&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;ks&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;https://raw.githubusercontent.com/&amp;lt;your-username&amp;gt;/&amp;lt;repo&amp;gt;/main/kickstart/ks-esxi-host01.cfg
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## Example https://raw.githubusercontent.com/darrylcauldwell/kickstart/refs/heads/main/host102.ks&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This approach lets me treat Kickstart files like code — complete with version control, pull requests, and commit history. It also opens the door for more structured configuration management across environments.  To support site-specific variations, I could use dedicated branches or even pin Kickstart URLs to specific commit hashes.&lt;/p&gt;
&lt;p&gt;To avoid duplicating standard settings in every Kickstart file, I’m moving reusable config (like syslog, NTP, and DNS settings) into a separate script stored in the same GitHub repo. Then, in the %firstboot section of the Kickstart file, I use wget to pull that down during the install:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;%firstboot --interpreter&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;busybox
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Fetch shared configuration script&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;wget -O /tmp/common-init.sh https://raw.githubusercontent.com/&amp;lt;your-username&amp;gt;/&amp;lt;repo&amp;gt;/main/scripts/common-init.sh
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;chmod +x /tmp/common-init.sh
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;/tmp/common-init.sh
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This keeps each Kickstart file lightweight and focused on host-specific logic, while shared behavior remains centrally managed and versioned.&lt;/p&gt;
&lt;h2 id=&#34;uefi-http-boot-discovery&#34;&gt;UEFI HTTP Boot Discovery&lt;/h2&gt;
&lt;p&gt;UEFI HTTP Boot can obtain the URL of the EFI bootloader in two primary ways. First, many DHCP servers support specific DHCP options (such as option 67 or the newer HTTP Boot options defined in RFC 7830) that provide the exact HTTP URL for the EFI file. This allows the client to dynamically discover where to fetch its bootloader without manual configuration. Alternatively, you can specify the HTTP Boot URL directly in the UEFI firmware settings or through your server’s management interface (e.g., iDRAC, iLO, UCS Manager). This flexibility lets you tailor the boot process for different environments—whether centralized via DHCP or set individually per host—making zero touch provisioning both scalable and adaptable.&lt;/p&gt;
&lt;p&gt;Instead of relying on DHCP to deliver the EFI boot file URL, I chose to configure the HTTP boot URL directly in the ESXi virtual machine’s UEFI firmware settings. This method gives precise control over the boot process on a per-host basis without requiring DHCP server configuration. It’s especially useful in nested ESXi lab environments or where DHCP option support is limited or unavailable. By specifying the exact URL in the firmware, each host knows exactly where to download its bootloader, streamlining the zero touch provisioning workflow. To set this up for a virtual ESXi host I used these steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create an empty VM for UEFI HTTP Boot.&lt;/li&gt;
&lt;li&gt;Open the VM’s Edit Settings dialog in the vSphere Client.&lt;/li&gt;
&lt;li&gt;Scroll down and click VM Options &amp;gt; Advanced &amp;gt; Edit Configuration.&lt;/li&gt;
&lt;li&gt;Add the following two advanced configuration entries:
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Name: firmware.networkBootProtocol
Value: httpv4&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Name: firmware.networkBootUri
Value: http://172.16.101.100/boot/esxi9/EFI/BOOT/BOOTX64.EFI&lt;/p&gt;
&lt;p&gt;(Replace with your actual HTTP server URL and path to the EFI boot file.)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Save and close the settings, then power on the VM.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With these settings, the VM’s UEFI firmware will directly request the specified EFI file over HTTP during boot, bypassing the need for DHCP-delivered boot URLs. This approach simplifies the boot configuration in virtualized environments and makes zero touch provisioning more predictable and manageable.&lt;/p&gt;
&lt;h2 id=&#34;vcf-orchestrator&#34;&gt;VCF Orchestrator&lt;/h2&gt;
&lt;p&gt;VMware Cloud Foundation (VCF) Orchestrator is a powerful automation and orchestration engine designed to simplify and accelerate the deployment and lifecycle management of VMware infrastructure. It provides a set of APIs and workflows that enable administrators to programmatically manage hosts, clusters, and workload domains within a VCF environment.&lt;/p&gt;
&lt;p&gt;In the following sections, I’ll explore how to integrate VCF Orchestrator with vCenter Server and use its workflow capabilities to automate complex tasks—such as onboarding newly provisioned ESXi hosts into clusters. This approach helps reduce manual steps, eliminate human error, improve consistency, and support compliance by maintaining a clear audit trail—all while enabling scalable operations across your infrastructure.&lt;/p&gt;
&lt;p&gt;Each workflow task within VCF Orchestrator can generate detailed logs—including standard output messages, errors, and warnings—that are sent directly to the VCF Operations Logs. This centralized logging not only aids troubleshooting but also creates a comprehensive, auditable record of all automated actions performed during the provisioning and management process.&lt;/p&gt;
&lt;p&gt;VCF Orchestrator can also integrate with external Configuration Management Databases (CMDBs) or similar systems of record to pull in site-specific variables and metadata. This dynamic data integration helps maintain accurate, up-to-date context for each site or environment, ensuring that orchestrated workflows execute with the correct configuration parameters tailored to the target location.&lt;/p&gt;
&lt;h3 id=&#34;deploying-vcf-orchestrator&#34;&gt;Deploying VCF Orchestrator&lt;/h3&gt;
&lt;p&gt;Deploying VCF Orchestrator involves provisioning it as a dedicated appliance. Here’s a high-level outline of the process:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Download the OVA - Obtain the VCF Orchestrator appliance OVA from the VMware Customer Connect portal.&lt;/li&gt;
&lt;li&gt;Deploy the OVA - Use the vSphere Client to deploy the OVA&lt;/li&gt;
&lt;li&gt;Configure Networking, NTP and DNS&lt;/li&gt;
&lt;li&gt;Initial Setup - Connect to CLI and configure identity provider, for simplicity I used vSphere SSO&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;adding-vcenter-server-as-endpoint&#34;&gt;Adding vCenter Server as endpoint&lt;/h3&gt;
&lt;p&gt;By integrating with vCenter Server, Orchestrator workflows can leverage the full vSphere API. The integration allows Orchestrator to create resource objects and perform tasks. Additionally, all actions executed through the vCenter endpoint are logged and auditable, providing enhanced visibility and supporting compliance across your infrastructure.&lt;/p&gt;
&lt;p&gt;For production environments, it’s best practice to create a dedicated service account with scoped permissions for integrating VCF Orchestrator with vCenter Server. This approach enhances security and auditability by limiting access to only the necessary resources and actions. However, for simplicity in my lab environment, I used the built-in &lt;a href=&#34;mailto:administrator@vsphere.local&#34;&gt;administrator@vsphere.local&lt;/a&gt; account to streamline initial testing and workflow development.&lt;/p&gt;
&lt;p&gt;To add  vCenter Endpoint expand the workflow library and run &amp;lsquo;Add vCenter Endpoint&amp;rsquo;. The workflow will:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Prompt for connection details (hostname/IP, port, protocol)&lt;/li&gt;
&lt;li&gt;Requires credentials&lt;/li&gt;
&lt;li&gt;Stores the vCenter connection securely in Orchestrator’s endpoint list&lt;/li&gt;
&lt;li&gt;Tests connectivity and verifies the endpoint before saving&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;governance-and-self-service-with-vcf-automation&#34;&gt;Governance and Self-Service with VCF Automation&lt;/h3&gt;
&lt;p&gt;While VCF Orchestrator excels at automating infrastructure workflows, adding a governance layer ensures these powerful automations align with organizational policies and control requirements. VMware Cloud Foundation Automation (VCF Automation) provides this additional layer by enabling policy-driven management and approval workflows on top of Orchestrator’s capabilities.&lt;/p&gt;
&lt;p&gt;With VCF Automation, users can submit self-service requests for infrastructure operations—such as provisioning or scaling hosts—while enforcing approval gates to maintain control and compliance. This means infrastructure changes are not only automated but also reviewed and authorized according to business rules, reducing risk.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Orchestrating AI Deployment: Azure Pipelines Meets VMware ECS</title>
      <link>https://darrylcauldwell.github.io/post/azure-pipeline-ecs/</link>
      <pubDate>Wed, 02 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/azure-pipeline-ecs/</guid>
      <description>
        
          &lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;kkk&lt;/p&gt;
&lt;h2 id=&#34;edge-ai-challenge&#34;&gt;Edge AI Challenge&lt;/h2&gt;
&lt;p&gt;AI applications at the edge offer significant advantages, including local data processing for real-time analysis, reduced latency, offline operation, and scalability without overwhelming central infrastructure. Regular retraining of models allows contiunal incremental accuracy improvements. Regular quantization, pruning, and knowledge distillation also helps reduce model size without loss and improve model performance. However the software lifecycle management of applications across a large estate of distributed edge compute environments presents several significant challenges:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ensuring compatibility across different operating systems and software environments is challenging.&lt;/li&gt;
&lt;li&gt;Edge environments can have limited or unreliable network connectivity.&lt;/li&gt;
&lt;li&gt;There&amp;rsquo;s a shortage of IT expertise in edge sites.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To address these challenges, organizations are turning to specialized edge platforms and tools that offer features like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pull-based architectures for efficient management of distributed sites.&lt;/li&gt;
&lt;li&gt;Zero-touch provisioning and lifecycle management.&lt;/li&gt;
&lt;li&gt;Edge-optimized runtimes for both virtualized and containerized applications.&lt;/li&gt;
&lt;li&gt;Integrated telemetry for visibility into application and platform behaviors.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It is a risk to deploy large-scale AI model updates to thousands of edge sites in an uncontrolled fashion. Potential widespread system failures or performance degradation if the updates contain bugs. Without proper testing, updates could introduce new biases or errors in AI decision-making. Without rollback capabilities in an uncontrolled process makes it difficult to quickly address problems. To address these challenges, organizations are looking to deploy continous delivery pipelines and employing testing stage gates.&lt;/p&gt;
&lt;h2 id=&#34;gitops&#34;&gt;GitOps&lt;/h2&gt;
&lt;p&gt;GitOps is an operational model for cloud-native applications that uses Git as the single source of truth for declarative infrastructure and applications its principles:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Infrastructure-as-Code (IaC) stored in Git repositories&lt;/li&gt;
&lt;li&gt;Automated synchronization between Git state and cluster state&lt;/li&gt;
&lt;li&gt;Declarative descriptions of the desired system state&lt;/li&gt;
&lt;li&gt;Continuous deployment and reconciliation&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;azure-devops-pipeline&#34;&gt;Azure DevOps Pipeline&lt;/h2&gt;
&lt;p&gt;Azure &lt;a href=&#34;https://azure.microsoft.com/en-us/products/devops/pipelines/&#34;&gt;Pipelines&lt;/a&gt; is a cloud-based service provided by Microsoft as part of the Azure DevOps suite. It is designed to automate the building, testing, and deployment of code projects. It enables controlled, stagegated deployments with approvals and gates allowing thorough testing in controlled environments before wider rollout. It provides visibility into the deployment process, which is essential for managing and troubleshooting across distributed edge sites.&lt;/p&gt;
&lt;h2 id=&#34;vmware-edge-compute-stack-ecs&#34;&gt;VMware Edge Compute Stack (ECS)&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://www.vmware.com/products/software-defined-edge/edge-compute/stack&#34;&gt;VMware Edge Compute Stack (ECS)&lt;/a&gt; is an edge-optimized runtime and orchestration platform designed for managing edge applications and infrastructure across multiple distributed sites. The edge-optimized runtime is managed by VMware Edge Cloud Orchestrator (VECO). VECO enables a GitOps approach to managing edge infrastructure and applications. This approach enables version control and auditing of changes to edge environments. VECO implements a zero-touch provisioning with a pull-based model suitable for edge sites with limited IT skills and intermittent connectivity.&lt;/p&gt;
&lt;p&gt;In VECO sites can be used to manage a number of edge hosts in the same location, these hosts can share a common set of configuration files and also be visualized and troubleshooted as a single location. A site can be associated with a GitHub repository, branch and or folder. In my environment I associate each site with a site folder one each for qa, uat and production environments. Within each folder I have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;configureHost.yaml - Defines the configuration to be applied to the ESXi host&lt;/li&gt;
&lt;li&gt;face-detector.yaml - Defines the configuration of the AI inference application&lt;/li&gt;
&lt;li&gt;vm.yaml - Defines the configuration of other VM workload to be deployed to the site&lt;/li&gt;
&lt;li&gt;metallb.yaml - Defines the configuration of load balancer to expose AI inference application kubernetes service to the outside network&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The file and folder structure for the sites and associated declarative configuration files looks something like:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;├── sites
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ├── qa
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    │   ├── configureHost.yaml
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    │   ├── face-detector.yaml
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    │   ├── vm.yaml
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    │   └── metallb.yaml
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ├── uat
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    │   ├── configureHost.yaml
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    │   ├── face-detector.yaml
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    │   ├── vm.yaml
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    │   └── metallb.yaml
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    └── production
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        ├── configureHost.yaml
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        ├── face-detector.yaml
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        ├── vm.yaml
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        └── metallb.yaml
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;computer-vision-pipeline&#34;&gt;Computer Vision Pipeline&lt;/h2&gt;
&lt;p&gt;A while ago I created a computer vision demonstration application the approach is documented &lt;a href=&#34;https://darrylcauldwell.github.io/post/computer-vision/&#34;&gt;here&lt;/a&gt;. This was designed to make simple changes to the code which reflected visually in the running application. There is the ability to simulate a change the AI model to detect a draw a green box around either full face or just the eyes. There is also the ability to simulate other changes to the app such as changing web app background colour or text.  The source code for this application is stored in a &lt;a href=&#34;https://github.com/darrylcauldwell/keswick&#34;&gt;GitHub repository&lt;/a&gt;. There are various other assets in this repository,  all face detector application files are within folder &lt;a href=&#34;https://github.com/darrylcauldwell/keswick/tree/main/face-detector&#34;&gt;face-detector&lt;/a&gt;. As such the first part of the pipeline watches for commits to files in the face-detector folder in the main repository branch.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;trigger&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;branches&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;include&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#ae81ff&#34;&gt;main&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;include&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#ae81ff&#34;&gt;face-detector/*&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;An Azure Pipeline agent is a software component that runs on virtual server in Azure cloud or on a physical device. The agent is responsible for executing the tasks defined in an Azure DevOps pipeline. There is a cost to using Azure cloud agents and I have perfectly good laptop which could run the pipeline agent software.&lt;/p&gt;
&lt;p&gt;Any software required by the pipeline to run needs to be available on the agent. I need my pipeline to build docker imaages and so I need to have the Docker Desktop engine running. I have an Arm based Mac in which the docker engine socket location changes from standard. In this standard configuration the Azure pipeline fails as it cannot find docker engine socket.  In this case it is possible to set environment variable to its correct location before starting the agent.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;export DOCKER_HOST&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;unix://$HOME/.docker/run/docker.sock
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;yq is a lightweight and portable command-line tool for processing YAML, JSON, XML, CSV, and other structured data formats. It is particularly useful for updating YAML files. My pipeline uses yq to update the infrastructure as code yaml files. As such I needed to install this on MacOS using command like:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;brew install yq
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The next section of the pipeline defines agent pool to use my laptop rather than a cloud instance.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;pool&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Default&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;demands&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#ae81ff&#34;&gt;agent.name -equals Darryls-MacBook-Pro&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With the base pipeline attributes defined we can look at the stages. A stage is a logical boundary in an Azure DevOps pipeline. Stages can be used to group actions, each stage contains one or more jobs. In this pipeline I have the following stages:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Build and push multi-arch docker image&lt;/li&gt;
&lt;li&gt;Update the QA environment&lt;/li&gt;
&lt;li&gt;Approval stagegate for QA&lt;/li&gt;
&lt;li&gt;Update the UAT environment&lt;/li&gt;
&lt;li&gt;Approval stagegate for UAT&lt;/li&gt;
&lt;li&gt;Update the production environment&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Introduce your approach using Azure Pipelines to manage VMware ECS site YAML files&lt;/p&gt;
&lt;p&gt;Emphasize the benefits of this method (automation, consistency, control)&lt;/p&gt;
&lt;p&gt;Azure Pipelines: Explain its role in orchestrating the deployment process&lt;/p&gt;
&lt;p&gt;VMware ECS: Briefly describe how it&amp;rsquo;s used as the deployment target&lt;/p&gt;
&lt;p&gt;YAML Configuration: Discuss the importance of YAML files in defining ECS site configurations&lt;/p&gt;
&lt;p&gt;Stage Gates: Explain how these control the flow between environments&lt;/p&gt;
&lt;h2 id=&#34;pipeline-structure&#34;&gt;Pipeline Structure&lt;/h2&gt;
&lt;p&gt;Describe the overall structure of your pipeline&lt;/p&gt;
&lt;p&gt;Explain the stages: QA, UAT, and Production&lt;/p&gt;
&lt;p&gt;Discuss how you implemented stage gates between environments&lt;/p&gt;
&lt;h2 id=&#34;desired-state---yaml-configuration-management&#34;&gt;Desired State - YAML Configuration Management&lt;/h2&gt;
&lt;p&gt;Explain how you manage different YAML configurations for each environment&lt;/p&gt;
&lt;p&gt;Discuss any templating or parameterization techniques used&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Managing Tanzu for vSphere Clusters Using Tanzu Misson Control</title>
      <link>https://darrylcauldwell.github.io/post/tanzu-tmc/</link>
      <pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/tanzu-tmc/</guid>
      <description>
        
          &lt;p&gt;Third in a series of posts which build on each other looking Tanzu.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://darrylcauldwell.github.io/post/tanzu-basic-nsx&#34;&gt;Deploying Tanzu for vSphere with NSX-T&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://darrylcauldwell.github.io/post/tanzu-tkc&#34;&gt;Managing Tanzu for vSphere Clusters Using ClusterAPI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://darrylcauldwell.github.io/post/tanzu-tmc&#34;&gt;Managing Tanzu for vSphere Clusters Using Tanzu Misson Control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://darrylcauldwell.github.io/post/tanzu-tmc-aws&#34;&gt;Deploying Tanzu for AWS Using Tanzu Misson Control&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I am moving next to look at Tanzu Mission Control. The Mission Control inventory has a hierachy, first I&amp;rsquo;ll create a group to place the clusters in. While adding the group add some metadata to group including a label so other users know who to contact with any queries.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-group.png&#34; alt=&#34;Tanzu Mission Control Cluster Group&#34;&gt;&lt;/p&gt;
&lt;p&gt;With the group in place we can look at attaching an existing cluster to TMC.  Following the wizard the first step is to enter some metadata for the cluster being attached.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-meta.png&#34; alt=&#34;Tanzu Mission Control Cluster Metadata&#34;&gt;&lt;/p&gt;
&lt;p&gt;anzu Misson Control does not need to connect inwards to the cluster to be managed.  Instead agents are deployed and configured on the cluster which initiate outbound connection to Tanzu Misson Control. Outbound internet communications are sometimes via proxy, if required add proxy details next.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-proxy.png&#34; alt=&#34;Tanzu Mission Control Cluster Proxy&#34;&gt;&lt;/p&gt;
&lt;p&gt;When the deployed agents initiate communications with TMC they require to advertise who they are and be associated with the right objects. The agent configuration is supplied via UI as YAML. A URL is supplied to the configuration on internet. The administrator of this cluster is Hermione so I connect with her credentials and apply the yaml file to deploy and configure the agents.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl vsphere login --server&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;172.16.1.2 --tanzu-kubernetes-cluster-name hogwarts-cluster01 --tanzu-kubernetes-cluster-namespace hogwarts --vsphere-username hermione@vsphere.local --insecure-skip-tls-verify
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl create -f &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://tanzuemea.tmc.cloud.vmware.com/installer?id=f1d706f38d6f3ad834314a941831b190e18508468ade1adf2e105a822c8c0b49&amp;amp;source=attach&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-kubectl.png&#34; alt=&#34;Tanzu Mission Control Cluster kubectl&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once the agent has be deployed can move back to TMC. The agents take a couple of minutes to initialize and gather data but in very short order can then view the cluster and extension health in TMC.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-health.png&#34; alt=&#34;Tanzu Mission Control Cluster Health&#34;&gt;&lt;/p&gt;
&lt;p&gt;To unlock more power of TMC we can register the TKG management cluster. The process is similar to attaching subordinate cluster but is slightly different as before we start with adding some metadata.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-sup-meta.png&#34; alt=&#34;Tanzu Mission Control Supervisor Cluster Metadata&#34;&gt;&lt;/p&gt;
&lt;p&gt;Again here I am not using proxy so just skip through next page.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-sup-proxy.png&#34; alt=&#34;Tanzu Mission Control Supervisor Cluster Proxy&#34;&gt;&lt;/p&gt;
&lt;p&gt;Rather than being presented with a yaml file to apply with kubectl cmd here the wizard outputs a URL link which contains config.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-sup-URL.png&#34; alt=&#34;Tanzu Mission Control Supervisor Cluster Config URL&#34;&gt;&lt;/p&gt;
&lt;p&gt;We then have to create environmentally specific yaml file which includes namespace and the supplied URL. This needs to be ran in the context of the cluster itself so we switch to that and then get the namespace name prefixed with svc-tmc.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl vsphere login --server&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;172.16.1.2 --vsphere-username administrator@vsphere.local --insecure-skip-tls-verify
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Logged in successfully.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;You have access to the following contexts:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   172.16.1.2
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   hogsmead
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   hogwarts
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;If the context you wish to use is not in this list, you may need to try
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;logging in again later, or contact your cluster administrator.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;To change context, use &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;kubectl config use-context &amp;lt;workload name&amp;gt;&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl config use-context 172.16.1.2
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Switched to context &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;172.16.1.2&amp;#34;&lt;/span&gt;.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl get ns | grep svc-tmc
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;NAME                                        STATUS   AGE
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;svc-tmc-c34                                 Active   11d
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With the namespace name and URL we can populate the yaml file to apply agent and config.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;installers.tmc.cloud.vmware.com/v1alpha1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;AgentInstall&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;tmc-agent-installer-config&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;svc-tmc-c34 &lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;operation&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;INSTALL&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;registrationLink&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://tanzuemea.tmc.cloud.vmware.com/installer?id=0f25b71394ac623867f64138ed9d27bb0db5e8d0436e874e4f65ff179e7be807&amp;amp;source=registration&amp;amp;type=tkgs&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can then apply the configuration using kubectl and the agents get created with correct configuration.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl create -f tmc-registration.yaml
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;agentinstall.installers.tmc.cloud.vmware.com/tmc-agent-installer-config created
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-sup-kubectl.png&#34; alt=&#34;Tanzu Mission Control Supervisor Cluster kubectl&#34;&gt;&lt;/p&gt;
&lt;p&gt;After a couple of minutes when the agents have started and have reported back we can get a view of supervisor cluster health.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-sup-health.png&#34; alt=&#34;Tanzu Mission Control Supervisor Cluster Health&#34;&gt;&lt;/p&gt;
&lt;p&gt;With the supervisor cluster registered we can start to some interesting things from TMC like provision a new cluster. To recap at this stage my deployment has two namespaces configured. Only Hogwarts has a workload cluster deployed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hogwarts
&lt;ul&gt;
&lt;li&gt;hogwarts-cluster-01&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Hogsmead&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Starting the TMC wizard to create cluster first prompt is for the provider. This is analagous to namespace so we&amp;rsquo;ll look at creating cluster under Hogsmead.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-prov-provider.png&#34; alt=&#34;Tanzu Mission Control Cluster Provising Provider&#34;&gt;&lt;/p&gt;
&lt;p&gt;Then add some cluster metadata.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-prov-meta.png&#34; alt=&#34;Tanzu Mission Control Cluster Provising Metadata&#34;&gt;&lt;/p&gt;
&lt;p&gt;Can then pick a version of Kubernetes which the cluster will run, the Pod and Service networking configuration and storage class.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-prov-config.png&#34; alt=&#34;Tanzu Mission Control Cluster Provising Config&#34;&gt;&lt;/p&gt;
&lt;p&gt;Followed by choosing the VM class, hardware specifcation of the cluster nodes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-prov-vmclass.png&#34; alt=&#34;Tanzu Mission Control Cluster Provising VM Class&#34;&gt;&lt;/p&gt;
&lt;p&gt;Finally the node pool configuration.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-prov-nodepool.png&#34; alt=&#34;Tanzu Mission Control Cluster Provising Node Pool&#34;&gt;&lt;/p&gt;
&lt;p&gt;It takes a few minutes to deploy the required VMs and configure the Kubernetes cluster.  You can get a view on how far it is along by looking in vCenter.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-prov-vcenter.png&#34; alt=&#34;Tanzu Mission Control Cluster Provising vCenter&#34;&gt;&lt;/p&gt;
&lt;p&gt;When deploying cluster from TMC the agents and configuration to register the cluster as with TMC is included.  After a few minutes when VMs are built and configured the cluster shows in UI.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-prov-registered.png&#34; alt=&#34;Tanzu Mission Control Cluster Provising Registered&#34;&gt;&lt;/p&gt;
&lt;p&gt;As well as provisioning TMC can also help with day two operations. When deploying the Hogsmead cluster I selected an v1.20.12 version of k8s. Upgrading to latest can be initiated very easily. Within the cluster view just hit Upgrade and select from drop down of all newer versions the version you need.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-lcm.png&#34; alt=&#34;Tanzu Mission Control Cluster Lifecycle Management&#34;&gt;&lt;/p&gt;
&lt;p&gt;A couple of minutes later and can see its now running v1.21.6.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-tmc-cluster-lcm-version.png&#34; alt=&#34;Tanzu Mission Control Cluster Lifecycle Management Complete&#34;&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Managing Tanzu for vSphere Clusters Using ClusterAPI</title>
      <link>https://darrylcauldwell.github.io/post/tanzu-tkc/</link>
      <pubDate>Fri, 03 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/tanzu-tkc/</guid>
      <description>
        
          &lt;p&gt;Second in a series of posts which build on each other looking Tanzu.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://darrylcauldwell.github.io/post/tanzu-basic-nsx&#34;&gt;Deploying Tanzu for vSphere with NSX-T&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://darrylcauldwell.github.io/post/tanzu-tkc&#34;&gt;Managing Tanzu for vSphere Clusters Using ClusterAPI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://darrylcauldwell.github.io/post/tanzu-tmc&#34;&gt;Managing Tanzu for vSphere Clusters Using Tanzu Misson Control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://darrylcauldwell.github.io/post/tanzu-tmc-aws&#34;&gt;Deploying Tanzu for AWS Using Tanzu Misson Control&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Clusters are created within organisational construct known as a namespace. The namespace configuration can be used to map user/groups to role/permission it uses the same users/groups available as vSphere Identity source, these can be aligned to one of three roles view, edit and owner. The namespace configuration also defines which vSphere storage policies can be aligned to namespace and the amount of available CPU, Memory and Storage can be limited. I have a single cluster prepared with the following Namespace configuration:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Namespace&lt;/th&gt;
&lt;th&gt;User&lt;/th&gt;
&lt;th&gt;Role&lt;/th&gt;
&lt;th&gt;Storage Policy&lt;/th&gt;
&lt;th&gt;Limit&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Hogwarts&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;mailto:hermione@vsphere.local&#34;&gt;hermione@vsphere.local&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Owner&lt;/td&gt;
&lt;td&gt;vSAN Default Storage Policy&lt;/td&gt;
&lt;td&gt;Unlimited&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hogwarts&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;mailto:harry@vsphere.local&#34;&gt;harry@vsphere.local&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Edit&lt;/td&gt;
&lt;td&gt;vSAN Default Storage Policy&lt;/td&gt;
&lt;td&gt;Unlimited&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hogwarts&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;mailto:ron@vsphere.local&#34;&gt;ron@vsphere.local&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;View&lt;/td&gt;
&lt;td&gt;vSAN Default Storage Policy&lt;/td&gt;
&lt;td&gt;Unlimited&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hogsmead&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;mailto:hermione@vsphere.local&#34;&gt;hermione@vsphere.local&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Edit&lt;/td&gt;
&lt;td&gt;vSAN Default Storage Policy&lt;/td&gt;
&lt;td&gt;Unlimited&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hogsmead&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;mailto:harry@vsphere.local&#34;&gt;harry@vsphere.local&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Owner&lt;/td&gt;
&lt;td&gt;vSAN Default Storage Policy&lt;/td&gt;
&lt;td&gt;Unlimited&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Hogsmead&lt;/td&gt;
&lt;td&gt;&lt;a href=&#34;mailto:ron@vsphere.local&#34;&gt;ron@vsphere.local&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;View&lt;/td&gt;
&lt;td&gt;vSAN Default Storage Policy&lt;/td&gt;
&lt;td&gt;Unlimited&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;With these in place I can attempt to connect as Hermione and check access to Hippogriff.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;set KUBECTL_VSPHERE_PASSWORD&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;VMware1!
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl vsphere login --server&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;172.16.1.2 --vsphere-username hermione@vsphere.local --insecure-skip-tls-verify 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Logged in successfully.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;You have access to the following contexts:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   172.16.1.2
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   hogsmead
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   hogwarts
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;If the context you wish to use is not in this list, you may need to try
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;logging in again later, or contact your cluster administrator.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;To change context, use &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;kubectl config use-context &amp;lt;workload name&amp;gt;&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl config use-context hogwarts
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To deploy a cluster you need to specify certain attributes such as which image and Kubernetes version to deploy and the dimensions of the nodes. The node images are stored in vSphere Content Library and some standard ones are available via a publiclu content library subscription. The VM hardware specification dimensions of node are defined as virtual machine classes. The VM storage location are defined as storage classes. The names of available images, vmclasses and storageclasses available to be deployed from supervisor can be easily checked.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl get tanzukubernetesreleases
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;NAME                                VERSION                          READY   COMPATIBLE   CREATED   UPDATES AVAILABLE
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.16.12---vmware.1-tkg.1.da7afe7   1.16.12+vmware.1-tkg.1.da7afe7   True    True         6m28s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.17.17+vmware.1-tkg.1.d44d45a 1.16.14+vmware.1-tkg.1.ada4837&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.16.14---vmware.1-tkg.1.ada4837   1.16.14+vmware.1-tkg.1.ada4837   True    True         6m25s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.17.17+vmware.1-tkg.1.d44d45a&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.16.8---vmware.1-tkg.3.60d2ffd    1.16.8+vmware.1-tkg.3.60d2ffd    False   False        6m33s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.17.11---vmware.1-tkg.1.15f1e18   1.17.11+vmware.1-tkg.1.15f1e18   True    True         6m29s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.18.19+vmware.1-tkg.1.17af790 1.17.17+vmware.1-tkg.1.d44d45a&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.17.11---vmware.1-tkg.2.ad3d374   1.17.11+vmware.1-tkg.2.ad3d374   True    True         6m28s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.18.19+vmware.1-tkg.1.17af790 1.17.17+vmware.1-tkg.1.d44d45a&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.17.13---vmware.1-tkg.2.2c133ed   1.17.13+vmware.1-tkg.2.2c133ed   True    True         6m30s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.18.19+vmware.1-tkg.1.17af790 1.17.17+vmware.1-tkg.1.d44d45a&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.17.17---vmware.1-tkg.1.d44d45a   1.17.17+vmware.1-tkg.1.d44d45a   True    True         6m27s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.18.19+vmware.1-tkg.1.17af790&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.17.7---vmware.1-tkg.1.154236c    1.17.7+vmware.1-tkg.1.154236c    True    True         6m24s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.18.19+vmware.1-tkg.1.17af790 1.17.17+vmware.1-tkg.1.d44d45a&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.17.8---vmware.1-tkg.1.5417466    1.17.8+vmware.1-tkg.1.5417466    True    True         6m28s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.18.19+vmware.1-tkg.1.17af790 1.17.17+vmware.1-tkg.1.d44d45a&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.18.10---vmware.1-tkg.1.3a6cd48   1.18.10+vmware.1-tkg.1.3a6cd48   True    True         6m30s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.19.14+vmware.1-tkg.1.8753786 1.18.19+vmware.1-tkg.1.17af790&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.18.15---vmware.1-tkg.1.600e412   1.18.15+vmware.1-tkg.1.600e412   True    True         6m27s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.19.14+vmware.1-tkg.1.8753786 1.18.19+vmware.1-tkg.1.17af790&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.18.15---vmware.1-tkg.2.ebf6117   1.18.15+vmware.1-tkg.2.ebf6117   True    True         6m29s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.19.14+vmware.1-tkg.1.8753786 1.18.19+vmware.1-tkg.1.17af790&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.18.19---vmware.1-tkg.1.17af790   1.18.19+vmware.1-tkg.1.17af790   True    True         6m27s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.19.14+vmware.1-tkg.1.8753786&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.18.5---vmware.1-tkg.1.c40d30d    1.18.5+vmware.1-tkg.1.c40d30d    True    True         6m30s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.19.14+vmware.1-tkg.1.8753786 1.18.19+vmware.1-tkg.1.17af790&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.19.11---vmware.1-tkg.1.9d9b236   1.19.11+vmware.1-tkg.1.9d9b236   True    True         6m33s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.20.9+vmware.1-tkg.1.a4cee5b 1.19.14+vmware.1-tkg.1.8753786&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.19.14---vmware.1-tkg.1.8753786   1.19.14+vmware.1-tkg.1.8753786   True    True         6m29s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.20.9+vmware.1-tkg.1.a4cee5b&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.19.7---vmware.1-tkg.1.fc82c41    1.19.7+vmware.1-tkg.1.fc82c41    True    True         6m26s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.20.9+vmware.1-tkg.1.a4cee5b 1.19.14+vmware.1-tkg.1.8753786&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.19.7---vmware.1-tkg.2.f52f85a    1.19.7+vmware.1-tkg.2.f52f85a    True    True         6m26s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.20.9+vmware.1-tkg.1.a4cee5b 1.19.14+vmware.1-tkg.1.8753786&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.20.2---vmware.1-tkg.1.1d4f79a    1.20.2+vmware.1-tkg.1.1d4f79a    True    True         6m26s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.21.2+vmware.1-tkg.1.ee25d55 1.20.9+vmware.1-tkg.1.a4cee5b&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.20.2---vmware.1-tkg.2.3e10706    1.20.2+vmware.1-tkg.2.3e10706    True    True         6m33s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.20.9+vmware.1-tkg.1.a4cee5b&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.20.7---vmware.1-tkg.1.7fb9067    1.20.7+vmware.1-tkg.1.7fb9067    True    True         6m24s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.21.2+vmware.1-tkg.1.ee25d55 1.20.9+vmware.1-tkg.1.a4cee5b&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.20.8---vmware.1-tkg.2            1.20.8+vmware.1-tkg.2            True    True         6m25s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.20.9---vmware.1-tkg.1.a4cee5b    1.20.9+vmware.1-tkg.1.a4cee5b    True    True         6m31s     &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1.21.2+vmware.1-tkg.1.ee25d55&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;v1.21.2---vmware.1-tkg.1.ee25d55    1.21.2+vmware.1-tkg.1.ee25d55    True    True         6m31s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl get vmclass
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;NAME                  CPU   MEMORY   AGE
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;best-effort-2xlarge   &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;     64Gi     80s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;best-effort-4xlarge   &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;    128Gi    80s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;best-effort-8xlarge   &lt;span style=&#34;color:#ae81ff&#34;&gt;32&lt;/span&gt;    128Gi    80s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;best-effort-large     &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;     16Gi     80s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;best-effort-medium    &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;     8Gi      78s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;best-effort-small     &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;     4Gi      80s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;best-effort-xlarge    &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;     32Gi     80s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;best-effort-xsmall    &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;     2Gi      77s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;guaranteed-2xlarge    &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;     64Gi     79s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;guaranteed-4xlarge    &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;    128Gi    74s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl get storageclass -n hogwarts
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;NAME                          PROVISIONER              RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vsan-default-storage-policy   csi.vsphere.vmware.com   Delete          Immediate           true                   19m
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Using the output from previous commands we can copy paste the appropriate storage class, vm class and Kuberneters version values from what is available in the namespace into a yaml file definition for the cluster.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;run.tanzu.vmware.com/v1alpha2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TanzuKubernetesCluster&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hogwarts-cluster01&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hogwarts&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;topology&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;controlPlane&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;vmClass&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;best-effort-small&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;storageClass&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;vsan-default-storage-policy&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;tkr&lt;/span&gt;:  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;reference&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1.21.2---vmware.1-tkg.1.ee25d55&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;nodePools&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;forbidden-forest&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;replicas&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;vmClass&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;best-effort-small&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;storageClass&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;vsan-default-storage-policy&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;tkr&lt;/span&gt;:  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;reference&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1.21.2---vmware.1-tkg.1.ee25d55&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;settings&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;network&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;cni&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;antrea&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;services&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;cidrBlocks&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;10.96.0.0/12&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;pods&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;cidrBlocks&lt;/span&gt;: [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;172.16.128.0/17&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;serviceDomain&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;hogwarts.local&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With all of this in place we can deploy the cluster,  we can check it has been deployed correctly.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl apply -f hogwarts-cls1.yml
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tanzukubernetescluster.run.tanzu.vmware.com/hogwarts-cluster01 created
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl get TanzuKubernetesCluster
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;NAMESPACE   NAME                 CONTROL PLANE   WORKER   TKR NAME                           AGE   READY   TKR COMPATIBLE   UPDATES AVAILABLE
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hogwarts    hogwarts-cluster01   &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;               &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;        v1.21.2---vmware.1-tkg.1.ee25d55   32m   True    True
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl get virtualmachines
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;NAME                                                         POWERSTATE   AGE
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hogwarts-cluster01-control-plane-9tljd                       poweredOn    35m
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hogwarts-cluster01-control-plane-cn89v                       poweredOn    37m
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hogwarts-cluster01-control-plane-wfnjg                       poweredOn    41m
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hogwarts-cluster01-forbidden-forest-hdqms-7b9bf6bb44-fqw2d   poweredOn    38m
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hogwarts-cluster01-forbidden-forest-hdqms-7b9bf6bb44-n2r5s   poweredOn    38m
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hogwarts-cluster01-forbidden-forest-hdqms-7b9bf6bb44-vbnnj   poweredOn    38m
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl get service
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;NAME                                               TYPE           CLUSTER-IP    EXTERNAL-IP   PORT&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;S&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;          AGE
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;service/hogwarts-cluster01-control-plane-service   LoadBalancer   10.96.0.231   172.16.1.3    6443:32176/TCP   44m
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If I swing over to take a look what has happened in NSX-T a second segment has been added to the T1 router which was created during the supervisor cluster enablement. The segment has connected the three control plane and three node pool VMs. An NSX Load Balancer is deployed to represent the Kubernetes API service resource.  There are also NAT Rules to represent cluster Egress via 172.16.2.2 IP address. All of the created NSX entities have the namespace name in this case &amp;lsquo;hogwarts&amp;rsquo; in their description they are really easy to identify via search.&lt;/p&gt;
&lt;p&gt;So all looks good and can try and connect to the subordinate cluster and interact with it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl vsphere login --server&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;172.16.1.2 --tanzu-kubernetes-cluster-name hogwarts-cluster01 --tanzu-kubernetes-cluster-namespace hogwarts --vsphere-username hermione@vsphere.local --insecure-skip-tls-verify
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Logged in successfully.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;You have access to the following contexts:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   172.16.1.2
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   hogsmead
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   hogwarts
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   hogwarts-cluster01
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;If the context you wish to use is not in this list, you may need to try logging in again later, or contact your cluster administrator.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;To change context, use &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;kubectl config use-context &amp;lt;workload name&amp;gt;&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl config use-context hogwarts-cluster01
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl get nodes
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;NAME                                                         STATUS   ROLES                  AGE   VERSION
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hogwarts-cluster01-control-plane-9tljd                       Ready    control-plane,master   58m   v1.21.2+vmware.1
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hogwarts-cluster01-control-plane-cn89v                       Ready    control-plane,master   61m   v1.21.2+vmware.1
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hogwarts-cluster01-control-plane-wfnjg                       Ready    control-plane,master   64m   v1.21.2+vmware.1
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hogwarts-cluster01-forbidden-forest-hdqms-7b9bf6bb44-fqw2d   Ready    &amp;lt;none&amp;gt;                 62m   v1.21.2+vmware.1
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hogwarts-cluster01-forbidden-forest-hdqms-7b9bf6bb44-n2r5s   Ready    &amp;lt;none&amp;gt;                 62m   v1.21.2+vmware.1
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;hogwarts-cluster01-forbidden-forest-hdqms-7b9bf6bb44-vbnnj   Ready    &amp;lt;none&amp;gt;                 62m   v1.21.2+vmware.1
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next step is to deploy first Pod into the cluster, but this got stuck ContainerCreating and Pod seems to show an Antrea socket error.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl run -i --tty busybox --image&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;quay.io/quay/busybox --restart&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Never -- sh
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl get pods
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;NAME      READY   STATUS              RESTARTS   AGE
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;busybox   0/1     ContainerCreating   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          3m12s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl describe pod busybox
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Events:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Type     Reason                  Age                From               Message
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  ----     ------                  ----               ----               -------
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Normal   Scheduled               3m55s              default-scheduler  Successfully assigned default/busybox to hogwarts-cluster01-forbidden-forest-8g2s2-66d6bfdd-8wzbr
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Warning  FailedCreatePodSandBox  3m52s              kubelet            Failed to create pod sandbox: rpc error: code &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Unknown desc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; failed to setup network &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; sandbox &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;354f63b2c94a0fb5a88c9611adb3008c7dd564deb157a39ebeca78f1b717316c&amp;#34;&lt;/span&gt;: rpc error: code &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Unavailable desc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; connection error: desc &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;transport: Error while dialing dial unix /var/run/antrea/cni.sock: connect: connection refused&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So first thing to look at I guess is Antrea Pods and can see one is broken.  Checking logs and it looks like OVS timed out on startup.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl get pods -A
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;NAMESPACE                      NAME                                                                       READY   STATUS              RESTARTS   AGE
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;default                        busybox                                                                    0/1     ContainerCreating   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          12m
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kube-system                    antrea-agent-8pf5t                                                         0/2     CrashLoopBackOff    &lt;span style=&#34;color:#ae81ff&#34;&gt;26&lt;/span&gt;         96m
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kube-system                    antrea-agent-bv9rq                                                         2/2     Running             &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;          102m
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kube-system                    antrea-agent-cth2b                                                         2/2     Running             &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          102m
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kube-system                    antrea-agent-p6qfq                                                         2/2     Running             &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;          96m
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kube-system                    antrea-agent-wh8k4                                                         2/2     Running             &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          91m
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kube-system                    antrea-agent-wsrtg                                                         2/2     Running             &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          99m
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl logs antrea-agent-8pf5t -n kube-system antrea-ovs
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1;34m2021-12-03T20:08:00Z &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;0;32mINFO &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;0;36mantrea-ovs&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;0m&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;: Starting ovsdb-server
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Starting ovsdb-server.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2021-12-03T20:09:05Z|00002|timeval|WARN|Unreasonably long 10026ms poll interval &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;7883ms user, 82ms system&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2021-12-03T20:09:05Z|00003|timeval|WARN|faults: &lt;span style=&#34;color:#ae81ff&#34;&gt;57&lt;/span&gt; minor, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; major
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2021-12-03T20:09:05Z|00004|timeval|WARN|context switches: &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; voluntary, &lt;span style=&#34;color:#ae81ff&#34;&gt;9309&lt;/span&gt; involuntary
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2021-12-03T20:09:17Z|00002|timeval|WARN|Unreasonably long 8797ms poll interval &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;7858ms user, 89ms system&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2021-12-03T20:09:17Z|00003|timeval|WARN|faults: &lt;span style=&#34;color:#ae81ff&#34;&gt;60&lt;/span&gt; minor, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; major
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2021-12-03T20:09:17Z|00004|timeval|WARN|context switches: &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; voluntary, &lt;span style=&#34;color:#ae81ff&#34;&gt;9264&lt;/span&gt; involuntary
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Configuring Open vSwitch system IDs.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;/usr/share/openvswitch/scripts/ovs-ctl: line 42: hostname: command not found
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Enabling remote OVSDB managers.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1;34m2021-12-03T20:09:19Z &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;0;32mINFO &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;0;36mantrea-ovs&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;0m&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;: Stopping OVS before quit
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1;34m2021-12-03T20:09:19Z &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;0;32mINFO &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;0;36mantrea-ovs&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;0m&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;: Stopping OVS
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ovs-vswitchd is not running.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;2021-12-03T20:09:21Z|00001|fatal_signal|WARN|terminating with signal &lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;Alarm clock&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As this was a timeout and that the cluster is very easy to remove and redeploy I tried this and with exact same configuration this issue resolved. And we can finally run our pod and run some commands!&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl config use-context hogwarts
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl delete -f hogwarts-cls1b.yml
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl apply -f hogwarts-cls1b.yml
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl vsphere login --server&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;172.16.1.2 --tanzu-kubernetes-cluster-name hogwarts-cluster01 --tanzu-kubernetes-cluster-namespace hogwarts --vsphere-username hermione@vsphere.local --insecure-skip-tls-verify
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Logged in successfully.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;You have access to the following contexts:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   172.16.1.2
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   hogsmead
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   hogwarts
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   hogwarts-cluster01
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;If the context you wish to use is not in this list, you may need to try logging in again later, or contact your cluster administrator.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;To change context, use &lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;kubectl config use-context &amp;lt;workload name&amp;gt;&lt;span style=&#34;color:#e6db74&#34;&gt;`&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl config use-context hogwarts-cluster01
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl run -i --tty busybox --image&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;quay.io/quay/busybox --restart&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Never -- sh
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;If you don&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&amp;#39;&lt;/span&gt;t see a command prompt, try pressing enter.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;/ &lt;span style=&#34;color:#75715e&#34;&gt;# nslookup bbc.co.uk&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Server:         10.96.0.10
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Address:        10.96.0.10:53
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Non-authoritative answer:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Name:	bbc.co.uk
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Address: 151.101.128.81
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Name:	bbc.co.uk
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Address: 151.101.0.81
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Name:	bbc.co.uk
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Address: 151.101.192.81
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Name:	bbc.co.uk
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Address: 151.101.64.81
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;/ &lt;span style=&#34;color:#75715e&#34;&gt;#&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
        
      </description>
    </item>
    
    <item>
      <title>Deploying Tanzu for vSphere with NSX-T</title>
      <link>https://darrylcauldwell.github.io/post/tanzu-basic-nsx/</link>
      <pubDate>Mon, 15 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/tanzu-basic-nsx/</guid>
      <description>
        
          &lt;p&gt;First in a series of posts which build on each other looking Tanzu.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://darrylcauldwell.github.io/post/tanzu-basic-nsx&#34;&gt;Deploying Tanzu for vSphere with NSX-T&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://darrylcauldwell.github.io/post/tanzu-tkc&#34;&gt;Managing Tanzu for vSphere Clusters Using ClusterAPI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://darrylcauldwell.github.io/post/tanzu-tmc&#34;&gt;Managing Tanzu for vSphere Clusters Using Tanzu Misson Control&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://darrylcauldwell.github.io/post/tanzu-tmc-aws&#34;&gt;Deploying Tanzu for AWS Using Tanzu Misson Control&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;supervisor-cluster-bootstrap&#34;&gt;Supervisor Cluster Bootstrap&lt;/h2&gt;
&lt;p&gt;The enablement of Workload Control Plane triggers the deployment of three VMs which initially have single vNIC eth0 connected to a PortGroup which can route to vCenter and NSX. This appears to configure the first VM as bootstrap Kubernetes Node on which are deployed multiple Pods. The NSX Container Plug-in (NCP) Pod performs integration of Kubernetes and NSX-T.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl get pods -n vmware-system-nsx
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;NAME                       READY   STATUS    RESTARTS   AGE
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;nsx-ncp-7c6578d9fd-xj6jk   1/1     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;          25h
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;During the bootstrap of the Supervisor Cluster the requires NSX objects are created by the NCP Pod.  The NCP Pod is configured with a ConfigMap which includes details of how to connect to the NSX API.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;```&lt;/span&gt;bash
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl get pods -A | grep ncp
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vmware-system-nsx                           nsx-ncp-7c6578d9fd-xdqvl                                          1/1     Running     &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;          2d22h
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl describe pod nsx-ncp-7c6578d9fd-xdqvl -n vmware-system-nsx | grep config
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ConfigMapName:       nsx-ncp-config
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl describe ConfigMap nsx-ncp-config -n vmware-system-nsx | grep manager
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;nsx_api_managers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 192.168.10.28:443
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;routing-domains&#34;&gt;Routing Domains&lt;/h2&gt;
&lt;p&gt;The Workload Control Plane configures various components on discreet network segments. The Management, Ingress and Egress networks all need to be part of a common routing. In this deployment the Managemenet network operates within a VLAN defined on the physical network. The Workload Control Plane deployment creates the Ingress and Egress networks as NSX-T logical segments. To faciliate the correct sharing of routes the NSX-T Tier-0 router is configured as a BGP Autonomous System which peers with the physical router, both advertise and share routes.&lt;/p&gt;
&lt;h2 id=&#34;no-nat-topology&#34;&gt;No-NAT Topology&lt;/h2&gt;
&lt;p&gt;It is possible [from vCenter Server 7.0 Update 3] to deploy a No-NAT (routed) topology which allows routing outside of the cluster network, more info on No-NAT &lt;a href=&#34;https://docs.vmware.com/en/VMware-vSphere/7.0/vmware-vsphere-with-tanzu/GUID-2BC5CC9D-7396-4700-A698-3C97A882AE23.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;nat-topology&#34;&gt;NAT Topology&lt;/h2&gt;
&lt;p&gt;In this deployment I deployed the default NAT (routed) topology where the Kubernetes POD and Service are within the same routing domain.  Communications to the wider network is facilitated by combination of NAT, Load Balancer Ingress and Egress.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-basic-nsx-nat.drawio.png&#34; alt=&#34;Tanzu Basic with NSX-T NAT&#34;&gt;&lt;/p&gt;
&lt;p&gt;A NSX-T logical segment is created for ther Kubernetes Cluster network ( 10.244.0.0 /20 ) which all the Nodes and Pods will attach.&lt;/p&gt;
&lt;p&gt;NSX-T Service Load Balancers get created which correspond to the Ingress resources:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Pod&lt;/th&gt;
&lt;th&gt;Ingress IP Allocation&lt;/th&gt;
&lt;th&gt;Port&lt;/th&gt;
&lt;th&gt;Protocol&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;vsphere-csi-controller&lt;/td&gt;
&lt;td&gt;172.16.1.1&lt;/td&gt;
&lt;td&gt;2112&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vsphere-csi-controller&lt;/td&gt;
&lt;td&gt;172.16.1.1&lt;/td&gt;
&lt;td&gt;2113&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-apiserver&lt;/td&gt;
&lt;td&gt;172.16.1.2&lt;/td&gt;
&lt;td&gt;6443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-apiserver&lt;/td&gt;
&lt;td&gt;172.16.1.2&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;NSX-T Distributed Load Balancers get created which correspond to the ClusterIP service resources:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Pod&lt;/th&gt;
&lt;th&gt;Ingress IP Allocation&lt;/th&gt;
&lt;th&gt;Port&lt;/th&gt;
&lt;th&gt;Protocol&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;kubernetes&lt;/td&gt;
&lt;td&gt;10.96.0.1&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-dns&lt;/td&gt;
&lt;td&gt;10.96.0.10&lt;/td&gt;
&lt;td&gt;53&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-dns&lt;/td&gt;
&lt;td&gt;10.96.0.10&lt;/td&gt;
&lt;td&gt;53&lt;/td&gt;
&lt;td&gt;L4 UDP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-dns&lt;/td&gt;
&lt;td&gt;10.96.0.10&lt;/td&gt;
&lt;td&gt;9153&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tkg-vmware-system-tkg-webhook-service&lt;/td&gt;
&lt;td&gt;10.96.0.57&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vmop-webhook-service&lt;/td&gt;
&lt;td&gt;10.96.0.70&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;capw-capw-controller-manager-metrics-service&lt;/td&gt;
&lt;td&gt;10.96.0.72&lt;/td&gt;
&lt;td&gt;9846&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;capw-capw-controller-webhook-service&lt;/td&gt;
&lt;td&gt;10.96.0.168&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-apiserver-lb-svc&lt;/td&gt;
&lt;td&gt;10.96.0.186&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-apiserver-lb-svc&lt;/td&gt;
&lt;td&gt;10.96.0.186&lt;/td&gt;
&lt;td&gt;6443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tkg-vmware-system-tkg-controller-manager-metrics-service&lt;/td&gt;
&lt;td&gt;10.96.0.202&lt;/td&gt;
&lt;td&gt;9847&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vmop-controller-manager-metrics-service&lt;/td&gt;
&lt;td&gt;10.96.0.204&lt;/td&gt;
&lt;td&gt;9848&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;capw-capi-kubeadm-bootstrap-webhook-service&lt;/td&gt;
&lt;td&gt;10.96.1.44&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;capw-capi-controller-manager-metrics-service&lt;/td&gt;
&lt;td&gt;10.96.1.45&lt;/td&gt;
&lt;td&gt;9844&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nsop-vmware-system-nsop-webhook-service&lt;/td&gt;
&lt;td&gt;10.96.1.87&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;capw-capi-kubeadm-control-plane-controller-manager-metrics-service&lt;/td&gt;
&lt;td&gt;10.96.1.95&lt;/td&gt;
&lt;td&gt;9848&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vmware-system-license-operator-webhook-service&lt;/td&gt;
&lt;td&gt;10.96.1.112&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;capw-capi-kubeadm-bootstrap-controller-manager-metrics-service&lt;/td&gt;
&lt;td&gt;10.96.1.136&lt;/td&gt;
&lt;td&gt;9845&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;cert-manager-cert-manager&lt;/td&gt;
&lt;td&gt;10.96.1.149&lt;/td&gt;
&lt;td&gt;9402&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;psp-operator-webhook-service&lt;/td&gt;
&lt;td&gt;10.96.1.150&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;capw-capi-webhook-service&lt;/td&gt;
&lt;td&gt;10.96.1.163&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;docker-registry&lt;/td&gt;
&lt;td&gt;10.96.1.170&lt;/td&gt;
&lt;td&gt;5000&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;apiserver-authproxy&lt;/td&gt;
&lt;td&gt;10.96.1.173&lt;/td&gt;
&lt;td&gt;8443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;csi-vsphere-csi-controller&lt;/td&gt;
&lt;td&gt;10.96.1.208&lt;/td&gt;
&lt;td&gt;2112&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;csi-vsphere-csi-controller&lt;/td&gt;
&lt;td&gt;10.96.1.208&lt;/td&gt;
&lt;td&gt;2113&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;capw-capi-kubeadm-control-plane-webhook-service&lt;/td&gt;
&lt;td&gt;10.96.0.235&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tkg-tkgs-plugin-service&lt;/td&gt;
&lt;td&gt;10.96.1.245&lt;/td&gt;
&lt;td&gt;8099&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;cert-manager-cert-manager-webhook&lt;/td&gt;
&lt;td&gt;10.96.1.252&lt;/td&gt;
&lt;td&gt;443&lt;/td&gt;
&lt;td&gt;L4 TCP&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The following three NAT rules are put in place to facilitate Egress with NSX-T:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Action&lt;/th&gt;
&lt;th&gt;Source&lt;/th&gt;
&lt;th&gt;Destination&lt;/th&gt;
&lt;th&gt;Translation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;SNAT&lt;/td&gt;
&lt;td&gt;Any&lt;/td&gt;
&lt;td&gt;Any&lt;/td&gt;
&lt;td&gt;172.16.2.1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No SNAT&lt;/td&gt;
&lt;td&gt;10.244.0.0/20&lt;/td&gt;
&lt;td&gt;10.244.0.0/20&lt;/td&gt;
&lt;td&gt;Any&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;No SNAT&lt;/td&gt;
&lt;td&gt;10.244.0.0/20&lt;/td&gt;
&lt;td&gt;172.16.2.0/24&lt;/td&gt;
&lt;td&gt;Any&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;supervisor-cluster-dns-complexities&#34;&gt;Supervisor Cluster DNS Complexities&lt;/h2&gt;
&lt;p&gt;At this stage the VMs have a single NIC eth0 and management DNS resolver.&lt;/p&gt;
&lt;p&gt;Once the NCP has created the required NSX objects the bootstrap node has a 2nd NIC added eth1 which is connected to the Cluster Network segment. This enables configuration to proceed on the bootstrap host after which the 2nd and 3rd Nodes are configured and have 2nd NIC added onnected to the Cluster Network segment.&lt;/p&gt;
&lt;p&gt;I initially had issue where the installation status got stuck in “configuring” state. When I SSH to the first control plane VM I saw various error messages with various components. For example authorization errors were due to the kube-apiserver not being able to resolve the vCenter’s fqdn for SSO.  I had the NCP Pod moving to CrashLoopBackoff status again with what looked like fqdn lookup issues. This was confusing as during the earlier stages of the bootstrap process the bootstrap node had connected to vCenter and NSX using Management DNS resolver.&lt;/p&gt;
&lt;p&gt;When working through the WCP installation UI there are two places to enter DNS server IP address.  The first is a DNS resolver with A and PTR records for the management components.  The Kubernetes CoreDNS performs lookups between Pods and Services within the Cluster. The second DNS entry what CoreDNS falls back to for FQDN lookups external of the Cluster. In my environment I have a single DNS solution which hosts both management and workload zones so the IP address (192.168.10.10) is common.&lt;/p&gt;
&lt;p&gt;When I connect to first control plane VM I noticed I could ping DNS server via its management interface but could not ping DNS via workload interface.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ifconfig
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;eth0      Link encap:Ethernet  HWaddr 00:50:56:b3:29:f3
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          inet addr:192.168.10.45  Bcast:192.168.10.255  Mask:255.255.255.0
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;eth1      Link encap:Ethernet  HWaddr 04:50:56:00:d0:00
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          inet addr:10.244.0.2  Bcast:10.244.0.15  Mask:255.255.255.240
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ping 192.168.10.10 -I eth0 -c &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;PING 192.168.10.10 &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;192.168.10.10&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; from 192.168.10.45 eth0: 56&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;84&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; bytes of data.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt; bytes from 192.168.10.10: icmp_seq&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; ttl&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt; time&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0.412 ms
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;--- 192.168.10.10 ping statistics ---
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; packets transmitted, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; received, 0% packet loss, time 0ms
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;rtt min/avg/max/mdev &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 0.412/0.412/0.412/0.000 ms
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ping 192.168.10.10 -I eth1 -c &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;PING 192.168.10.10 &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;192.168.10.10&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; from 10.244.0.2 eth1: 56&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;84&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; bytes of data.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;From 10.244.0.1 icmp_seq&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; Destination Host Unreachable
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;--- 192.168.10.10 ping statistics ---
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; packets transmitted, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; received, +1 errors, 100% packet loss, time 0ms
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So here I know I have a routing or Egress issue from workload to management routing domain. I would expect however the VM should still resolve vCenter and NSX-T records on management DNS and bootstrapping should comlete. I checked the network config to ensure that eth0 had correct DNS server IP and the routing table appeared correct.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cat /etc/systemd/network/10-eth0.network | grep DNS
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;DNS &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 192.168.10.10
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cat /etc/systemd/network/10-eth1.network | grep DNS
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;DNS &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 10.244.0.2
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;route -n
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Kernel IP routing table
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;0.0.0.0         192.168.10.254  0.0.0.0         UG    &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;      &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; eth0
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;10.244.0.0      0.0.0.0         255.255.255.240 U     &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;      &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; eth1
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;192.168.10.0    0.0.0.0         255.255.255.0   U     &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;      &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; eth0
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Looking deeper at routing policy redirects traffic to 192.168.10.10 via eth1.  As we know routing from eth1 was to DNS was not possible this causes the DNS issue.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ip rule show
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;0:      from all lookup local
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;0:      from all to 192.168.10.10  lookup &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;0:      from all to 172.16.1.0 /24 lookup &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;0:      from all to 10.244.0.0 /20 lookup &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;0:      from 10.244.0.2 lookup &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;0:      from all to 100.64.0.0 /16 lookup &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;0:      from all to 10.96.0.0 /23 lookup &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;32766:  from all lookup main
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;32767:  from all lookup default
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ip route show table &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;default via 10.244.0.1 dev eth1 proto static
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;10.244.0.0/28 dev eth1 proto static scope link
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Interestingly if I remove Workload Management and then redeploy specifing Management DNS 192.168.10.10 but chaning Workload DNS to alternate IP but still on unroutable address 192.168.10.11. The deployment does not get stuck at the same place and completes successfully.  Looking at routing policy having unique DNS IP addressing does not force traffic to 192.168.10.10 via eth1.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ip rule show
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;0:      from all lookup local
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;0:      from all to 172.16.1.0 /24 lookup &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;0:      from all to 10.244.0.0 /20 lookup &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;0:      from all to 10.96.0.0 /23 lookup &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;0:      from 10.244.0.2 lookup &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;0:      from all to 100.64.0.0 /16 lookup &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;0:      from all to 192.168.10.11  lookup &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;32766:  from all lookup main
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;32767:  from all lookup default
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ip route show table &lt;span style=&#34;color:#ae81ff&#34;&gt;200&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;default via 10.244.0.1 dev eth1 proto static
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;10.244.0.0/28 dev eth1 proto static scope link
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;workload-cluster-dns-traffic&#34;&gt;Workload Cluster DNS Traffic&lt;/h2&gt;
&lt;p&gt;While useful to understand if the workloa cluster CoreDNS cannot route to the configured upstream DNS server it will cause an issue later so the unerlying cause needs resolving.  For me the issue was the DNS server was dual homed and required static route adding for 172.16.0.0/16 to redirect to the router which was BGP peered to the NSX Edge.&lt;/p&gt;
&lt;h2 id=&#34;kubectl-vsphere-login-traffic&#34;&gt;Kubectl vSphere Login Traffic&lt;/h2&gt;
&lt;p&gt;To connect to the Supervisor Cluster we connect to the LoadBalancer VIP representation of Kubernetes API ( 172.16.1.2 ).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;set KUBECTL_VSPHERE_PASSWORD&lt;span style=&#34;color:#f92672&#34;&gt;={&lt;/span&gt;my password&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl vsphere login --server&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;172.16.1.2 --vsphere-username administrator@vsphere.local --insecure-skip-tls-verify --verbose &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl config use-context 172.16.1.2
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
        
      </description>
    </item>
    
    <item>
      <title>VMware Event Broker Appliance (VEBA) - Knative</title>
      <link>https://darrylcauldwell.github.io/post/veba-knative/</link>
      <pubDate>Fri, 30 Apr 2021 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/veba-knative/</guid>
      <description>
        
          &lt;p&gt;When I worked as an architect working with AWS I used event-driven automation with AWS Lambda to integrate distributed systems. This event-driven automation allowed me to put complex systems in place very simply. The VMware Event Broker Appliance (VEBA) aims to facilitate event-driven automation based on vCenter Server events. The VMware Event Broker Appliance is made available as a &lt;a href=&#34;https://flings.vmware.com/vmware-event-broker-appliance&#34;&gt;fling&lt;/a&gt;. The &lt;a href=&#34;https://vmweventbroker.io/kb/architecture&#34;&gt;system architecture&lt;/a&gt; shows that the appliance is built on a Photon OS running Kubernetes with Contour acting as ingress controller. The event broker appliance is composed of two components an event router and a choice of event stream processor Knative, OpenFaaS or AWS EventBridge.&lt;/p&gt;
&lt;p&gt;The appliance OVA installation takes parameters that configure the event router with a binding to vCenter Server. Once installed we can take a look at the resource description.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## SSH to appliance&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;cat config/event-router-config.yml&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;event-router.vmware.com/v1alpha1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;RouterConfig&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;router-config-knative&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;eventProcessor&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;veba-knative&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;knative&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;knative&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;insecureSSL&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;encoding&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;binary&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;destination&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;ref&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;eventing.knative.dev/v1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Broker&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;vmware-functions&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;eventProvider&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;veba-vc-01&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;vcenter&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;vcenter&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;address&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https://&amp;lt;MY VCENTER FQDN&amp;gt;/sdk&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;auth&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;basicAuth&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;password&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;lt;MY PASSWORD&amp;gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;username&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;administrator@vsphere.local&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;basic_auth&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;insecureSSL&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;checkpoint&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metricsProvider&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;default&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;bindAddress&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;0.0.0.0&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;8082&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;veba-metrics&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;eventProcessor&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;veba-knative&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;knative&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;knative&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;insecureSSL&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;false&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;encoding&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;binary&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;destination&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;ref&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;eventing.knative.dev/v1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Broker&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;vmware-functions&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;knative&#34;&gt;Knative&lt;/h2&gt;
&lt;p&gt;Knative is an open-source community project which extends a Kubernetes deployment with components for deploying, running, and managing serverless applications. It consists of three core components, build, serving and eventing. The build component offers a flexible approach to building source code into containers. The serving component enables rapid deployment and automatic scaling of containers through a request-driven model for serving workloads based on demand. The eventing component enables universal subscription, delivery, and management of events. Knative eventing is composed of Knative Broker and Trigger objects which make it easy to filter events based on event attributes. A Broker provides a bucket of events that can be selected by attribute. It receives events and forwards them to subscribers defined by one or more matching Triggers.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/veba-knative-broker-trigger-overview.svg&#34; alt=&#34;Broker Trigger Architecture&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;consuming-example-knative-function&#34;&gt;Consuming Example Knative Function&lt;/h2&gt;
&lt;p&gt;The vmware-samples GitHub repository contains a folder containing &lt;a href=&#34;https://github.com/vmware-samples/vcenter-event-broker-appliance/tree/development/examples/knative&#34;&gt;example Knative functions&lt;/a&gt;. The most simplistic example of a function is kn-ps-echo this receives all events from the event router and outputs them to Stdout.&lt;/p&gt;
&lt;p&gt;A Knative function requires a container image the example function is in a publicly available repository projects.registry.vmware.com/veba/kn-ps-echo. Within the example folder is the Dockerfile used to create the container. We can see this defines a Powershell runtime environment with the &lt;a href=&#34;https://www.powershellgallery.com/packages/CloudEvents.Sdk&#34;&gt;CloudEvents SDK&lt;/a&gt; and &lt;a href=&#34;https://www.powershellgallery.com/packages/ThreadJob&#34;&gt;ThreadJob&lt;/a&gt; modules installed. We can see two Powershell scripts are copied in server.ps1 and handler.ps1 and that server.ps1 is executed when the container is started. The server.ps1 acts as a System.Net.HttpListener which receives the event and on message receipt passes this on to handler.ps1 to execute.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-dockerfile&#34; data-lang=&#34;dockerfile&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;FROM&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; photon:3.0&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;ENV&lt;/span&gt; TERM linux&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;ENV&lt;/span&gt; PORT &lt;span style=&#34;color:#ae81ff&#34;&gt;8080&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Set terminal. If we don&amp;#39;t do this, weird readline things happen.&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; echo &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/usr/bin/pwsh&amp;#34;&lt;/span&gt; &amp;gt;&amp;gt; /etc/shells &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    echo &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/bin/pwsh&amp;#34;&lt;/span&gt; &amp;gt;&amp;gt; /etc/shells &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    tdnf install -y powershell-7.0.3-2.ph3 unzip &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    pwsh -c &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Set-PSRepository -Name PSGallery -InstallationPolicy Trusted&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    find / -name &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;net45&amp;#34;&lt;/span&gt; | xargs rm -rf &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    tdnf erase -y unzip &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;    tdnf clean all&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; pwsh  -Command &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Install-Module ThreadJob -Force -Confirm:$false&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;RUN&lt;/span&gt; pwsh -Command &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Install-Module -Name CloudEvents.Sdk&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;COPY&lt;/span&gt; server.ps1 ./&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;COPY&lt;/span&gt; handler.ps1 handler.ps1&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;CMD&lt;/span&gt; [&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;pwsh&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;./server.ps1&amp;#34;&lt;/span&gt;]&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As the manifest file and dependant container image are publicly available to test we can just execute the manifest file and it will create the Knative kservice and trigger resources.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl -n vmware-function -f https://github.com/vmware-samples/vcenter-event-broker-appliance/blob/development/examples/knative/powershell/kn-ps-echo/function.yaml
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;service.serving.knative.dev/kn-ps-echo created
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;trigger.eventing.knative.dev/kn-ps-echo-trigger created
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;vCenter Server constantly generates events and as the example has an unfiltered trigger events are output to Stdout. We can view the output by looking at the log file of the container executing the script.  If we list the pods in the vmware-functions namespace and filter using function name we can see two pods and the deployment pod has two containers.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl -n vmware-functions get pods | grep kn-ps-echo
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;NAME                                             READY   STATUS    RESTARTS   AGE
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kn-ps-echo-00001-deployment-6c9f77855c-ddz8w     2/2     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          18m
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kn-ps-echo-trigger-dispatcher-7bc8f78d48-5cwc7   1/1     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          18m
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The scripts are run in a container named user-container in the deployment pod so we can follow its logs and see the flow of vCenter events being echo&amp;rsquo;d.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl logs -na vmware-functions kn-ps-echo-00001-deployment-6c9f77855c-ddz8w user-container --follow

Server start listening on &amp;#39;http://*:8080/&amp;#39;
Cloud Event
  Source: https://&amp;lt;MY VCENTER FQDN&amp;gt;/sdk
  Type: com.vmware.event.router/event
  Subject: UserLogoutSessionEvent
  Id: b2cb5b99-baf2-4b0b-93e7-33795e56ec88
CloudEvent Data:



Cloud Event
  Source: https://&amp;lt;MY VCENTER FQDN&amp;gt;/sdk
  Type: com.vmware.event.router/event
  Subject: UserLoginSessionEvent
  Id: 4256ead8-b86d-4bc0-96ac-92ccaae02605
CloudEvent Data:



Cloud Event
  Source: https://&amp;lt;MY VCENTER FQDN&amp;gt;/sdk
  Type: com.vmware.event.router/event
  Subject: UserLogoutSessionEvent
  Id: a160d7bd-542d-4729-98bd-bbb14d505373
CloudEvent Data:
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So we can see all events are of the same Type: but the Subject: is populated with the Event name.&lt;/p&gt;
&lt;p&gt;Note: At the time of running the kn-ps-echo didn&amp;rsquo;t seem to be working correctly as it did not output the CloudEvent Data.&lt;/p&gt;
&lt;h2 id=&#34;creating-a-knative-function&#34;&gt;Creating A Knative Function&lt;/h2&gt;
&lt;p&gt;So we can see it is very easy to consume a pre-built function but I wonder how hard it is to create one to meet a bespoke need.  Pleased to report that it turns out that is also pretty easy.&lt;/p&gt;
&lt;p&gt;If we start off by thinking about a use case for event driven architecture. I got a lot of value from eventing in AWS in forming loosely coupled relationship between discreet systems and maintaining synchronicity of state between the systems. vCenter Server and vRealize Operations Manager both have concept of maintenance mode and when performing lifecycle management tasks its common to enable this on both products. Here I&amp;rsquo;ll look at the usecase where when I set vCenter Server maintenance mode state the event triggers functions which mirror the state in vRealize Operations Manager.&lt;/p&gt;
&lt;p&gt;The first thing we need to do is identify the vCenter Server event created when a host is placed in maintenance mode. Checking the event documentaion we can find the two events are &lt;a href=&#34;https://vdc-repo.vmware.com/vmwb-repository/dcr-public/fe08899f-1eec-4d8d-b3bc-a6664c168c2c/7fdf97a1-4c0d-4be0-9d43-2ceebbc174d9/doc/vim.event.EnteredMaintenanceModeEvent.html&#34;&gt;EnteredMaintenanceModeEvent&lt;/a&gt; and &lt;a href=&#34;https://vdc-repo.vmware.com/vmwb-repository/dcr-public/fe08899f-1eec-4d8d-b3bc-a6664c168c2c/7fdf97a1-4c0d-4be0-9d43-2ceebbc174d9/doc/vim.event.ExitMaintenanceModeEvent.html&#34;&gt;ExitMaintenanceModeEvent&lt;/a&gt;. Next we need to establish a API call to set state in vRealize Operations Manager checking the APIU documentation the &lt;a href=&#34;https://vdc-download.vmware.com/vmwb-repository/dcr-public/1e2150d8-8682-4213-a6f0-03fb3b1dc410/b10f698a-21c4-4194-84a5-d3aca9002a07/index.html#markResourceAsBeingMaintained&#34;&gt;markResourceAsBeingMaintained&lt;/a&gt; can be used to mark and unmark resourde maintenance mode.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll start by creating a container image for the enter event. The kn-ps-echo example Dockerfile and server.ps1 contain nothing I need to change so can be reused.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## Create folders and pull down reusable example files&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mkdir veba-knative-mm-enter
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cd veba-knative-mm-enter
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl -O https://raw.githubusercontent.com/vmware-samples/vcenter-event-broker-appliance/master/examples/knative/powershell/kn-ps-echo/Dockerfile
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl -O https://raw.githubusercontent.com/vmware-samples/vcenter-event-broker-appliance/master/examples/knative/powershell/kn-ps-echo/server.ps1
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;What I do need to do though is create a handler.ps1 which takes input from event and calls REST API of vROps.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-powershell&#34; data-lang=&#34;powershell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;Function&lt;/span&gt; Process-Handler {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   &lt;span style=&#34;color:#66d9ef&#34;&gt;param&lt;/span&gt;(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      [Parameter(&lt;span style=&#34;color:#a6e22e&#34;&gt;Position&lt;/span&gt;=&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#a6e22e&#34;&gt;Mandatory&lt;/span&gt;=$true)][&lt;span style=&#34;color:#66d9ef&#34;&gt;CloudNative.CloudEvents.CloudEvent&lt;/span&gt;]$CloudEvent
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   )
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Form cloudEventData object and output to console for debugging&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$cloudEventData = $cloudEvent | Read-CloudEventJsonData -ErrorAction SilentlyContinue -Depth &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt;($cloudEventData &lt;span style=&#34;color:#f92672&#34;&gt;-eq&lt;/span&gt; $null) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   $cloudEventData = $cloudEvent | Read-CloudEventData
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Write-Host &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Full contents of CloudEventData&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;`n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt; &lt;/span&gt;$(${cloudEventData} | ConvertTo-Json)&lt;span style=&#34;color:#ae81ff&#34;&gt;`n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Extract hostname from CloudEventData object&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$esxiHost=$cloudEventData.Host.Name
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Write-Host &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Hostname from CloudEventData&amp;#34;&lt;/span&gt; $esxiHost
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## Check secret in place which supplies vROps environment variables&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Write-Host &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;vropsFqdn:&amp;#34;&lt;/span&gt; ${env&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;:&lt;/span&gt;vropsFqdn}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Write-Host &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;vropsUser:&amp;#34;&lt;/span&gt; ${env&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;:&lt;/span&gt;vropsUser}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Write-Host &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;vropsPassword:&amp;#34;&lt;/span&gt; ${env&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;:&lt;/span&gt;vropsPassword}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## Form unauthorized headers payload&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$headers = @{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Content-Type&amp;#34;&lt;/span&gt; = &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;application/json&amp;#34;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Accept&amp;#34;&lt;/span&gt;  = &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;application/json&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## Acquire bearer token using environment variables&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$uri = &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://&amp;#34;&lt;/span&gt; + $env:vropsFqdn + &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/suite-api/api/auth/token/acquire&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$basicAuthBody = @{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    username = $env:vropsUser;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    password = $env:vropsPassword;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$basicAuthBodyJson = $basicAuthBody | ConvertTo-Json -Depth &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Write-Host &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Acquiring bearer token ...&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$bearer = Invoke-WebRequest -Uri $uri -Method POST -Headers $headers -Body $basicAuthBodyJson -SkipCertificateCheck | ConvertFrom-Json
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Write-Host &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bearer token is&amp;#34;&lt;/span&gt; $bearer.token
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## Form authorized headers payload&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$authedHeaders = @{
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Content-Type&amp;#34;&lt;/span&gt; = &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;application/json&amp;#34;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Accept&amp;#34;&lt;/span&gt;  = &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;application/json&amp;#34;&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Authorization&amp;#34;&lt;/span&gt; = &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;vRealizeOpsToken &amp;#34;&lt;/span&gt; + $bearer.token
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;   }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## Get host ResourceID&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$uri = &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://&amp;#34;&lt;/span&gt; + $env:vropsFqdn + &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/suite-api/api/adapterkinds/VMWARE/resourcekinds/HostSystem/resources?name=&amp;#34;&lt;/span&gt; + $esxiHost
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Write-Host &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Acquiring host ResourceID ...&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$resource = Invoke-WebRequest -Uri $uri -Method GET -Headers $authedHeaders -SkipCertificateCheck
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$resourceJson = $resource.Content | ConvertFrom-Json
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Write-Host &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ResourceID of host is &amp;#34;&lt;/span&gt; $resourceJson.resourceList[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;].identifier
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## Mark host resource as in maintenance mode&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$uri = &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://&amp;#34;&lt;/span&gt; + $env:vropsFqdn + &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/suite-api/api/resources/&amp;#34;&lt;/span&gt; + $resourceJson.resourceList[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;].identifier + &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/maintained&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Write-Host &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Marking host as vROps maintenance mode ...&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Invoke-WebRequest -Uri $uri -Method PUT -Headers $authedHeaders -SkipCertificateCheck
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;## Get host resource maintenance mode state&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$uri = &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://&amp;#34;&lt;/span&gt; + $env:vropsFqdn + &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/suite-api/api/adapterkinds/VMWARE/resourcekinds/HostSystem/resources?name=&amp;#34;&lt;/span&gt; + $esxiHost
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Write-Host &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Acquiring host maintenance mode state ...&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$resource = Invoke-WebRequest -Uri $uri -Method GET -Headers $authedHeaders -SkipCertificateCheck
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$resourceJson = $resource.Content | ConvertFrom-Json
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Write-Host &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Host maintenence mode state is &amp;#34;&lt;/span&gt; $resourceJson.resourceList[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;].resourceStatusStates[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;].resourceState
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Write-Host &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Note: STARTED=Not In Maintenance | MAINTAINED_MANUAL=In Maintenance&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In order to be environmentally agnositic I have the script use Environment Variables. These can be storedm in a Kubernetes Secret resource which can be associated with the containers and available at script runtime.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;kubectl -n vmware-functions create secret generic veba-knative-mm-vrops \
  --from-literal=vropsFqdn=&amp;lt;MY VCENTER FQDN&amp;gt; \
  --from-literal=vropsUser=admin \
  --from-literal=vropsPassword=&amp;#39;VMware1!&amp;#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With the Dockerfile and scripts in place we can look to build the container image locally and then push this to a public container registry.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Build local image with tag for GitHub Container Registry&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker build --tag ghcr.io/darrylcauldwell/veba-ps-enter-mm:1.0 .
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Generate GitHub Personal Access Token&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Connect to GitHub Container Registry&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Use Personal Access Token when prompted for password&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker login ghcr.io -u darrylcauldwell
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Push local image to GitHub Container Registry&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker push ghcr.io/darrylcauldwell/veba-ps-enter-mm:1.0
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once the container is available in public repository I can look to create a Knative service and trigger resource which links to container image and has association with the secret:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;serving.knative.dev/v1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;veba-ps-enter-mm-service&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;veba-ui&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;vmware-functions&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;template&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;autoscaling.knative.dev/maxScale&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;autoscaling.knative.dev/minScale&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;containers&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        - &lt;span style=&#34;color:#f92672&#34;&gt;image&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ghcr.io/darrylcauldwell/veba-ps-enter-mm:1.0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;          &lt;span style=&#34;color:#f92672&#34;&gt;envFrom&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            - &lt;span style=&#34;color:#f92672&#34;&gt;secretRef&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;veba-knative-mm-vrops&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;---
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;eventing.knative.dev/v1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Trigger&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;veba-ps-enter-mm-trigger&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;labels&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;app&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;veba-ui&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;vmware-functions&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;broker&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;default&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;filter&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;attributes&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;com.vmware.event.router/event&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#f92672&#34;&gt;subject&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;EnteredMaintenanceModeEvent&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;subscriber&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;ref&lt;/span&gt;:
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;serving.knative.dev/v1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;veba-ps-enter-mm-service&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The exit function is logically laid out the same but with slightly different trigger filter and handler action. With the manifest files created both can now be applied.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl apply -f https://raw.githubusercontent.com/darrylcauldwell/veba-knative-mm-enter/main/veba-knative-mm-enter.yml
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;service.serving.knative.dev/veba-ps-enter-mm-service created
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;trigger.eventing.knative.dev/veba-ps-enter-mm-trigger created
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl apply -f https://raw.githubusercontent.com/darrylcauldwell/veba-knative-mm-exit/master/veba-knative-mm-exit.yml
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;service.serving.knative.dev/veba-ps-exit-mm-service created
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;trigger.eventing.knative.dev/veba-ps-exit-mm-trigger created
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once the container image has pulled down from repository can check the created resources.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl -n vmware-functions get kservice | grep mm
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;NAME                       URL                                                                LATESTCREATED                    LATESTREADY                      READY   REASON
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;veba-ps-enter-mm-service   http://veba-ps-enter-mm-service.vmware-functions.veba.cork.local   veba-ps-enter-mm-service-00001   veba-ps-enter-mm-service-00001   True
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;veba-ps-exit-mm-service    http://veba-ps-exit-mm-service.vmware-functions.veba.cork.local    veba-ps-exit-mm-service-00001    veba-ps-exit-mm-service-00001    True
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl -n vmware-functions get triggers | grep mm
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;NAME                       BROKER    SUBSCRIBER_URI                                                       AGE    READY   REASON
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;veba-ps-enter-mm-trigger   default   http://veba-ps-enter-mm-service.vmware-functions.svc.cluster.local   61s    True
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;veba-ps-exit-mm-trigger    default   http://veba-ps-exit-mm-service.vmware-functions.svc.cluster.local    45s    True
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl -n vmware-functions get pods | grep mm
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;veba-ps-enter-mm-service-00001-deployment-d689d7fbd-9gtlv   2/2     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          87s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;veba-ps-enter-mm-trigger-dispatcher-848ff8c858-qnxg8        1/1     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          76s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;veba-ps-exit-mm-service-00001-deployment-b98b6f795-chqpx    2/2     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          71s
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;veba-ps-exit-mm-trigger-dispatcher-5fc8cbc978-6n2nf         1/1     Running   &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;          65s
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With the functions in place we can follow the logs on the container and place a host into maintenance mode to check it works.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;kubectl -n vmware-functions logs veba-ps-enter-mm-service-00001-deployment-d689d7fbd-9gtlv user-container --follow
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Server start listening on &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;http://*:8080/&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Full contents of CloudEventData
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ChangeTag&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ChainId&amp;#34;&lt;/span&gt;: 8122827,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Host&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;lt;MY HOST&amp;gt;&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Host&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Value&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;host-19&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;HostSystem&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ComputeResource&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Virtual-SAN-Cluster-5425b3e6-6e38-4221-8804-500f1360c7a3&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ComputeResource&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Value&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;domain-c9&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ClusterComputeResource&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Net&amp;#34;&lt;/span&gt;: null,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Datacenter&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Datacenter&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Datacenter&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Value&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;datacenter-3&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Type&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Datacenter&amp;#34;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Vm&amp;#34;&lt;/span&gt;: null,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Dvs&amp;#34;&lt;/span&gt;: null,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;UserName&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;VSPHERE.LOCAL\\Administrator&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;CreatedTime&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2021-05-06T16:20:05.137999Z&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;FullFormattedMessage&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Host &amp;lt;MY HOST&amp;gt; in Datacenter has entered maintenance mode&amp;#34;&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Key&amp;#34;&lt;/span&gt;: 8122856,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Ds&amp;#34;&lt;/span&gt;: null
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Hostname from CloudEventData esx01.cork.local
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vropsFqdn: vrops.cork.local
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vropsUser: admin
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;vropsPassword: VMware1!
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Acquiring bearer token ...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Bearer token is b3898f1c-3a94-4dff-8b80-2ab835bd53bb::1c46ab4d-0e5e-44e4-ba4b-9898811bc645
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Acquiring host ResourceID ...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ResourceID of host is  8f07b6de-9918-4849-af0f-7a1cca3ff5c7
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Marking host as vROps maintenance mode ...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Acquiring host maintenance mode state ...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Host maintenence mode state is  MAINTAINED_MANUAL
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Note: STARTED&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Not In Maintenance | MAINTAINED_MANUAL&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;In Maintenance
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
        
      </description>
    </item>
    
    <item>
      <title>Programaticly configuring VMware Storage Profile API with Python</title>
      <link>https://darrylcauldwell.github.io/post/python_spbm/</link>
      <pubDate>Wed, 12 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/python_spbm/</guid>
      <description>
        
          &lt;p&gt;I was looking to programaticly configuring VMware Storage Profile recently. My scripting language of choice is Python, VMware maintain &lt;a href=&#34;https://github.com/vmware/pyvmomi&#34;&gt;pyVmomi&lt;/a&gt; is which is the Python SDK for the VMware vSphere API. pyVmomi can be used to form binding and configure the &lt;a href=&#34;https://code.vmware.com/apis/968/vsphere&#34;&gt;vSphere Web Services API&lt;/a&gt; and the &lt;a href=&#34;https://code.vmware.com/apis/971&#34;&gt;VMware Storage Policy API&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The Storage Policy API is exposed as a Web service, running on VMware vCenter server systems.  When I approached the requirement it wasn&amp;rsquo;t clear to me how to connect to the Storage Policy API Web service. I read the &lt;a href=&#34;https://code.vmware.com/docs/11900/vmware-storage-policy-sdk-programming-guide&#34;&gt;VMware Storage Policy SDK Programming Guide&lt;/a&gt; section on forming connection which describes using the session cookie from a vCenter Server session to establish the Storage Policy session.&lt;/p&gt;
&lt;p&gt;pyVim is a client-side Python API which wraps pvVmomi these are made available as a package.  These are published to PyPI repository and as such can be installed easily with pip.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;pip install --upgrade pyvmomi
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;It we look at pyVmomi in GitHub we can see that &lt;a href=&#34;https://github.com/vmware/pyvmomi/blob/master/pyVim/connect.py&#34;&gt;pyVim.connect&lt;/a&gt; defines a function SmartConnect which can be used to form connection to the vSphere Web Services API.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;#!/usr/bin/env python3
# Connect to a VMOMI ServiceInstance.

import ssl argparse atexit getpass
from pyVim.connect import SmartConnect

def get_args():
    parser = argparse.ArgumentParser(
        description=&amp;#39;Arguments for talking to vCenter&amp;#39;)

    parser.add_argument(&amp;#39;-s&amp;#39;, &amp;#39;--host&amp;#39;,
                        required=True,
                        action=&amp;#39;store&amp;#39;,
                        help=&amp;#39;vCenter Server FQDN or IP address&amp;#39;)

    parser.add_argument(&amp;#39;-o&amp;#39;, &amp;#39;--port&amp;#39;,
                        type=int,
                        default=443,
                        action=&amp;#39;store&amp;#39;,
                        help=&amp;#39;vCenter Server TCP port&amp;#39;)

    parser.add_argument(&amp;#39;-u&amp;#39;, &amp;#39;--user&amp;#39;,
                        required=True,
                        action=&amp;#39;store&amp;#39;,
                        help=&amp;#39;Username to login to vCenter Server&amp;#39;)

    parser.add_argument(&amp;#39;-p&amp;#39;, &amp;#39;--password&amp;#39;,
                        required=False,
                        action=&amp;#39;store&amp;#39;,
                        help=&amp;#39;Password to login to vCenter Server&amp;#39;)

    args = parser.parse_args()

    if not args.password:
        args.password = getpass.getpass(prompt=&amp;#39;Enter password:&amp;#39;)

    return args

def main():
    args = get_args()

    &amp;#34;&amp;#34;&amp;#34; connect to vcenter service instance &amp;#34;&amp;#34;&amp;#34;

    context = None
    if hasattr(ssl, &amp;#34;_create_unverified_context&amp;#34;):
        context = ssl._create_unverified_context()

    serviceInstance = SmartConnect(
                    host=args.host,
                    user=args.user,
                    pwd=args.password,
                    port=args.port,
                    sslContext=context)

    atexit.register(Disconnect, serviceInstance)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To connect to the VMware Storage Policy API we need to get the session cookie. The &lt;a href=&#34;https://github.com/vmware/pyvmomi/blob/master/pyVmomi/VmomiSupport.py&#34;&gt;VmomiSupport&lt;/a&gt; provides us helper functions to do things like gather the context which includes session cookie details. With this we can form a stub session to the Storage Policy API.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;    &amp;#34;&amp;#34;&amp;#34; connect to pbm service instance &amp;#34;&amp;#34;&amp;#34;

    VmomiSupport.GetRequestContext()[&amp;#34;vcSessionCookie&amp;#34;] = \
    serviceInstance._stub.cookie.split(&amp;#39;&amp;#34;&amp;#39;)[1]
    hostname = serviceInstance._stub.host.split(&amp;#34;:&amp;#34;)[0]
    pbmStub = SoapStubAdapter(
        host=hostname,
        version=&amp;#34;pbm.version.version1&amp;#34;,
        path=&amp;#34;/pbm/sdk&amp;#34;,
        poolSize=0,
        sslContext=ssl._create_unverified_context())
    pbmServiceInstance = pbm.ServiceInstance(&amp;#34;ServiceInstance&amp;#34;, pbmStub)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With this connection in place we can make our request, in this example pull back all of the Storage Profiles.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;
    &amp;#34;&amp;#34;&amp;#34; get profiles &amp;#34;&amp;#34;&amp;#34;

    profileManager = pbmServiceInstance.RetrieveContent().profileManager
    profiles = profileManager.PbmQueryProfile(resourceType=pbm.profile.ResourceType(resourceType=&amp;#34;STORAGE&amp;#34;))
    print(profiles)

if __name__ == &amp;#39;__main__&amp;#39;:
    main()
&lt;/code&gt;&lt;/pre&gt;
        
      </description>
    </item>
    
    <item>
      <title>vSphere with Kubernetes Homelab Build</title>
      <link>https://darrylcauldwell.github.io/post/vsphere-k8s/</link>
      <pubDate>Wed, 29 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/vsphere-k8s/</guid>
      <description>
        
          &lt;p&gt;I&amp;rsquo;d been exploring Project Pacific during its beta in my homelab for a while. When vSphere 7 and NSX-T 3.0 went GA I took chance to rebuild lab and consolidate much of the configuration I&amp;rsquo;d been applying iteratively.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-vsphere.png&#34; alt=&#34;vSphere with Tanzu&#34;&gt;&lt;/p&gt;
&lt;p&gt;My lab hardware specification is a constraint so I&amp;rsquo;ve had to deviate from documentation in a few areas of configuration. During stand up and power up hosts experience CPU and RAM pressure but once everything is running in steady state it is tight but just fits.&lt;/p&gt;
&lt;h2 id=&#34;single-vlan--subnet-lab-network&#34;&gt;Single VLAN / subnet Lab Network&lt;/h2&gt;
&lt;p&gt;My lab has a very simple physical network namely a single subnet (192.168.1.0/24) with DHCP enabled and which has default route to the internet.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Host&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Allocation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ad&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;esx1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;esx2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;esx3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vcenter&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nsx&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;edge&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;t0-uplink&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tep-pool&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.20-24&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;workload control plane&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.30-34&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kubernetes Ingress&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.48-63&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kubernetes Egress&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.64-79&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DHCP&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.64-253&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gateway&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.254&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I look to configure subnets on the NSX overlay network using the other RFC1918 private address range. The physical router does not support dynamic routing protocol so I configure static routes for these two CIDR with next hop as tier-0 uplink IP 192.168.1.17.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;CIDR&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;IP Range&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;10.0.0.0/8&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;10.0.0.0 – 10.255.255.255&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;172.16.0.0/12&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;172.16.0.0 – 172.31.255.255&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;compute-resource-nodes&#34;&gt;Compute Resource Nodes&lt;/h2&gt;
&lt;p&gt;Lab compute resources are provided by three &lt;a href=&#34;https://ark.intel.com/content/www/us/en/ark/products/89190/intel-nuc-kit-nuc6i5syh.html&#34;&gt;Intel NUC6i5SYH&lt;/a&gt; hardware, each provides 2x 1.8Ghz CPU and 32GB RAM. I install ESXi boot partition on USB drives. To create a bootable USB for ESXi on macOS by creating a new VM in VMware Fusion with ESXi ISO attached as CD/DVD. It is only possible to connect USB to a running VM so power on VM and connect USB drive, work through ESXi installation via Fusion console.&lt;/p&gt;
&lt;p&gt;At the end of installation message to disconnect the CD/DVD and reboot VM.  I remain connected for reboot, using remote console navigate Troubleshoot menu to enable ESXi Shall and SSH, then configure networking to reflect homelab IP allocation.&lt;/p&gt;
&lt;h2 id=&#34;ensure-unique-system-uuid-and-mac-address&#34;&gt;Ensure unique System UUID and MAC address&lt;/h2&gt;
&lt;p&gt;Once ESXi has a basic network configuration I power down the VM and move USB to the Intel NUC and power on. During installation of ESXi the System UUID and MAC address are formed from the MAC address of the NIC. When  using Fusion to create multiple ESXi VMs this scenario can lead to duplicatation of System UUID and MAC address. We can configure ESXi to move to physical NIC MAC address and form new ESXi System UUID by running commands like.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;esxcli system settings advanced set -o /Net/FollowHardwareMac -i 1
sed -i &amp;#39;s#/system/uuid.*##&amp;#39; /etc/vmware/esx.conf
/sbin/auto-backup.sh
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;add-external-usb-nic-drivers&#34;&gt;Add external USB NIC drivers&lt;/h2&gt;
&lt;p&gt;The Intel NUC has a single onboard 1GB NIC, I add an external USB NIC to each NUC. ESXi doesn&amp;rsquo;t ship with required drivers for these external USB NIC, for labs these are provides via &lt;a href=&#34;https://flings.vmware.com/usb-network-native-driver-for-esxi&#34;&gt;VMware Flings&lt;/a&gt;. I copy the downloaded driver bundle to the /tmp folder on ESXi via an SFTP client like CyberDuck. Once the bundle is uploaded I following command to install the bundle.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;esxcli software vib install -d /tmp/ESXi700-VMKUSB-NIC-FLING-34491022-component-15873236.zip
esxcli system maintenanceMode set --enable true
esxcli system shutdown reboot --reason &amp;#34;USB NIC driver&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;suppress-warnings&#34;&gt;Suppress Warnings&lt;/h2&gt;
&lt;p&gt;When booting ESXi from USB the system logs and coredumps can not persist to local storage. I also prefer to leave SSH enabled in lab so get warnings. Both of these choices gives me warnings which I prefer to suppress.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;vim-cmd hostsvc/advopt/update UserVars.SuppressShellWarning long 1
vim-cmd hostsvc/advopt/update UserVars.SuppressCoredumpWarning long 1
vim-cmd hostsvc/advopt/update Syslog.global.logHost string 127.0.0.1
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;form-vsan-cluster&#34;&gt;Form vSAN Cluster&lt;/h2&gt;
&lt;p&gt;Two of the Intel NUC I use each have SSD the third is used to provide compute resources only. The ESXi installation allows Management traffic only on VMkernel port so need to bind vSAN to VMkernel port. In my lab only two devices contribute disk to vSAN the default storage policy for will prevent any VM deployment. To configure this I run the following on each host in cluster.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;esxcli system maintenanceMode set --enable false
esxcli vsan network ip add -i vmk0
esxcli vsan policy setdefault -c cluster -p &amp;#34;((\&amp;#34;hostFailuresToTolerate\&amp;#34; i0)&amp;#34;
esxcli vsan policy setdefault -c vdisk -p &amp;#34;((\&amp;#34;hostFailuresToTolerate\&amp;#34; i0)&amp;#34;
esxcli vsan policy setdefault -c vmnamespace -p &amp;#34;((\&amp;#34;hostFailuresToTolerate\&amp;#34; i0)&amp;#34;
esxcli vsan policy setdefault -c vmswap -p &amp;#34;((\&amp;#34;hostFailuresToTolerate\&amp;#34; i0) (\&amp;#34;forceProvisioning\&amp;#34; i1))&amp;#34;
esxcli vsan policy setdefault -c vmem -p &amp;#34;((\&amp;#34;hostFailuresToTolerate\&amp;#34; i0) (\&amp;#34;forceProvisioning\&amp;#34; i1))&amp;#34;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With the pre-requists in place a new vSAN cluster can be formed on first host using command like.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;esxcli vsan cluster new 
esxcli vsan cluster get | grep &amp;#34;Sub-Cluster UUID&amp;#34;
    Sub-Cluster UUID: 5291783f-77a6-c1dc-2ed8-6cfc200618b1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Add other nodes to the cluster using output of Sub-Cluster Master UUID using command like.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;esxcli vsan cluster join -u 5291783f-77a6-c1dc-2ed8-6cfc200618b1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;At this stage vSAN cluster while nodes are looking to form the quorum cannot be formed. Prior to vSAN 6.6 the cluster can discover members using multicast. Forming cluster without vCenter requires formation of unicast networking from the command line by manually building table &lt;a href=&#34;https://kb.vmware.com/s/article/2150303&#34;&gt;kb2150303&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;active-directory&#34;&gt;Active Directory&lt;/h2&gt;
&lt;p&gt;At this stage I upload Windows Server 2019 ISO to vSAN and use this to deploy a VM which acts as a environment Remote Desktop, File Store, DNS server, NTP source and Active Directory. Before proceeding create DNS A &amp;amp; PTR records for the environment.&lt;/p&gt;
&lt;h2 id=&#34;vcenter-server&#34;&gt;vCenter Server&lt;/h2&gt;
&lt;p&gt;I use the Active Directory jump server to mount the vCenter ISO and use UI installer to deploy a &amp;lsquo;Tiny&amp;rsquo; sized vCenter Server to existing Datastore. This creates a cluster with the move deployed to, at this stage I attach remaining hosts to cluster.&lt;/p&gt;
&lt;p&gt;We can then pass ownership of vSAN cluster membership back to vCenter by running following on each host.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;esxcfg-advcfg -s 0 /VSAN/IgnoreClusterMemberListupdates
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The Workload Control Plane feature requires DRS and HA are required to be enabled on clusters it manages. Ensure these are enabled and HA admission control is disabled to maintain capacity.&lt;/p&gt;
&lt;h2 id=&#34;default-vsan-storage-policy&#34;&gt;Default vSAN Storage Policy&lt;/h2&gt;
&lt;p&gt;During the setup of VSAN cluster we configured the local policies to FTT=0 as I only have 2x hosts providing disk capacity. To deploy VMs and appliances via vCenter similarly need to adjust the &amp;lsquo;vSAN Default Storage Policy&amp;rsquo; to &amp;lsquo;No data redundancy&amp;rsquo;.&lt;/p&gt;
&lt;h2 id=&#34;nsx-installation&#34;&gt;NSX Installation&lt;/h2&gt;
&lt;p&gt;Deploy NSX-T 3.0 appliance sized Small, I need to remove CPU reservation to power on.&lt;/p&gt;
&lt;h2 id=&#34;connect-nsx-and-vcenter&#34;&gt;Connect NSX and vCenter&lt;/h2&gt;
&lt;p&gt;Connect to NSX appliance navigate to System &amp;gt; Fabric &amp;gt; Compute Managers and create bindng to vCenter. In order vCenter Workload Platform services can communicate with NSX ensure Enable Trust option is checked.&lt;/p&gt;
&lt;h2 id=&#34;vsphere-distributed-switch&#34;&gt;vSphere Distributed Switch&lt;/h2&gt;
&lt;p&gt;Prior to release of vSphere 7 and NSX-T 3.0 to install NSX required the creation of a N-VDS host switch on each ESXi host. While segments created on N-VDS where visible in vSphere they were not as rich as VDS. It is a constraint that physical NIC uplinks cannot be assigned to both a VDS and N-VDS.&lt;/p&gt;
&lt;p&gt;It was possible to have hosts with two pNIC which initially get built as VDS and then migrate to N-VDS and remove VDS. When running only with N-VDS the segments show in vCenter as Opaque networks. Not many 3rd party products support Opaque networks, automation became more complex as the network could not be correctly gathered via vCenter. Many production deployments moved to hosts with four pNIC two assigned to N-VDS and two assigned to VDS to hold the VMkernel ports.&lt;/p&gt;
&lt;p&gt;With these latest product versions the VDS and N-VDS capabilities converge to singluar VDS construct for use as host switch on ESXi the N-VDS remains for Edge and non-ESXi Transport Node types. The converged VDS for ESXi improves pNIC design and also makes operational management much easier as there is a singluar place for configuring NIOC, MTU and LLDP configuration.&lt;/p&gt;
&lt;p&gt;The lab hosts have two pNIC I leave onboard to vSphere Standard Switch with VMkernel ports attached. I create a VDS Version 7 configured with 1 uplink,  NIOS enabled but without default port group. The default VDS is created with MTU 1500, to support NSX I increase this to Advanced configuration option for MTU 1700.&lt;/p&gt;
&lt;p&gt;Once VDS is created I add usb0 NIC from all cluster hosts to be assigned to Uplink1.  I do not migrate VMkernel or VM networking to VDS.&lt;/p&gt;
&lt;h2 id=&#34;create-a-tunnel-endpoint-ip-address-pool&#34;&gt;Create a Tunnel Endpoint IP Address Pool&lt;/h2&gt;
&lt;p&gt;System &amp;gt; Networking &amp;gt; IP Management &amp;gt; IP Address Pools &amp;gt; Add IP Address Pool&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Name:             tep-pool
IP Range:         192.168.1.20-192.168.1.24
CIDR:             192.168.1.0/24
Gateway IP:       192.168.1.254
DNS Server:       192.168.1.10
DNS Suffix:       darrylcauldwell.com
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;configure-esxi-hosts-as-transport-nodes&#34;&gt;Configure ESXi hosts as Transport Nodes&lt;/h2&gt;
&lt;p&gt;To consistently configure ESXi hosts as NSX Transport Nodes I create a Transport Node Profile.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Name                           Host Transport Node Profile
Host Switch - Type             VDS
Host Switch - Mode             Standard
Host Switch - Name             vCenter \ DSwitch
Host Switch - Transport Zone   nsx-overlay-transportzone &amp;amp; nsx-vlan-transportzone
Host Switch - Uplink Profile   nsx-default-uplink-hostswitch-profile
Host Switch - IP Assignment    IP Pool - tep-pool
Host Switch - Teaming Policy   uplink1 (active)= Uplink 1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once created using Host Transport Nodes menu select cluster and apply profile. Once completed it is possible to view the NSX components installed on all ESXi hosts using:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;esxcli software vib list | grep nsx
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;configure-nsx-edge&#34;&gt;Configure NSX Edge&lt;/h2&gt;
&lt;p&gt;The NSX Edge provides a Layer 3 routing capability the NSX Container Plugin requires at least a medium sized Edge to deploy a small loadbalancer.&lt;/p&gt;
&lt;p&gt;Deploy a medium sized Edge node&lt;/p&gt;
&lt;p&gt;System &amp;gt; Fabric &amp;gt;  Nodes &amp;gt; Edge Transport Nodes &amp;gt; Add Edge VM&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Name:                   nsxEdge
FQDN:                   edge
Size:                   Large
Shares:                 Normal
Memory Reservation:     0
IP Assignment:          Static
Management Interface:   VM Network
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Node Switch&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Name:                       nvds1
Transport Zone:             nsx-overlay-transportzone
Uplink Profile:             nsx-edge-single-nic-uplink-profile
IP Pool:                    tep-pool
DPDK Fastpath Interfaces:   VM Network

Name:                       nvds2
Transport Zone:             nsx-vlan-transportzone
Uplink Profile:             nsx-edge-single-nic-uplink-profile
DPDK Fastpath Interfaces:   VM Network
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;configure-edge-cluster&#34;&gt;Configure Edge Cluster&lt;/h2&gt;
&lt;p&gt;When Edge has fully deployed create Edge Cluster&lt;/p&gt;
&lt;p&gt;System &amp;gt; Fabric &amp;gt; Nodes &amp;gt; Edge Clusters &amp;gt; Add&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Name:                       edgeCluster
Transport Nodes:            nsxEdge
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;uplink-network-segment&#34;&gt;Uplink Network Segment&lt;/h2&gt;
&lt;p&gt;A network segment is required to to connect Tier0 router uplink to VLAN&lt;/p&gt;
&lt;p&gt;Networking &amp;gt; Segments &amp;gt;  Add Segment&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Segment Name:               edgeUplink
Connectivity:               None
Transport Zone:             nsx-vlan-transportzone
VLAN:                       0
Multicast:                  Disabled
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;tier0-router&#34;&gt;Tier0 Router&lt;/h2&gt;
&lt;p&gt;A logical router is required I create one like:&lt;/p&gt;
&lt;p&gt;Networking &amp;gt; Tier-0 Gateways &amp;gt; Add Gateway &amp;gt; Tier0&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Name:                       tier0
HA Mode:                    Active-Standby
Failover:                   non preemptive
Edge Cluster:               edgeCluster
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Add Interface&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Name:                       tier0uplink
Type:                       External
IP Address:                 192.168.1.17/24
Connected To:               edgeUplink
Edge Node:                  nsxEdge
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once interface is added you can test it works as it will now be able to ping.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ping 192.168.1.17 -c &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;enable-a-cluster-as-workload-control-plane-wcp&#34;&gt;Enable a cluster as Workload Control Plane (WCP)&lt;/h2&gt;
&lt;p&gt;With all of the pre-requists in place we can now use wizard to enable integrated Kubernetes.&lt;/p&gt;
&lt;p&gt;[EDIT 4th May] since initial publication I found another &lt;a href=&#34;https://www.virtuallyghetto.com/2020/04/deploying-a-minimal-vsphere-with-kubernetes-environment.html&#34;&gt;blog post&lt;/a&gt; around deploying minimal lab. By default when enabling Workload Control Plane this deploys a 3x VMs which form the Kubernetes supervisor cluster. This can be reduced to 2x by updating &lt;code&gt;/etc/vmware/wcp/wcpsvc.yaml&lt;/code&gt; on the VCSA and changing the minmasters and maxmasters properties from 3 to 2. Then restarting the wcp service &lt;code&gt;service-control --restart wcp&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Menu &amp;gt; Workload Management &amp;gt; Enable&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Select a Cluster&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If cluster does not show available check reason via GUI and or these vCenter logs for clues.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;/var/log/vmware/wcp/wcpsvc.log
/var/log/vmware/wcp/nsxd.log
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Cluster Settings&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Cluster Size:                  Tiny
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Network&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The management network refers to the control plane needs to communicate with vCenter and NSX.  The workload network will come from the RFC1918 addressable space. Enter network details&lt;/p&gt;
&lt;p&gt;Workload Control Plane Networking&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Network:                        VM Network
Starting IP Address:            192.168.1.30
Subnet Mask:                    255.255.255.0
Gateway:                        192.168.1.254
DNS Server:                     192.168.1.10
NTP Server:                     192.168.1.10
DNS Search Domains:             darrylcauldwell.com
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Workload Network&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;vDS:                            DSwitch
Edge Cluster:                   edgeCluster
DNS Server:                     192.168.1.10
Pod CIDRs:                      10.244.0.0/21 (default)
Service CIDRS:                  10.96.0.0/24 (default)
Ingress CIDR:                   192.168.1.48/28
Egress CIDR:                    192.168.1.64/28
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Configure storage to use&lt;/li&gt;
&lt;/ol&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;Control Plane Node              vSAN Default Storage Policy
Ephemeral Disks                 vSAN Default Storage Policy
Image Cache                     vSAN Default Storage Policy
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;Review and confirm&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Once submitted we see in vCenter a resource pool named Namespaces and three virtual appliances named WcpAPIServerAgent each is allocated 2CPU and 8GB RAM.  An installation occurs on the three virtual appliances which,  these attempt to run before appliances deploy it is normal to see install failures during this phase.&lt;/p&gt;
&lt;p&gt;The progress messages available via UI aren&amp;rsquo;t superb its useful to SSH to VCSA and tail the log:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;tail -f /var/log/vmware/wcp/wcpsvc.logtail -f /var/log/vmware/wcp/wcpsvc.log
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If all has goes to plan an hour and a half or so later and if all gone to plan.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/tanzu-wcp.png&#34; alt=&#34;WCP Success&#34;&gt;&lt;/p&gt;
&lt;p&gt;From here I can begin to create and consume native Kubernetes namespaces and resources.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Cloudbased-init For vSphere</title>
      <link>https://darrylcauldwell.github.io/post/cloudbased-init/</link>
      <pubDate>Thu, 26 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/cloudbased-init/</guid>
      <description>
        
          &lt;p&gt;vRealize Automation Cloud and 8.x allows you to deploy and configure cloud agnostic virtual machines. To achieve agnostic in-guest OS customization capability across for on-premises vSphere, AWS, GCP and Azure endpoints &lt;a href=&#34;https://cloud-init.io/&#34;&gt;cloud-init&lt;/a&gt; is used for Linux guests and &lt;a href=&#34;https://cloudbase.it/cloudbase-init/&#34;&gt;cloudbase-init&lt;/a&gt; is used for Windows guests.&lt;/p&gt;
&lt;h2 id=&#34;inital-attempt-all-defaults&#34;&gt;Inital Attempt All Defaults&lt;/h2&gt;
&lt;p&gt;In vSphere I created a Windows 2019 VM with defaults except for mapping ISO as CD/ROM. I then ran through installer to install Standard edition with Desktop experience, installed VMware Tools, disabled firewall and enabled Remote Desktop. I installed Cloudbased- init 0.9.12.dev76 and optional Carbon PowerShell module to default location. For configuration options I left the default Admin user be created into Administrators group, in addition I selected option to ‘Run Cloudbased-Init service as LocalSystem’. I chose options to &amp;lsquo;Run Sysprep to create a generalized image&amp;rsquo; and &amp;lsquo;Shutdown when Sysprep terminates&amp;rsquo;. When complete I change virtual machine to be a virtual machine template.&lt;/p&gt;
&lt;p&gt;I connect to vRealize Automation Cloud &amp;gt; Infrastructure &amp;gt; Cloud Accounts &amp;gt; {my account} and run &amp;lsquo;Sync Images&amp;rsquo;.  Once image sync is completed vRealize Automation Cloud &amp;gt; Infrastructure &amp;gt; Image Mappings and create &amp;lsquo;New Image Mapping&amp;rsquo;. Create a Blueprint with single Cloud Agnostic Machine linked to the image mapping.&lt;/p&gt;
&lt;p&gt;When I provision a vRA Cloud blueprint to deploy a VM&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;formatVersion: 1
inputs: {}
resources:
Cloud_Machine_1:
    type: Cloud.Machine
    properties:
    image: dc-win2019
    flavor: small
    cloudConfig: |
        #cloud-config
        hostname: i-got-a-new-name
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The VM powers on and cloud-data gets passed by vRealize Automation Cloud as an OVF which is mounted as CDROM with CDROM contents ovf-env.xml in its root.&lt;/p&gt;
&lt;p&gt;Cloud-init runs on startup but appears to fail to do what it is asked, Cloudbase-init.log contains error.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;2019-09-26 16:00:15.041 4360 ERROR cloudbaseinit.metadata.services.base [-] HTTPConnectionPool(host=&amp;#39;169.254.169.254&amp;#39;, port=80): Max retries exceeded with url: /openstack/latest/meta_data.json
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;adding-ovf-metadata-service&#34;&gt;Adding OVF Metadata Service&lt;/h2&gt;
&lt;p&gt;Looking at the error suggested to me that Cloudbase-Init was not finding the OVF and file. I read through some of the Cloudbase-Init documentation and found that metadata service configuration was specified in,&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;C:\Program Files\Cloudbase Solutions\Cloudbase-Init\conf\cloudbase-init-unattend.conf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Checking the default configuration file does not include the &lt;a href=&#34;https://cloudbase-init.readthedocs.io/en/latest/services.html&#34;&gt;OvfService metadata service&lt;/a&gt;:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;metadata_services=cloudbaseinit.metadata.services.configdrive.ConfigDriveService,cloudbaseinit.metadata.services.httpservice.HttpService,cloudbaseinit.metadata.services.ec2service.EC2Service,cloudbaseinit.metadata.services.maasservice.MaaSHttpService
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If I follow similar steps to before to creeate templat but during install of Cloudbase-init but DO NOT chose wizard options &amp;lsquo;Sysprep and Shutdown&amp;rsquo;.  Instead I update the metadata_services entry in the cloudbase-init-unattend.conf file include OvfService like:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;metadata_services=cloudbaseinit.metadata.services.ovfservice.OvfService
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Other Cloudbase-Init &lt;a href=&#34;https://cloudbase-init.readthedocs.io/en/latest/plugins.html#plugins&#34;&gt;plugins&lt;/a&gt; which require reading metadata, such that which sets the hostname, pickup their configuration from this config file:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;C:\Program Files\Cloudbase Solutions\Cloudbase-Init\conf\cloudbase-init.conf
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The default installation of cloudbase-init.conf does not have a metadata_services entry at all. To allow the plugins be capable of reading metadata from OVF source add the following line to the file:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;metadata_services=cloudbaseinit.metadata.services.ovfservice.OvfService
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When both files have been updated execute sysprep passing the same parameters as the Cloudbase-Init installer wizard would:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;cd “C:\Program Files\Cloudbase Solutions\Cloudbase-Init\conf\”
C:\Windows\System32\Sysprep\Sysprep.exe /quiet /generalize /oobe /shutdown /unattend:unattend.xml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Performing a deployment of thee vRealize Automation Cloud blueprint example from earlier now deploys a VM with its hostname updated.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Understanding VMware Guest OS Customization</title>
      <link>https://darrylcauldwell.github.io/post/guest-customization/</link>
      <pubDate>Wed, 11 Sep 2019 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/guest-customization/</guid>
      <description>
        
          &lt;p&gt;I recently began looking more closely at cloud-init for customizing VMware guest VMs. This caused me some heachaches! I took various notes while troubleshooting and researching around this.&lt;/p&gt;
&lt;h2 id=&#34;why-guest-os-customization-is-needed&#34;&gt;Why Guest OS Customization Is Needed&lt;/h2&gt;
&lt;p&gt;When you clone a virtual machine or deploy a virtual machine from a template it clones the virtual hard disk and all guest operating system settings. In many deployment scenarios if virtual machines with identical settings are deployed conflicts can occur, such as duplicate computer names. Typically for this deployment scenario during deployment guest operating system customization is required to be performed to give them uniqueness.&lt;/p&gt;
&lt;h2 id=&#34;vm-guest-os-customization-approach&#34;&gt;VM Guest OS Customization Approach&lt;/h2&gt;
&lt;p&gt;VMware has long provided the facility to perform guest operating system customization through a combination of vCenter Server and VMware Tools. This gives the option to choose to launch the Guest Customization wizard during the cloning or deployment process,  or can create standard customization specifications and apply these during the cloning or deployment process.&lt;/p&gt;
&lt;p&gt;When the virtual machine template is Linux the VMware guest operating system customization is performed using a combination of cloud-init and a collection of perl scripts packaged with open-vm-tools.&lt;/p&gt;
&lt;h2 id=&#34;logfiles-and-log-configuration&#34;&gt;Logfiles and Log Configuration&lt;/h2&gt;
&lt;p&gt;Guest OS can be complex and things can go wrong during guest OS customization. When they do its useful to understand where to look for clues as to cause.&lt;/p&gt;
&lt;p&gt;If there were any problems with the extraction and deployment of the customization package onto the guest operating system these are written to:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;/var/log/vmware-imc/toolsDeployPkg.log
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;VMware guest operating system customization steps performed using the perl scripts output to default location of /var/log/messages. There are many other things in this log file so can use grep to view only the customization steps:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;grep &amp;#34;customize-guest&amp;#34; /var/log/messages 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;There are many options for configuring cloud-init logging. These can be configured these within your template virtual machine. The configuration settings are stored in:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;/etc/cloud/cloud.cfg.d/05_logging.cfg
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Within this configuration file we can identify that by default cloud-init directs the main handler output to a file:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;/var/log/cloud-init.log
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can also see that by default the stdout and stderr of the specific we tell it to perform is written to:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;/var/log/cloud-init-output.log
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;customization-guest-customatization-configuration&#34;&gt;Customization Guest Customatization Configuration&lt;/h2&gt;
&lt;p&gt;There are many options which cloud-init can be configured with. These can be configured these within your template virtual machine. The configuration settings are stored in:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;/etc/cloud/cloud.cfg
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;One of the options which can be configured is whether VMware tools perl scripts are ran. How to enable or disable this is documented in this article:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;https://kb.vmware.com/s/article/59557
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;It is useful to note here that there is currently a known issue when cloud-init calls the open-vm-tools perl scripts for &amp;lsquo;Ubuntu 18.04&amp;rsquo;. The boot sequence and bring up order can cause the perl scripts to fail. A high level cause and workaround are described here:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;https://kb.vmware.com/s/article/56409 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;A more detailed description of the bug including some very interesting detail about how cloud-init and open-vm-tools perl scripts are called is described here:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;https://bugs.launchpad.net/ubuntu/+source/open-vm-tools/+bug/1793715
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;cloud-init-user-data&#34;&gt;Cloud-Init User-Data&lt;/h2&gt;
&lt;p&gt;One of the more useful enhancements of using cloud-init is the ability to instruct the guest operating system to run things such as custom scripts during boot.&lt;/p&gt;
&lt;p&gt;User-Data content supports the following types of content &amp;lsquo;gzip compressed&amp;rsquo; or &amp;lsquo;mime multi-part archive&amp;rsquo;. With a mime multi-part file, the user can specify more than one type of data within the content.  For full details on the User-Data content format see here:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;https://cloudinit.readthedocs.io/en/latest/topics/format.html
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The two most common data types I use at the moment are Cloud Config Data or simple shell scripts. To differentiate sections when using a &amp;lsquo;mime multi-part archive&amp;rsquo; begin each section with data type for example  &lt;code&gt;#!&lt;/code&gt; for shell script or &lt;code&gt;#cloud-config&lt;/code&gt; for cloud config data.&lt;/p&gt;
&lt;p&gt;Cloud-init can be fussy about syntax of the file contents once created you can validate the syntaxt using a command like this:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;cloud-init devel schema --config-file /tmp/user-data
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;vcenter-deploy-ova-import&#34;&gt;vCenter Deploy OVA Import&lt;/h2&gt;
&lt;p&gt;Some operating systems packages which include cloud-init expose options to confiure during deployment.  For example Ubuntu which can be found here:&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;https://cloud-images.ubuntu.com/releases/18.04/release/ubuntu-18.04-server-cloudimg-amd64.ova
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The Ubunutu image includes property &amp;lsquo;Encoded user-data&amp;rsquo; this has property description &lt;em&gt;&amp;ldquo;In order to fit into a xml attribute, this value is base64 encoded . It will be decoded, and then processed normally as user-data&amp;rdquo;&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;I like to keep a file copy of the un-encoded and encoded user-data. To do this I use text editior to create a file with the required cloud-init user-data contents.  I then use &lt;a href=&#34;https://linux.die.net/man/1/base64&#34;&gt;base64&lt;/a&gt; to output contents into a file appended with .b64.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;base64 --input /tmp/user-data --output /tmp/user-data.b64
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If encoded user-data is passed during deployment of OVA file in this way this will run when the VM boots.&lt;/p&gt;
&lt;h2 id=&#34;cloud-assembly&#34;&gt;Cloud Assembly&lt;/h2&gt;
&lt;p&gt;Other VMware products such as Cloud Assembly can utilize open-vm-tools and cloud-init to perform guest customization.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;https://docs.vmware.com/en/VMware-Cloud-Assembly/services/Using-and-Managing/GUID-70EA052D-FABF-4CE5-875D-9B52FED08AA3.html
&lt;/code&gt;&lt;/pre&gt;&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;https://blogs.vmware.com/management/2018/11/customizing-cloud-assembly-deployments-with-cloud-init.html
&lt;/code&gt;&lt;/pre&gt;
        
      </description>
    </item>
    
    <item>
      <title>VMware Data Encryption At Rest</title>
      <link>https://darrylcauldwell.github.io/post/encryption-at-rest/</link>
      <pubDate>Fri, 13 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/encryption-at-rest/</guid>
      <description>
        
          &lt;p&gt;Encryption of data at rest is a requirement for many customers,  with VMware hosted Virtual Machines (VMs) there are two ways to achieve this. VM data can be encrypted using vSAN whole-datastore encryption or using Storage Policy (VMcrypt). Both VM Encryption and vSAN Encryption require a Key Management Interoperability Protocol (KMIP) 1.1 compliant Key Management Server (KMS), the same KMS provider can be used for both types of encryption.&lt;/p&gt;
&lt;p&gt;There are important differences between these two encryption methods.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Feature/Function&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;VSAN Encryption&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;VMcrypt Encryption&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;External key-management server (KMS)&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;√&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;√&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Per VM Encryption&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;x&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;√&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Datastore Encryption&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;√&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;x&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Data At Rest Encryption&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;√&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;√&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;End To End Encryption&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;x&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;√&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;VMs Encrypted By&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Placement on datastore&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Storage Policy&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Encryption Occurs&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;After Deduplication&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Before Deduplication&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;vm-encryption-vmcrypt&#34;&gt;VM Encryption (VMcrypt)&lt;/h2&gt;
&lt;p&gt;Encryption is done in the hypervisor, “beneath” the virtual machine. As I/O comes out of the virtual disk controller in the VM it is immediately encrypted by a module in the kernel before being send to the kernel storage layer. Because encryption happens at the hypervisor level and not in the VM, the Guest OS and datastore type are not a factor. The encyrption is hardware accelerated when host CPU support &lt;a href=&#34;https://software.intel.com/en-us/articles/intel-advanced-encryption-standard-instructions-aes-ni&#34;&gt;Intel Advanced Encryption Standard Instructuctions (AES-NI)&lt;/a&gt;. The encryption option is exposed via Storage Policy, therefore it can be applied to only those VMs or groups of VMs which require it.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/encryption.jpg&#34; alt=&#34;VMware Encryption&#34;&gt;&lt;/p&gt;
&lt;p&gt;VM Encryption is implemented through &lt;a href=&#34;https://code.vmware.com/programs/vsphere-apis-for-io-filtering&#34;&gt;vSphere APIs for IO Filters (VAIO)&lt;/a&gt;. The VAIO framework allows a filter driver to intercept IO that a VM sends down to a storage device. Ecryption occurs before any data is send across the wire.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/encryption-vaio-architecture.png&#34; alt=&#34;vSphere APIs for IO Architecture&#34;&gt;&lt;/p&gt;
&lt;p&gt;## VMcrypt VSAN Efficiency Consideration
An important consideration when using VSAN is that, VMcrypt writes an encrypted data stream whereas vSAN encryption receives an unencrypted data stream and encrypts it during the write process. As the encrypted data written by VMcrypt appears to be random, it does not deduplicate well. If using VMcrypt with VSAN deduplication, expect deduplication efficiency to approach zero for encrypted VMs.&lt;/p&gt;
&lt;h2 id=&#34;vsan-encyption&#34;&gt;VSAN Encyption&lt;/h2&gt;
&lt;p&gt;With vSAN Encryption we can achieve “data encryption at rest” however the data travels to the destination unencrypted then when it reaches its destination it is encrypted, and it will be encrypted after it is deduplicated and/or compressed again. This allows the benefit of deduplicated and/or compressed space saving functionality.&lt;/p&gt;
&lt;p&gt;Thie data encyption at rest method is a cluster wide option, which means that every VM will be encrypted.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/encryption-vsan-hytrust.png&#34; alt=&#34;VMware Encryption&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;key-management-server-kms-topologies&#34;&gt;Key Management Server (KMS) Topologies&lt;/h2&gt;
&lt;p&gt;Key Management Server (KMS) availability is a critical component when deploying encryption as without it all data access is not allowed. If you lose the keys, you’ve lost the data (unless you’ve backed it up).&lt;/p&gt;
&lt;p&gt;Key Managers today are usually set up in a way that they replicate keys to one another. If I have three instances of a key manager, KMS-A, KMS-B and KMS-C, they replicate the keys between them. If I create a key on KMS-A it will show up in KMS-B &amp;amp; KMS-C at some point. Using this example, in vCenter I would create a key manager cluster/alias. In this example I’ll call it “KMSCluster” and add KMS-A, KMS-B and KMS-C into that KMS Cluster. I would then establish a trust with each of the key managers.&lt;/p&gt;
&lt;p&gt;KMS can either be purchased as hardware appliances or virtual appliances. When using KMS virtual appliances avoid circular dependancy where the datastore hosting the appliance is encrypted using itself. The same is true for vCenter and PSC’s in a VM Encryption scenario. You shouldn’t encrypt them using VM Encryption because they would then need to boot up to get their encryption key to boot up.&lt;/p&gt;
&lt;p&gt;If you have a single site then you probably want to have, at minimum, two replicating key managers. For multi-site,  you want all the KMS servers running and replicating across the sites.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/encryption-multisite.png&#34; alt=&#34;VMware Encryption&#34;&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Installing vSphere Integrated Containers In Five Minutes</title>
      <link>https://darrylcauldwell.github.io/post/vic/</link>
      <pubDate>Wed, 13 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/vic/</guid>
      <description>
        
          &lt;p&gt;As it looks like vSphere Integrated Containers will feature a lot at the upcoming VMworld I took the chance to install this and find out more.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/vmware/vic/tree/master/doc/user_doc/vic_installation&#34;&gt;documentation available&lt;/a&gt; on isn&amp;rsquo;t extensive, but, contains all I needed to know and I think benefits from being to the point.&lt;/p&gt;
&lt;p&gt;The files needed to set this up are provided as a gzip file
&lt;a href=&#34;https://bintray.com/vmware/vic-repo/build#files&#34;&gt;hosted on bintray&lt;/a&gt;. Just checking the file dates you can see the development is iterating at a great pace with around six releases per day. I chose to download the latest package which contains installers to be run from different operating systems.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Platform&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Supported Versions&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Windows&lt;/td&gt;
&lt;td&gt;7, 10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mac OS X&lt;/td&gt;
&lt;td&gt;10.11 (TBC)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Linux&lt;/td&gt;
&lt;td&gt;Ubuntu 15.04, others TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;My homelab is vCenter, single cluster of two ESX hosts, storage VSAN and networking NSX. Security is not important in my homelab and TLS adds complication so I chose not to add this. There are great syntaxt examples to follow in the documentation, but here are options I used.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;./vic-machine-darwin create --target 192.168.1.13 --user Administrator@vsphere.local --password VMware1! --compute-resource ElectricChair --image-datastore vsanDatastore --bridge-network DPortGroup --name vch
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/vic-install.jpg&#34; alt=&#34;vSphere Integrated Containers Install&#34;&gt;&lt;/p&gt;
&lt;p&gt;In order to use vSphere Integrated Containers you&amp;rsquo;ll need a Docker client, for Mac there is the choice of &lt;a href=&#34;https://www.docker.com/products/docker-toolbox&#34;&gt;Docker Toolbox&lt;/a&gt; or the newer &lt;a href=&#34;https://docs.docker.com/docker-for-mac/&#34;&gt;Beta Docker for Mac&lt;/a&gt;. I had the beta installed, and found I needed to remove this and replace with Docker Toolbox.&lt;/p&gt;
&lt;p&gt;The first thing to do is ensure that you can connect and that it looks healthy, to do this run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker -H ipaddress-vch:2375 info
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/vic-info.jpg&#34; alt=&#34;vSphere Integrated Containers Info&#34;&gt;&lt;/p&gt;
&lt;p&gt;Its always useful to look in the log files when your setting something up for the first time, however SSH is disabled on the VCH host so I enabled SSH for current session.  Logon via console using &lt;em&gt;root&lt;/em&gt; with password of &lt;em&gt;password&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;systemctl start sshd
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;There are five log files I found so far&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;/var/log/vic/docker-personality.log
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;/var/log/vic/imagec.log
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;/var/log/vic/init.log
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;/var/log/vic/port-layer.log
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;/var/log/vic/vicadmin.log
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So now we&amp;rsquo;re running and can access logs the next thing to do is pull a container&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker -H paddress-vch:2375 pull vmwarecna/nginx
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/vic-nginx.jpg&#34; alt=&#34;vSphere Integrated Containers NGINX&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once its local we can start it up,  we&amp;rsquo;ll start it in the background and put a port mapping of port 80 in place and then view it.&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;docker -H ipaddress-vch:2375 run -d -p 80:80 vmwarecna/nginx
docker -H ipaddress-vch:2375 ps
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/vic-nginx-go.jpg&#34; alt=&#34;vSphere Integrated Containers NGINX Go&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can then also see that a new VM has been created in vCenter with VM name matching the UID of the docker container.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/vic-nginx-vm.jpg&#34; alt=&#34;vSphere Integrated Containers NGINX VM&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can then stop this and tidy up our test container&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker -H ipaddress-vch:2375 stop container-id
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;docker -H ipaddress-vch:2375 rm container-id
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As its so easy to install and configure its probably worth while removing VCH and when you come to use it pull the latest to remove is as easy as installing.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;./vic-machine-darwin delete --target vcenter.darrylcauldwell.local --user Administrator@vsphere.local --password VMware1! --name vch --force
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/vic-bye.jpg&#34; alt=&#34;vSphere Integrated Containers Bye!&#34;&gt;&lt;/p&gt;
&lt;p&gt;There is a little bug just now and the image files don&amp;rsquo;t seem to get removed by uninstall process but these are easy enough to delete along with the VIC folder from the datatore.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m disappointed in myself for waiting so long to look at this, what was putting me off was the thought it might be complex but it proved to be very much straight forwards.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>vRealize Orchestrator 7 (vRO) Install</title>
      <link>https://darrylcauldwell.github.io/post/vro-install/</link>
      <pubDate>Tue, 14 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/vro-install/</guid>
      <description>
        
          &lt;p&gt;While attempting to setup vRealize Orchestrator 7 at home it soon became clear the documentation isn&amp;rsquo;t great after much flipping between documents and blog posts I got this installed and working. Here I are the steps I followed.&lt;/p&gt;
&lt;h2 id=&#34;deploy-vrealize-orchestrator&#34;&gt;Deploy vRealize Orchestrator&lt;/h2&gt;
&lt;p&gt;Deploy the OVA via web client and specify network details appropriate to your home lab,  ensure a DNS A and PTR record are created for the vRealize Orchestrator appliance.  The Orchestrator client relies on a Java Runtime so while OVA deploys its a good time to install this if its not already.&lt;/p&gt;
&lt;p&gt;Once deployed the various interfaces to vRO can be accessed from the main menu screen https://vro-fqdn:8281/vco/.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/vROstartPage.jpg&#34; alt=&#34;vRO Start Page&#34;&gt;&lt;/p&gt;
&lt;p&gt;From the menu select &amp;lsquo;Orchestrator Control Center&amp;rsquo; from the start menu and authenticate with the account named root with the password you specified during OVA deployment.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/vROcontrolCenter.jpg&#34; alt=&#34;vRO Control Center&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;configure-vcenter-connection&#34;&gt;Configure vCenter Connection&lt;/h2&gt;
&lt;p&gt;The first thing to do once vRealize Orchestrator is deployed is to import the vCenter SSL certitificate. Select the Control Center &amp;lsquo;Manage \ Certificates&amp;rsquo; menu item.  Within trusted certificates tab click &amp;lsquo;Import \ Import from URL&amp;rsquo;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/vROtrustVcenter.jpg&#34; alt=&#34;vRO Trust vCenter&#34;&gt;&lt;/p&gt;
&lt;p&gt;Within this wizard enter the FQDN of your vCenter and at next wizard screen
click Import.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/vROimportCert.jpg&#34; alt=&#34;vRO Import Certificate&#34;&gt;&lt;/p&gt;
&lt;p&gt;I use vCenter as an appliance with embedded Platform Services Controller, I&amp;rsquo;d prefer to manage vRealize Orchestrator using the same Single Sign On account.  To do this use the vRealize Orchestrator Control Center menu option &amp;lsquo;Manage \ Configure Authentication Provider&amp;rsquo;.&lt;/p&gt;
&lt;p&gt;Select &amp;lsquo;Authentication mode&amp;rsquo; to be vSphere,  enter FQDN of vCenter and click Connect.  Your then asked for a User name and password, this account is used to bind to Single Sign On so enter your logon details and click Register. Your then asked to enter an Admin group, start to type Administrator and click the Search button to the right and it should list all available groups from SSO with Administrator in the name.  Select &amp;lsquo;vsphere.local\Administrators&amp;rsquo; and click Save.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/vROauth.jpg&#34; alt=&#34;vRO Authentication Mode&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once added the vRO service needs to be restarted there is a link presented once configuration is saved,  or this can be selected from &amp;lsquo;Mange \ Startup Options&amp;rsquo;.&lt;/p&gt;
&lt;p&gt;You should now be able to open the main menu and start the vRO Client, login with your SSO credentials and start having fun.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/vROclientStart.jpg&#34; alt=&#34;vRO Client Start&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;redirect-syslog-to-log-insight&#34;&gt;Redirect Syslog to Log Insight&lt;/h2&gt;
&lt;p&gt;I use vRealize Log Insight to to centralize all my lab log files. To set Log Insight as a remote syslog use &amp;lsquo;Control Center \ Log \ Logging Integration&amp;rsquo;.  Complete the details of Log Insight server.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/vROlogInsight.jpg&#34; alt=&#34;vRO Log Insight&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;configure-ntp&#34;&gt;Configure NTP&lt;/h2&gt;
&lt;p&gt;I sync time from Active Directory in my homelab,  this is done from the appliance configuration application located https://&lt;!-- raw HTML omitted --&gt;:5480. Within this go to the &amp;lsquo;Admin \ Time Settings&amp;rsquo; menu option and enter the NTP server IP address.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/vROconfigurator.jpg&#34; alt=&#34;vRO Configurator&#34;&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Create VMware Virtual Machine Templates</title>
      <link>https://darrylcauldwell.github.io/post/vsphere-vm-templates/</link>
      <pubDate>Tue, 17 May 2016 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/vsphere-vm-templates/</guid>
      <description>
        
          &lt;p&gt;When creating virtual machine templates I&amp;rsquo;d like to do this consistently as such I&amp;rsquo;ll try and keep this post up to date with the settings I am including.&lt;/p&gt;
&lt;h2 id=&#34;centos-7&#34;&gt;CentOS 7&lt;/h2&gt;
&lt;p&gt;New custom VM named CentOS7, hardware version 11, guest operating system Linux version &amp;lsquo;CentOS 4/5/6/7 (64-bit)&amp;rsquo;. We&amp;rsquo;d like a small template which we expand if required, so single virtual socket with single core, 2GB virtual memory, one VMXNET3 NIC, default LSI Logic Parallel SCSI controller with a thin 16GB hard disk and mount CentOS7 ISO as CD-ROM.&lt;/p&gt;
&lt;p&gt;Power on VM and open console, ensure you change NIC to connected and enter password otherwise leave all as default and install. Now I install common tools to use within lab, so once installed reboot and logon still using remote console.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;yum install open-vm-tools net-tools epel-release gcc git
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;yum update
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cp -f /etc/sysconfig/network-scripts/ifcfg-eth0 /tmp/eth0
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sed &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;/^HWADDR/d&amp;#34;&lt;/span&gt; /tmp/eth0 &amp;amp;gt; /etc/sysconfig/network-scripts/ifcfg-eth0
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Shutdown the virtual machine and convert to template using vCenter.&lt;/p&gt;
&lt;h2 id=&#34;windows-2012-r2&#34;&gt;Windows 2012 R2&lt;/h2&gt;
&lt;p&gt;New custom VM named Win2012R2, hardware version 11, guest operating system Windows version &amp;lsquo;Microsoft Windows Server 2012 (64-bit)&amp;rsquo;. We&amp;rsquo;d like a small template which we expand if required, so single virtual socket with single core, 4GB virtual memory, one VMXNET3 NIC, default LSI Logic SAS SCSI controller with a thin 40GB hard disk and mount Windows 2012 R2 ISO as CD-ROM.&lt;/p&gt;
&lt;p&gt;Power on VM and open console, select Datacenter Edition with GUI and enter password otherwise leave all as default and install. Once installed rebooted logon and perform Typical VMware tools install.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Apply MSDN License Key and Active Windows&lt;/li&gt;
&lt;li&gt;Disable IE Enhanced Security&lt;/li&gt;
&lt;li&gt;Use Windows Update to apply all current patches, repeat until all dependant patches are installed.&lt;/li&gt;
&lt;li&gt;Copy \Sources\SxS folder from CD-ROM to C:\&lt;/li&gt;
&lt;li&gt;As some Updates will update .net we should force the Assemblies to get updated&lt;/li&gt;
&lt;/ul&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-powershell&#34; data-lang=&#34;powershell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;%&lt;/span&gt;windir%\Microsoft.NET\Framework\v4.0.&lt;span style=&#34;color:#ae81ff&#34;&gt;30319&lt;/span&gt;\ngen.exe update /force
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;%&lt;/span&gt;windir%\Microsoft.NET\Framework64\v4.0.&lt;span style=&#34;color:#ae81ff&#34;&gt;30319&lt;/span&gt;\ngen.exe update /force&amp;lt;/code&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Shutdown the virtual machine and convert to template using vCenter.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Project Bonneville - vSphere Integrated Containers</title>
      <link>https://darrylcauldwell.github.io/post/vic-bonneville/</link>
      <pubDate>Tue, 06 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/vic-bonneville/</guid>
      <description>
        
          &lt;p&gt;Working for a company which develops software for business I&amp;rsquo;ve been keen on following the story of containers unfold. The explosion of containers and microservices in the software development space has been a revolution in the way applications can be architected and deployed. This has put a great deal of power in the hands of the developer but with great power comes great responsibility for operational management.&lt;/p&gt;
&lt;p&gt;Working in the operational side the key words to my working life are Availability, Reliability, Maintainability, Supportability and Security. The operational model to effectively run containers in production requires a paradigm shift where the approach and underlying assumptions are revisited. Working in a company promoting DevOps culture and values we in the operational side are beginning to gain greater empathy to the challenges a developer faces and how they believe containers will help. Gaining a technical understanding of Docker has therefore become a priority so I can ensure the infrastructure I design and engineer can help the developer deliver not only velocity but help them to deliver Availability, Reliability, Maintainability, Supportability and Security.&lt;/p&gt;
&lt;p&gt;During &lt;a href=&#34;https://darrylcauldwell.github.io/post/fargo&#34;&gt;VMworld 2014 project Fargo (VMfork)&lt;/a&gt; was described which has become the Instant Clone feature of vSphere 6. At the time I didn&amp;rsquo;t take too much notice as this was signed to virtual desktop space.  During April this year VMware released project Photon which is essentially a minimal Linux container host,  at the time this was confusing to me as RedHat Atomic, Ubuntu Snappy and CoreOS seemed to have all bases covered in this space. That is until two months later when Ben Corrie of VMware announced project Bonneville to the world.&lt;/p&gt;
&lt;p&gt;The fundamental to this solution is a 1:1 model between container host and containers rather than one host with many containers. Project Bonneville is a Docker daemon that has custom drivers for networks and execution that is compatible with the Docker Client APIs, and that means a vSphere VM is treated exactly like a Docker container. With this it redefined a Docker container as a x86 hardware virtualised VM, (not tied to host running the Linux kernel), a Bonneville VM can theoretically run containers of any kernel version of any operating system and can do it fast and efficiently.&lt;/p&gt;
&lt;p&gt;‘Instant Clone’ also known as VMfork or Project Fargo, gives the ability to clone and deploy virtual machines, as much as 10x faster than what is currently possible before. It does this by using rapid in-memory cloning of running virtual machines and copy-on-write to quickly deploy clones of a parent virtual machine. Using this clone method of the container host ensures the only changed blocks and memory pages are those used by container itself this.&lt;/p&gt;
&lt;p&gt;A fantastic architecture flow of how Bonneville might work is described in this diagram as posted by George Hicken @hickeng.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/vic-bonneville.png&#34; alt=&#34;vSphere Integrated Containers&#34;&gt;&lt;/p&gt;
&lt;p&gt;The benefits this technology appears to offer appear fantastic and I am really looking forwards to attending Ben Corrie&amp;rsquo;s session &amp;lsquo;INF5229 — Docker and Fargo: Exploding the Linux Container Host&amp;rsquo; to solidify my understanding and learn more about how we can begin exploiting this to help our developers deliver Availability, Reliability, Maintainability, Supportability and Security alongside the awesome features they write.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>VMware Projects Fargo (VMFork) &amp; Meteor</title>
      <link>https://darrylcauldwell.github.io/post/fargo/</link>
      <pubDate>Tue, 06 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/fargo/</guid>
      <description>
        
          &lt;p&gt;There was announced during VMworld a technology preview for Project Fargo (formerly VMFork) it is likely this will be launched with vSphere6. The aim of Fargo is to provides a fast, scalable differential clone of a running VM.  I see this as very similar to Redirect-on-Write (RoW) methodology used by NetApp snapshots where at the point of snap the existing blocks are frozen and any writes (creations/changes/deletions) are redirected to new blocks. However with Fargo rather than than a snapshot we are creating a Copy-on-Write(CoW) the difference being that with CoW the original data that is being written to is copied into a new file that is set aside for the snapshot before original data is overwritten. So before a write is allowed to a block, copy-on-write moves the original data block to the snapshot storage.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/fargo-cow.gif&#34; alt=&#34;Copy On Write&#34;&gt;&lt;/p&gt;
&lt;p&gt;The key benefits of the use of this method is that its near instantaneous and can be done from a running VM,  so a new VM spawned would typically take less than 1 second and be in the same running state. As well as this as only changed blocks are written the solution will take up dramatically less disk space.&lt;/p&gt;
&lt;p&gt;While there are many potential use cased for Fargo it was presented with virtual desktop in mind where providing an instant clone of running non-persistent desktop would avoid the boot storms to the storage subsystem.&lt;/p&gt;
&lt;p&gt;So now you have quick provisioning of operating system you then need to deliver the right user applications to it quickly and this is I assume to be done by &lt;a href=&#34;http://blogs.vmware.com/euc/2014/08/cloudvolumes.html&#34;&gt;CloudVolumes&lt;/a&gt; where applications are abstracted and layered onto the users operating system.  The combination of these two tools appears to be known as Project Meteor.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Configuring Network Core Dump With PowerCLI</title>
      <link>https://darrylcauldwell.github.io/post/core-dump/</link>
      <pubDate>Wed, 17 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/core-dump/</guid>
      <description>
        
          &lt;p&gt;The VMware vSphere Network Dump Collector service enables a host to transmit diagnostic information via the network to a remote netdump service, which stores it on disk. Network-based coredump collection can be configured in addition to or instead of disk-based coredump collection. This may be useful in stateless environments with no local disk usable for a diagnostic partition.&lt;/p&gt;
&lt;p&gt;vSphere ESXi Dump Collector service pre-packaged with the vSphere vCenter Server Virtual Appliance, and if vCenter on Windows vSphere ESXi Dump Collector typically is installed on same server.&lt;/p&gt;
&lt;p&gt;It is a little bit of a pain to configure this on every server, so I wrote this little script to get all hosts and then configure each ESXi host registered with vCenter to point its core dumps to the vCenter.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-powershell&#34; data-lang=&#34;powershell&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Add-PSSnapin VMware.VimAutomation.Core
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$vcenter = &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Connect-VIServer $vcenter
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;foreach&lt;/span&gt;($vmhost &lt;span style=&#34;color:#66d9ef&#34;&gt;in&lt;/span&gt; Get-VMHost){
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$esxcli = Get-EsxCli -VMHost $vmhost.Name
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$esxcli.system.coredump.network.set($null,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;vmk0&amp;#34;&lt;/span&gt;,$vcenter,&lt;span style=&#34;color:#ae81ff&#34;&gt;6500&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$esxcli.system.coredump.network.set(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;$esxcli.system.coredump.network.get()
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
        
      </description>
    </item>
    
    <item>
      <title>OpenStack Windows Image</title>
      <link>https://darrylcauldwell.github.io/post/openstack-glance-win/</link>
      <pubDate>Thu, 23 Apr 2015 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/openstack-glance-win/</guid>
      <description>
        
          &lt;p&gt;While looking at OpenStack as the control plane for vSphere it appears there isn&amp;rsquo;t too much detail and I found it tricky to create my first OpenStack image.  Here are the steps I followed.&lt;/p&gt;
&lt;h2 id=&#34;create-vsphere-donor-windows-virtual-machine&#34;&gt;Create vSphere Donor Windows Virtual Machine&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Use vSphere web client wizard to create new hardware v10 virtual machine in vSphere with Thin vHDD&lt;/li&gt;
&lt;li&gt;Attach Windows DVD ISO&lt;/li&gt;
&lt;li&gt;Install Windows to virtual machine&lt;/li&gt;
&lt;li&gt;Disconnect DVD ISO&lt;/li&gt;
&lt;li&gt;Install VMware Tools&lt;/li&gt;
&lt;li&gt;Complete VMware Tools Reboot&lt;/li&gt;
&lt;li&gt;Power Down VM&lt;/li&gt;
&lt;li&gt;Export VM as OVF&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;create-openstack-image&#34;&gt;Create OpenStack Image&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Open OpenStack Console&lt;/li&gt;
&lt;li&gt;Change to your Project in left pane&lt;/li&gt;
&lt;li&gt;Select Manage Compute, Images and Snapshots&lt;/li&gt;
&lt;li&gt;Click Create Image&lt;/li&gt;
&lt;li&gt;Complete form using this as an example,  for Image File,  open the OVF file and select the VMDK you exported earlier&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/openstack-glance-win-Image-Create.png&#34; alt=&#34;Openstack Image&#34;&gt;&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;Click Create Image&lt;/li&gt;
&lt;li&gt;Wait while file uploads, it takes a while, you&amp;rsquo;ll be returned to console when it completes&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Note: this updates the file to folder glance_openstack on the datastore glance is configured to use. Once created you can cross ref the file UID with the OpenStack Console.&lt;/p&gt;
&lt;h2 id=&#34;test-openstack-image-works-as-instance&#34;&gt;Test OpenStack Image Works As Instance&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Open OpenStack Console&lt;/li&gt;
&lt;li&gt;Change to your Project in left pane&lt;/li&gt;
&lt;li&gt;Select Manage Compute Instances&lt;/li&gt;
&lt;li&gt;Click Launch Instance in right pane&lt;/li&gt;
&lt;li&gt;Complete Details Form&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/openstack-glance-win-InstanceLaunch.png&#34; alt=&#34;Openstack Instance Launch&#34;&gt;&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;Ensure correct Security Group selected on Access and Security Tab&lt;/li&gt;
&lt;li&gt;Ensure correct Network is selected on Networking tab&lt;/li&gt;
&lt;li&gt;Click Launch&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;vsan-abnormality&#34;&gt;VSAN Abnormality&lt;/h2&gt;
&lt;p&gt;If your Image is to go to a VSAN datastore and your using OpenStack Havana the above method will fail, this is because VSAN introduces a new disk type [streamOptimized] (&lt;a href=&#34;http://specs.openstack.org/openstack/nova-specs/specs/kilo/approved/vmware-vsan-support.html&#34;&gt;http://specs.openstack.org/openstack/nova-specs/specs/kilo/approved/vmware-vsan-support.html&lt;/a&gt;) which the UI is not aware of (this is fixed in Icehouse and later).&lt;/p&gt;
&lt;p&gt;In order to import these images you would need to use the OpenStack command line interface.  First open WebUI then  &amp;ldquo;Project -&amp;gt; Manage Compute -&amp;gt; Access &amp;amp; Security&amp;rdquo; and click Download OpenStack RC File.&lt;/p&gt;
&lt;p&gt;SCP the RC file to your Linux jump box /var/tmp and then use&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;source /var/tmp/&amp;lt;filename&amp;gt;.rc
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Enter password when prompted.&lt;/p&gt;
&lt;p&gt;Once you have authenticated,  run script like (substituting name and filename as required.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;glance --insecure --os-endpoint-type internalURL image-create &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--name Windows2008R2-VM10 &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--property vmware_disktype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;streamOptimized &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--property vmware_adaptertype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;lsiLogic &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--container-format&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;bare --disk-format&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;vmdk &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--is-public&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True &lt;span style=&#34;color:#ae81ff&#34;&gt;\
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;&lt;/span&gt;--file&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;/var/tmp/Windows2008R2-VM10-disk1.vmdk&amp;lt;/code&amp;gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;You should get output like&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/openstack-glance-win-streamOptimized2.png&#34; alt=&#34;Stream Optimized&#34;&gt;&lt;/p&gt;
&lt;p&gt;If you have uploaded an image already and found that its not streamOptimized you can change the attribute.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;glance image-update &amp;lt;image_name or uuid&amp;gt; --property vmware_disktype&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;streamOptimized
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
        
      </description>
    </item>
    
    <item>
      <title>VMware Virtual Volumes Deep Dive</title>
      <link>https://darrylcauldwell.github.io/post/vvols/</link>
      <pubDate>Fri, 24 Oct 2014 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/vvols/</guid>
      <description>
        
          &lt;h2 id=&#34;how-vvols-will-help&#34;&gt;How vVols Will Help&lt;/h2&gt;
&lt;p&gt;The constraints and issues which VVOLs address include, LUNs introduce constrains such as the quantity of LUNs we can present,  having lots of LUNs each with unused space is in efficient as such we currently shared VMs on data stores and this  gives a lack of granular control for passing storage attribute on a per-VM basis.&lt;/p&gt;
&lt;p&gt;Virtual Volumes addresses these challenges by rather than using the construct of LUNs on the storage array which is then formatted with VMFS filesystem.  Instead we would create on the storage array a Storage Container, the capabilities of the  Storage Container are exposed via the VMware Aware Storage API (VASA) provider and the data path is exposed via a Protocol Endpoint (PE). While the PE exposes the data path traffic is not funneled through it it provides a pointer, the ESXi to array IO is direct. The removal of the LUN construct and replacement the storage container removes the constraints of using LUNs.&lt;/p&gt;
&lt;p&gt;The capabilities of the VMware Aware Storage API (VASA) provider needs to be extended for VVOLs so will increment to version 2. The storage array vendor will provide this extended VASA provider, the current deployment of VASA providers is most typically on a management server, as this is now more critical, it is likely vendors will move its hosting to the array, by way of firmware update. By moving the VASA provider to the array will give the solution the inherent resilience built in to the array.&lt;/p&gt;
&lt;p&gt;The Virtual Volume is analogous to the file object which makes up a virtual machine. The other key thing which VVOLs gives us is that by directly exposing the storage we get more full access to offload work to the storage array.  For example snapshots traditionally are managed snapshots within vSphere but now these are performed directly as storage array operations as Un-(vSphere)managed snapshots which allows this to be done as efficiently as possible. As well as snapshotting all SCSI operations, ATS, XCOPY, UNMAP, TRIM etc are also performed natively. One thing to note is that snapshotting of a running virtual machine while being much fantastically improved is not instantaneous for a running virtual machine as the memory bitmap is still required to be written.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://www.wooditwork.com/wp-content/uploads/2014/08/image_thumb18.png&#34; alt=&#34;Virtual Volumes&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;vvols-deep-dive&#34;&gt;VVOLs Deep Dive&lt;/h2&gt;
&lt;p&gt;The Protocol Endpoint (PE) is compatible with all SAN and NAS protocols and each can support any one of the protocols at a given time. The PE is still a SCSI device and as such is managed by the Pluggable Storage Architecture (PSA) which applies the multi-path policies, the PSA is modified slightly to be aware of the PE construct. There can be multiple PEs defined an SCSI PE is picked up during a rescan. While you may at first thing that VVOLs is purely for block storage it is also a construct which can be used with NFS, a NFS PE is maintained as an IP address or file path, ESX identifies and maintains all discovered PEs within a database. Each Virtual volumes has a UID, the data paths are established through a VASA bind request, the bind request is processed by VASA and the mapping of UID and paths can be many to many and all bindings are stored within the database.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;http://datacenterdude.com/wp-content/uploads/2012/10/vmware-vvol-1.png&#34; alt=&#34;Protocol Endpoint&#34;&gt;&lt;/p&gt;
&lt;p&gt;The storage container holds the capabilities of the storage, capabilities would be things like performance tier (Diamond, Gold, Silver, Bronze), backup, snapshots, whether it is de-duplicated. We can therefore use storage containers as logical partitions to apply  storage needs and requirements. There would be a minimum of one storage container per array and maximum would depend on vendor. There is no direct mapping between storage container and PE, a PE can manage multiple storage containers and multiple PEs can manage a single storage container. These storage container capabilities are then advertised for use by ESX through the VMware Aware Storage API (VASA) provider. Surprisingly at first we still need to form Storage Containers into datastores within vSphere. This requirement is to allow all the traditional features of vSphere HA, SDRS etc, which need to interact with the datastore construct.&lt;/p&gt;
&lt;p&gt;A virtual volume is per virtual machine but within it is broken up by capability requirement into five sub-types,&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Config-VVOL&lt;/li&gt;
&lt;li&gt;Data-VVOL&lt;/li&gt;
&lt;li&gt;Mem-VVOL&lt;/li&gt;
&lt;li&gt;Swap-VVOL&lt;/li&gt;
&lt;li&gt;Other-VVOL&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With this granularity we can now place each VVOL sub-type onto a Storage Container with the best match for its capability requirement.&lt;/p&gt;
&lt;p&gt;To consume VVOLs effectively and on a per object (per virtual machine) basis we need to move towards Storage Policy Based Management (SPBM) so we can consume the storage capabilities presented with VVOLs by VMware Aware Storage API (VASA) provider.&lt;/p&gt;
&lt;p&gt;In this current release mentioned as 2015 SRM is not supported, although it was mentioned that that is on radar for 2016. The use of RDMs is also not supported within VVOLs, you can however present these as normal outside of the VVOL container.&lt;/p&gt;
&lt;h2 id=&#34;try-vvols-at-home&#34;&gt;Try VVOLs At Home&lt;/h2&gt;
&lt;p&gt;Up until recently you couldn&amp;rsquo;t realistically configure a VVOLs solution unless you had a spare array running beta firmware which supported VASA2. That is until NetApp shipped &lt;a href=&#34;http://mysupport.netapp.com/NOW/download/tools/simulator/ontap/8.X/&#34;&gt;OnTap simulator 8.2.1&lt;/a&gt;, the NetApp Simulator is run as a nested VM under Fusion, Workstaion or ESX and provides the interface for vSphere 6 to connect to.&lt;/p&gt;

        
      </description>
    </item>
    
  </channel>
</rss>
