<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>raspberry on </title>
    <link>https://darrylcauldwell.github.io/categories/raspberry/</link>
    <description>Recent content in raspberry on </description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 22 May 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://darrylcauldwell.github.io/categories/raspberry/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Raspberry Pi Kubernetes Cluster</title>
      <link>https://darrylcauldwell.github.io/post/homelab-pi-microk8s/</link>
      <pubDate>Sat, 22 May 2021 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/homelab-pi-microk8s/</guid>
      <description>
        
          &lt;p&gt;The Raspberry Pi is a useful platform for self-hosting applications. More often than not self-hosted applications are supplied as container images. I used to run one application on one Raspberry Pi. The Raspberry Pi 4 ships with a 1.5GHz Quad-core CPU, USB3 and upto 8GB RAM. Here I am looking at clustering four to host the various containers I run and use a scheduling engine. I am using 8GB Pi 4Bs configured to mass storage boot from 128GB USB3 flash drives built with Ubuntu 21.04 LTS.&lt;/p&gt;
&lt;h2 id=&#34;rancher-k3s-or-canonical-microk8s&#34;&gt;Rancher K3S OR Canonical Microk8s&lt;/h2&gt;
&lt;p&gt;K3s and Microk8s are both lightweight implementation of Kubernetes they have various differences but a key one to understand is high availablity approach.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Function&lt;/th&gt;
&lt;th&gt;MicroK8s&lt;/th&gt;
&lt;th&gt;K3s&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Kubernetes Node Roles&lt;/td&gt;
&lt;td&gt;Every node is a worker node&lt;/td&gt;
&lt;td&gt;Agent Node(s)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kubernetes API Server&lt;/td&gt;
&lt;td&gt;Every node is an API server&lt;/td&gt;
&lt;td&gt;Server Node (s)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kubernetes Datastore&lt;/td&gt;
&lt;td&gt;Dqlite&lt;/td&gt;
&lt;td&gt;etcd&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Datastore HA&lt;/td&gt;
&lt;td&gt;Embedded Dqlite when &amp;gt;3 nodes&lt;/td&gt;
&lt;td&gt;Requires External datastore&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I want my four node cluster to be highly available as easily as possible so for this deployment I chose Microk8s.&lt;/p&gt;
&lt;h2 id=&#34;microk8s-pre-requisit&#34;&gt;Microk8s Pre-requisit&lt;/h2&gt;
&lt;p&gt;Some Raspberry Pis have limited RAM so cgroup memory support is disabled by default. We can update all of the Pi&amp;rsquo;s to enable cgroup memory during bootstrap.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vi `/boot/firmware/cmdline.txt`

## Add the following options
cgroup_enable=memory cgroup_memory=1

## Reboot for changes to take effect.

sudo reboot
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/homelab-pi-microk8s-cgroup.png&#34; alt=&#34;Microk8s cgroup&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;install-microk8s&#34;&gt;Install Microk8s&lt;/h2&gt;
&lt;p&gt;MicroK8s is supplied as a snap, there are various Kubernetes releases these are available as snap channels.  To see all available versions we can query what channels are available. When I&amp;rsquo;m running this the current Kubernetes version is 1.21 and I filter results on this we can see all.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;snap info microk8s | grep 1.21

latest/stable:    v1.21.1  2021-05-17 (2215) 168MB classic
latest/candidate: v1.21.1  2021-05-13 (2214) 168MB classic
latest/beta:      v1.21.1  2021-05-13 (2214) 168MB classic
latest/edge:      v1.21.1  2021-05-19 (2227) 168MB classic
1.21/stable:      v1.21.1  2021-05-17 (2215) 168MB classic
1.21/candidate:   v1.21.1  2021-05-14 (2215) 168MB classic
1.21/beta:        v1.21.1  2021-05-14 (2215) 168MB classic
1.21/edge:        v1.21.1  2021-05-20 (2232) 168MB classic
1.14/stable:      v1.14.10 2019-12-20 (1121) 164MB classic
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;I&amp;rsquo;ll look to install the stable release of 1.21 and add ubuntu user to microk8s group on each using:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo snap install microk8s --classic --channel=1.21/stable
sudo usermod -a -G microk8s ubuntu
newgrp microk8s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When the snap is loaded and running on all nodes we can look to form them into a cluster.  On first node run following and it will generate a token to run on remote node to add it.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## From first node in cluster generate token  
sudo microk8s add-node

From the node you wish to join to this cluster, run the following:
microk8s join 192.168.1.100:25000/73d707e38ddb7c7bbcf29a328a505179/7e8d2c9a15af
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then on remote node run command with generated token.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo microk8s join 192.168.1.100:25000/73d707e38ddb7c7bbcf29a328a505179/7e8d2c9a15af

Contacting cluster at 192.168.1.100
Waiting for this node to finish joining the cluster. ..  
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Repeat this process (generate a token, run it from the joining node) for the third and forth nodes.  When all done from any node we can query the cluster state see all four nodes and check high availability status.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo microk8s status

microk8s is running
high-availability: yes
  datastore master nodes: 192.168.1.100:19001 192.168.1.101:19001 192.168.1.102:19001
  datastore standby nodes: 192.168.1.103:19001
addons:
  enabled:
    ha-cluster           # Configure high availability on the current node
  disabled:
    dashboard            # The Kubernetes dashboard
    dns                  # CoreDNS
    helm                 # Helm 2 - the package manager for Kubernetes
    helm3                # Helm 3 - Kubernetes package manager
    host-access          # Allow Pods connecting to Host services smoothly
    ingress              # Ingress controller for external access
    linkerd              # Linkerd is a service mesh for Kubernetes and other frameworks
    metallb              # Loadbalancer for your Kubernetes cluster
    metrics-server       # K8s Metrics Server for API access to service metrics
    portainer            # Portainer UI for your Kubernetes cluster
    prometheus           # Prometheus operator for monitoring and logging
    rbac                 # Role-Based Access Control for authorisation
    registry             # Private image registry exposed on localhost:32000
    storage              # Storage class; allocates storage from host directory
    traefik              # traefik Ingress controller for external access
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;install-kubectl&#34;&gt;Install kubectl&lt;/h2&gt;
&lt;p&gt;The Microk8s install deploys its own client and to execute have to remember to prefix everything with microk8s. I have to switch between systems often and to avoid confusion prefer to install normal kubectl. Again this is available as a snap in multiple versions.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;snap info kubectl | grep 1.21

latest/stable:    1.21.1         2021-05-14 (1970)  9MB classic
latest/candidate: 1.21.1         2021-05-14 (1970)  9MB classic
latest/beta:      1.21.1         2021-05-14 (1970)  9MB classic
latest/edge:      1.21.1         2021-05-14 (1970)  9MB classic
1.21/stable:      1.21.1         2021-05-13 (1970)  9MB classic
1.21/candidate:   1.21.1         2021-05-13 (1970)  9MB classic
1.21/beta:        1.21.1         2021-05-13 (1970)  9MB classic
1.21/edge:        1.21.1         2021-05-13 (1970)  9MB classic
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We want to install version to match Kubernetes and copy the Microk8s kubectl config to be kubectl default config.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo snap install kubectl --classic --channel=1.21/stable
mkdir ~/.kube
sudo microk8s config &amp;gt; ~/.kube/config
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;cluster-networking&#34;&gt;Cluster Networking&lt;/h2&gt;
&lt;p&gt;The base cluster networking provides communication between different Pods within the cluster. A kubernetes service resource is an abstraction which defines a logical set of Pods. The service can be defined as type ClusterIP, NodePort or LoadBalancer. The type:ClusterIP is only available between pods. The easiest way to expose externally is via type:NodePort this allocates a high port, between 30,000 to 32,767 and provides external port mapping on every host.&lt;/p&gt;
&lt;h2 id=&#34;nodeport&#34;&gt;NodePort&lt;/h2&gt;
&lt;p&gt;Microk8s provides this out of the box. We can test this is working by first creating a web server which itself listens on port 80. Then create a service of type:NodePort. We can then view the service details to obtain port allocation and either run curl localhost or off-host to IP via external web browser.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl create namespace net-test
kubectl config set-context --current --namespace&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;net-test
kubectl create deployment nginx --image&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;nginx --replicas&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; --port&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
kubectl get deployments -o wide

NAME    READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES   SELECTOR
nginx   2/2     &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;            &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;           65s   nginx        nginx    app&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;nginx

kubectl expose deployment nginx --type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;NodePort
kubectl get service -o wide

NAME    TYPE       CLUSTER-IP       EXTERNAL-IP   PORT&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;S&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;        AGE
nginx   NodePort   10.152.183.212   &amp;lt;none&amp;gt;        80:31508/TCP   9s

curl localhost:31508
kubectl delete service nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;cluster-ingress&#34;&gt;Cluster Ingress&lt;/h2&gt;
&lt;p&gt;Ingress exposes HTTP and HTTPS routes from outside the cluster to services within the cluster. To configure Ingress requires an Ingress Controller, Microk8s offers choice of two addons namely NGINX or Traefik. I looked to install NGINX with the Microk8s ingress addon.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;microk8s enable dns ingress
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;cluster-load-balancer&#34;&gt;Cluster Load Balancer&lt;/h2&gt;
&lt;p&gt;A virtual IP is provided by load-lalancer, private and public cloud platforms typically provide an external layer 4 load-balancer. For bare metal there is no external load-balancer, instead we can look to &lt;a href=&#34;https://metallb.org/&#34;&gt;MetalLB&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Usefully Microk8s ships a MetalLB addon option, the installation takes parameter of address range of IPs to issue. When we pass the parameter the installer created the MetalLB ConfigMap.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;microk8s enable metallb:192.168.1.104-192.168.1.110
kubectl expose deployment nginx --port&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt; --type&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;LoadBalancer
kubectl get service

NAME    TYPE           CLUSTER-IP       EXTERNAL-IP     PORT&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;S&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;        AGE   SELECTOR
nginx   LoadBalancer   10.152.183.132   192.168.1.104   80:30461/TCP   6s    app&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;nginx

kubectl delete service nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;load-balanced-ingress&#34;&gt;Load Balanced Ingress&lt;/h2&gt;
&lt;p&gt;The ingress controller is deployed as a DaemonSet which creates a ingress controller pod on each host in the cluster. Each pod listens has a containerPort 80 for http,  443 for https and 10254 which is a health probe.&lt;/p&gt;
&lt;p&gt;We can look to create a service resource type:LoadBalancer which re-directs and balances traffic across the cluster.&lt;/p&gt;
&lt;p&gt;Save the following as file named ingress-service.yaml&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Service&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ingress&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;selector&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx-ingress-microk8s&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;type&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;LoadBalancer&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;ports&lt;/span&gt;:
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;http&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
    - &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;https&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;protocol&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;TCP&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;443&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;targetPort&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;443&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Apply the file to create the service&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f ingress-service.yaml
kubectl -n ingress get svc -o wide

NAME      TYPE           CLUSTER-IP      EXTERNAL-IP     PORT&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;S&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;                      AGE   SELECTOR
ingress   LoadBalancer   10.152.183.54   192.168.1.104   80:31707/TCP,443:31289/TCP   7s    name&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;nginx-ingress-microk8s
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With this in place we can look to create a Ingress.&lt;/p&gt;
&lt;p&gt;Save the following as file named nginx-ingress.yaml&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;apiVersion&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;networking.k8s.io/v1&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;kind&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Ingress&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;metadata&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx-ingress&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;namespace&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;net-test&lt;/span&gt;
  &lt;span style=&#34;color:#f92672&#34;&gt;annotations&lt;/span&gt;:
    &lt;span style=&#34;color:#f92672&#34;&gt;kubernetes.io/ingress.class&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;public&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;spec&lt;/span&gt;:
  &lt;span style=&#34;color:#f92672&#34;&gt;rules&lt;/span&gt;:
  - &lt;span style=&#34;color:#f92672&#34;&gt;http&lt;/span&gt;:
      &lt;span style=&#34;color:#f92672&#34;&gt;paths&lt;/span&gt;:
      - &lt;span style=&#34;color:#f92672&#34;&gt;backend&lt;/span&gt;:
          &lt;span style=&#34;color:#f92672&#34;&gt;service&lt;/span&gt;:
            &lt;span style=&#34;color:#f92672&#34;&gt;name&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;nginx&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;port&lt;/span&gt;: 
              &lt;span style=&#34;color:#f92672&#34;&gt;number&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;path&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;/&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;pathType&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;Prefix&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Apply the file to create the service and we can use cURL to get nginx homepage.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl apply -f nginx-ingress.yaml
kubectl -n net-test get ingress -o wide

NAME            CLASS    HOSTS   ADDRESS   PORTS   AGE
nginx-ingress   &amp;lt;none&amp;gt;   *                 &lt;span style=&#34;color:#ae81ff&#34;&gt;80&lt;/span&gt;      4s

curl 192.168.1.104
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;web-ui-dashboard&#34;&gt;Web UI (Dashboard)&lt;/h2&gt;
&lt;p&gt;The Kubernetes web UI is a convenient way to view the cluster. This is supplied as a microk8s addon so simple to install.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;sudo microk8s enable dashboard
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This creates a deployment exposed as a service type:ClusterIP on port 8443. We can edit the service and change to type:LoadBalancer so we can easier consume.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;kubectl -n kube-system get service kubernetes-dashboard -o wide

NAME                   TYPE        CLUSTER-IP       EXTERNAL-IP   PORT&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;S&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;   AGE     SELECTOR
kubernetes-dashboard   ClusterIP   10.152.183.172   &amp;lt;none&amp;gt;        443/TCP   4m14s   k8s-app&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;kubernetes-dashboard

kubectl -n kube-system edit service kubernetes-dashboard
kubectl -n kube-system get service kubernetes-dashboard -o wide

NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP     PORT&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;S&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;         AGE     SELECTOR
kubernetes-dashboard   LoadBalancer   10.152.183.172   192.168.1.105   443:31093/TCP   5m39s   k8s-app&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;kubernetes-dashboard
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/homelab-pi-microk8s-dashboard.png&#34; alt=&#34;Microk8s Dashboard&#34;&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Pi Supercomputer</title>
      <link>https://darrylcauldwell.github.io/post/homelab-pi-mpi/</link>
      <pubDate>Tue, 30 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/homelab-pi-mpi/</guid>
      <description>
        
          &lt;p&gt;Following on from &lt;a href=&#34;https://darrylcauldwell.github.io/post/homelab-pi&#34;&gt;base Ubuntu build&lt;/a&gt; here I begin to look at Parrallel Programming.&lt;/p&gt;
&lt;h2 id=&#34;parrallel-programming&#34;&gt;Parrallel Programming&lt;/h2&gt;
&lt;p&gt;Each Raspberry Pi is a small unit of compute, one of my goals is to understand how operating many in a cluster. There are various approaches to parallel computational models, message-passing has proven to be an effective one. MPI the Message Passing Interface, is a standardized and portable message-passing system designed to function on a wide variety of parallel computers.&lt;/p&gt;
&lt;p&gt;My prefered language is Python and usefully there is &lt;a href=&#34;https://mpi4py.readthedocs.io/en/stable/install.html&#34;&gt;MPI for Python&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get install -y libopenmpi-dev python-dev pip
sudo pip install mpi4py
mpirun --version
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;All the RPIs in the cluster will access each other via SSH, this communication needs to be passwordless. The first thing you need to do is generate an SSH key pair on first host.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh-keygen -t rsa -b 4096
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once key generated to enable passwordless access, upload a copy of the public key to the other three servers.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh-copy-id ubuntu@[server_ip_address]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Repeat keygen and copy public key process on all four hosts.&lt;/p&gt;
&lt;p&gt;In order for mpi to distribute workload the node where execution occurs needs to understand which nodes are available.  The machinename parameter can be used to point to a text file containing list of nodes.&lt;/p&gt;
&lt;p&gt;In order name we can use names we can add entries to hosts file.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo sh -c &#39;echo &amp;quot;192.168.1.100 rpi-0&amp;quot; &amp;gt;&amp;gt; /etc/hosts&#39;
sudo sh -c &#39;echo &amp;quot;192.168.1.101 rpi-1&amp;quot; &amp;gt;&amp;gt; /etc/hosts&#39;
sudo sh -c &#39;echo &amp;quot;192.168.1.102 rpi-2&amp;quot; &amp;gt;&amp;gt; /etc/hosts&#39;
sudo sh -c &#39;echo &amp;quot;192.168.1.103 rpi-3&amp;quot; &amp;gt;&amp;gt; /etc/hosts&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can then create a file in home directory listing nodes.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt;&amp;gt; ~/machinefile
rpi-0
rpi-1
rpi-2
rpi-3
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The easiest way to run commands or code is with the mpirun command. This command, run in a shell, will launch multiple copies of your code, and set up communications between them. As each Pi has four cores and we have four we can specify number of processors to run as sixteen.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mpirun -np 16 -machinefile ~/machinefile vcgencmd measure_temp
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/mpirun_temp.png&#34; alt=&#34;MPIRUN Measure Temparature&#34;&gt;&lt;/p&gt;
&lt;p&gt;While its interesting running individual commands across nodes the MPI for Python module exposes options for programs to spread load.  A simple test of this is where we scatter a bunch of elements, like those in a list, to processing nodes.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt;&amp;gt; ~/scatter.py
from mpi4py import MPI

comm = MPI.COMM_WORLD
size = comm.Get_size()
rank = comm.Get_rank()

if rank == 0:
   data = [(x+1)**x for x in range(size)]
   print (&#39;we will be scattering:&#39;,data)
else:
   data = None
   
data = comm.scatter(data, root=0)
print (&#39;rank&#39;,rank,&#39;has data:&#39;,data)
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once script created execute:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mpirun -np 16 -machinefile ~/machinefile python3 ~/scatter.py
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/mpirun_scatter.png&#34; alt=&#34;MPIRUN Scatter&#34;&gt;&lt;/p&gt;
&lt;p&gt;While scattering elements of a list is interesting splitting up a processing problem and distributing to multiple processing nodes is more fun.  Calculating Pi is a nice example of this,  and its nice for a Pi cluster to calculate Pi with MPI.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pi.py&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; mpi4py &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; MPI

comm &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; MPI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COMM_WORLD
rank &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; comm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Get_rank()
size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; comm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Get_size()

slice_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1000000&lt;/span&gt;
total_slices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# This is the controller node.&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; rank &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
    pi &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
    slice &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
    process &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;

    print (size)

    &lt;span style=&#34;color:#75715e&#34;&gt;# Send the first batch of processes to the nodes.&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; process &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; size &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; slice &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; total_slices:
        comm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;send(slice, dest&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;process, tag&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
        print (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Sending slice&amp;#34;&lt;/span&gt;,slice,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;to process&amp;#34;&lt;/span&gt;,process)
        slice &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
        process &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;

    &lt;span style=&#34;color:#75715e&#34;&gt;# Wait for the data to come back&lt;/span&gt;
    received_processes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; received_processes &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; total_slices:
        pi &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; comm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recv(source&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;MPI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ANY_SOURCE, tag&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
        process &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; comm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recv(source&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;MPI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ANY_SOURCE, tag&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
        print (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Recieved data from process&amp;#34;&lt;/span&gt;, process)
        received_processes &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;

        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; slice &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; total_slices:
            comm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;send(slice, dest&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;process, tag&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
            print (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Sending slice&amp;#34;&lt;/span&gt;,slice,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;to process&amp;#34;&lt;/span&gt;,process)
            slice &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;

    &lt;span style=&#34;color:#75715e&#34;&gt;# Send the shutdown signal&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; process &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,size):
        comm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;send(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, dest&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;process, tag&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)

    print (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pi is&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4.0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; pi)

&lt;span style=&#34;color:#75715e&#34;&gt;# These are the worker nodes, where rank &amp;gt; 0. They do the real work&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;True&lt;/span&gt;:
        start &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; comm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recv(source&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, tag&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; start &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;

        i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
        slice_value &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; slice_size:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i&lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
                slice_value &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;(start&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;slice_size&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;i)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
            &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
                slice_value &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;(start&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;slice_size&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;i)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
            i &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;

        comm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;send(slice_value, dest&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, tag&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
        comm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;send(rank, dest&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, tag&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Execute script with mpi and watch it distribute the calculation around the nodes and aggregate these to final answer.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mpirun -np 16 -machinefile ~/machinefile python3 pi.py
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/pi_with_mpi_on_pi.png&#34; alt=&#34;Pi with MPI on Pi&#34;&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Comparing SD and USB 3 Storage Performance With Raspberry Pi4B</title>
      <link>https://darrylcauldwell.github.io/post/homelab-pi-storage/</link>
      <pubDate>Mon, 29 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/homelab-pi-storage/</guid>
      <description>
        
          &lt;p&gt;Following on from &lt;a href=&#34;https://darrylcauldwell.github.io/post/homelab-pi&#34;&gt;base Ubuntu build&lt;/a&gt; here I look at the comparing the storage performance of SD and USB.&lt;/p&gt;
&lt;h2 id=&#34;sd-card-performance&#34;&gt;SD Card Performance&lt;/h2&gt;
&lt;p&gt;Linux FIO tool will be used to measure sequential write performance of a 4GB file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=4G --readwrite=write
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/sd_reads.png&#34; alt=&#34;SD Card Read Performance&#34;&gt;&lt;/p&gt;
&lt;p&gt;The tool output shows sequential read rate:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;IOPS=10.1k, BW=39.5MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=10.1k, BW=39.3MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=10.1k, BW=39.4MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=10.1k, BW=39.3MiB/s&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Similarly, the tool will be used to measure sequential read performance of a 4GB file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=4G --readwrite=read
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/sd_writes.png&#34; alt=&#34;SD Card Write Performance&#34;&gt;&lt;/p&gt;
&lt;p&gt;The tool output shows sequential write rate:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;IOPS=5429, BW=21.2MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=5128, BW=20.0MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=5136, BW=20.1MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=5245, BW=20.5MiB/s&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With the SD card the performance bottleneck is the reader which supports peak bandwidth 50MiB/s. To test this I has a lower spec SanDisk Ultra card so I repeated test and got near exact throughput to the SanDisk Extreme.&lt;/p&gt;
&lt;p&gt;The tool output shows sequential read rate:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;IOPS=10.1k, BW=39.4MiB/s&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The tool output shows sequential write rate:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;IOPS=5245, BW=20.5MiB/s&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;usb-flash-performance&#34;&gt;USB Flash Performance&lt;/h2&gt;
&lt;p&gt;The USB flash drive should deliver improved performance, first check it can be seen by the system and note its device.&lt;/p&gt;
&lt;p&gt;Then repeated the same performance tests using fio on the SSD. Linux FIO tool will be used to measure sequential write/read performance of a 4GB file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /mnt/ssd
fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=4G --readwrite=write
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/usb_writes.png&#34; alt=&#34;USB Write Performance&#34;&gt;&lt;/p&gt;
&lt;p&gt;The tool output shows sequential write rate:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;IOPS=14.5k, BW=56.6MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=14.4k, BW=56.4MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=14.4k, BW=56.2MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=11.9k, BW=46.6MiB/s&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;cd /mnt/ssd
fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/mnt/ssd/test --bs=4k --iodepth=64 --size=4G --readwrite=read
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/usb_reads.png&#34; alt=&#34;USB Read Performance&#34;&gt;&lt;/p&gt;
&lt;p&gt;The tool output shows sequential read rate:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;IOPS=33.3k, BW=130MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=37.0k, BW=148MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=42.6k, BW=166MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=42.5k, BW=166MiB/s&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;So for a very small investment in the USB Flash Drives we&amp;rsquo;ve increased sequential read potential by 4X and write potential by 3X over the SD card.  The Pi 4 firmware doesn&amp;rsquo;t presently offer option for USB boot so the SD cards are needed but hopefully soon the firmware will get updated.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Raspberry Pi Cluster Ubuntu Base</title>
      <link>https://darrylcauldwell.github.io/post/homelab-pi-ubuntu/</link>
      <pubDate>Sun, 28 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/homelab-pi-ubuntu/</guid>
      <description>
        
          &lt;p&gt;Following on from &lt;a href=&#34;https://darrylcauldwell.github.io/post/homelab-pi&#34;&gt;Raspberry Pi Cluster&lt;/a&gt; here I look at the base Ubuntu build.&lt;/p&gt;
&lt;h2 id=&#34;install-and-configure&#34;&gt;Install and Configure&lt;/h2&gt;
&lt;p&gt;I used &lt;a href=&#34;https://www.raspberrypi.org/software/&#34;&gt;Raspberry Pi imager&lt;/a&gt; to install the Ubuntu 20.10 64bit on each SD Card.  Insert these into the Pi&amp;rsquo;s and power them on.&lt;/p&gt;
&lt;p&gt;The image is configured with DHCP client, &lt;a href=&#34;https://maclookup.app/macaddress/DCA632&#34;&gt;Pi device MAC addresses are prefixed DC:A6:32&lt;/a&gt;. I connected to my router which acts as DHCP server and found the four leases sorting by MAC. With the DHCP addresses can connect via SSH, the Ubuntu image has default username of &lt;code&gt;ubuntu&lt;/code&gt; and password &lt;code&gt;ubuntu&lt;/code&gt;. You&amp;rsquo;re prompted to change password at first connect.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/ubuntu-pw.png&#34; alt=&#34;Ubuntu Password&#34;&gt;&lt;/p&gt;
&lt;p&gt;I want to reliably know how to connect to these and like to change from dynamic to a staticly asssigned IP address. To do this for Ubuntu 20.10 we update Netplan configuration.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo vi /etc/netplan/50-cloud-init.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Here is example of how I update this to reflect static IP.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;network:
    ethernets:
        eth0:
            addresses: [192.168.1.100/24]
            gateway4: 192.168.1.254
            nameservers:
              addresses: [8.8.8.8,8.8.4.4]
            dhcp4: no
            match:
                driver: bcmgenet smsc95xx lan78xx
            optional: true
            set-name: eth0
    version: 2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With the configuration file updated can have netplan load the config.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo netplan --debug apply
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With any new install its useful to apply latest patches.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt update
sudo apt upgrade -y
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;install-tools&#34;&gt;Install Tools&lt;/h2&gt;
&lt;p&gt;The VideoCore packages provide command line utilities that can get various pieces of information from the VideoCore GPU on the Raspberry Pi. The linux flexible I/O tester tool is  easy to use and useful for understanding storage sub-system performance.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt install -y libraspberrypi-bin
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;disable-red-led&#34;&gt;Disable Red LED&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;sudo su
echo none &amp;gt; /sys/class/leds/led1/trigger
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;storage-performance&#34;&gt;Storage Performance&lt;/h2&gt;
&lt;p&gt;The linux flexible I/O tester tool is  easy to use and useful for understanding storage sub-system performance.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt install -y fio
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;usb-flash-disk&#34;&gt;USB Flash Disk&lt;/h2&gt;
&lt;p&gt;The SD card on the Pi will normally show as /dev/mmcblk0. The USB drive will normally show as /dev/sda. The following could be data destructive so check the enumeration before proceeding.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo fdisk -l
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/usb_dev.png&#34; alt=&#34;USB Device&#34;&gt;&lt;/p&gt;
&lt;p&gt;Then create primary partition on USB device&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo sfdisk /dev/sda &amp;lt;&amp;lt; EOF
;
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/usb_dev.png&#34; alt=&#34;USB Partition&#34;&gt;&lt;/p&gt;
&lt;p&gt;Then format and label the partition then mount and set permissions for the parition&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo mkfs.ext4 -L SSD /dev/sda1
sudo mkdir /mnt/ssd
sudo mount /dev/sda1 /mnt/ssd
echo &amp;quot;LABEL=SSD  /mnt/ssd  ext4  defaults 0 2&amp;quot; | sudo cat /etc/fstab -
sudo chmod 777 .
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/usb_ext4.png&#34; alt=&#34;USB Mount&#34;&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Raspberry Pi Cluster Hardware</title>
      <link>https://darrylcauldwell.github.io/post/homelab-pi/</link>
      <pubDate>Sat, 27 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/homelab-pi/</guid>
      <description>
        
          &lt;p&gt;I have worked most of my professional life with systems built with Intel x86, x64 and ia64 chips. Intel processors use Complex Instruction Set Computing while Arm uses Reduced Instruction Set Computing. Processors with CISC architecture look to move complexity to hardware to simlify code, so small code sizes with high cycles per second. Processors with RISC architecture invert the relationship larger code size with low cycles per second.&lt;/p&gt;
&lt;p&gt;Anyone who has worked in a data center has an awareness of importance of the amount of heat generated by Intel powered servers. There isn’t much heat generated from allowing electricity to flow through something (unless there is A LOT of electricity). All of Arm’s designs are energy efficient which is why they have become popular for running in smartphones, tablets and other embedded devices. If Arm processors can get traction used in servers this can only be good news for data center electricity usage.&lt;/p&gt;
&lt;p&gt;I enjoy learning by doing and to begin to better understand Arm looked to build a cluster of Raspberry Pi. My goals of cluster include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Understanding more about Raspberry Pi hardware&lt;/li&gt;
&lt;li&gt;Understanding how an distributed application might work across physical hosts using MPI. MPI is a programming model that is widely used for parallel programming in a cluster.&lt;/li&gt;
&lt;li&gt;Understanding how micro-servies application may work in Edge locations running Kubernetes on low-cost hardware.&lt;/li&gt;
&lt;li&gt;Understanding how ESXi on Arm development is progressing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Project repository: &lt;a href=&#34;https://github.com/darrylcauldwell/piCluster&#34;&gt;https://github.com/darrylcauldwell/piCluster&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;pi-cluster-rack-hardware&#34;&gt;Pi Cluster Rack Hardware&lt;/h1&gt;
&lt;h2 id=&#34;power-over-ethernet&#34;&gt;Power over Ethernet&lt;/h2&gt;
&lt;p&gt;One of my goals for the cluster was the hardware self-contained and easy to connect to network and power. Out of the box the Raspberry Pi 4b is powered over USB-C and operates at 5.1V upto 3A so has maximum draw (5.1*3=) 15.3watt. While I could use one official Raspberry Pi USB-C 15.3watt power supply for each this would require four power sockets. A cleaner cabling solution is to equip each Pi 4b with a PoE HAT and power each using a Power over Ethernet–enabled network. When the Pi is cabled to a PoE network switch port the power gets routed to four PoE pins on the Pi board. The PoE switch ports deliver between 27-57volts. The PoE HAT connects to the four PoE pins and has onboard transformer which converts input voltage to 5volt and then routes this to the GPIO power pins.&lt;/p&gt;
&lt;h2 id=&#34;network-switch&#34;&gt;Network Switch&lt;/h2&gt;
&lt;p&gt;For selecting the PoE network switch there are two important factors I considered.  PoE has two specifications the initial 802.3af (upto 15watt) and revised 802.3at (upto 30watt), the Pi 4b has 15.3watt maximum draw so ports supporting 802.3af.  Another important factor to consider is maximum draw across  all PoE ports here our maximum draw would be (15.3*4=) 61.2watts. The Netgear GS305P has five ports with four supported for 802.3af or 802.3at and is rated for 63W power draw, it is also compact with dimensions 158 x 101 x 29 mm.&lt;/p&gt;
&lt;h2 id=&#34;storage&#34;&gt;Storage&lt;/h2&gt;
&lt;p&gt;For storage selection the SanDisk Extreme offers sequential read up to 160MB/sec and sequential write up to 90MB/sec. The SanDisk Ultra offers sequential read up to 100MB/sec and sequential write up to 90MB/sec. The Pi4B has a dedicated SD card socket which suports 1.8V, DDR50 mode (at a peak bandwidth of 50 Megabytes / sec). The Pi 4h as a USB 3 interface which has peak bandwidth 620 Megabytes / sec. The Arcanite 128GB USB 3.1 Flash Drive has a small formfactor and low cost and offers read speeds up to 400 MB/s and write speeds up to 100 MB/s.&lt;/p&gt;
&lt;h2 id=&#34;mounting-rack&#34;&gt;Mounting Rack&lt;/h2&gt;
&lt;p&gt;Towards goal of keeping cluster self contained I wanted to house the network switch and four Pi 4b in a rack. There are vaious off the shelf options for stacking Pi&amp;rsquo;s but I couldn&amp;rsquo;t find a great solution for my specific requirement. I&amp;rsquo;d not done a project using custom cut metal before and thought this would be a  good opportunity to explore. I thought aluminium would be good to polish to a nice finish so I decided to use 3mm medium strength 5251 aluminium.&lt;/p&gt;
&lt;p&gt;I found a UK based mail-order laser cutting provider &lt;a href=&#34;https://lasered.co.uk/&#34;&gt;Lasered&lt;/a&gt;. To make the order required drawing in either Drawing Interchange Format (DXF), AutoCAD (DWG) of Mastercam Numerical Control File (NC) format to  create this I used open-source LibreCAD software. I&amp;rsquo;d not used CAD software before so there was a learning curve but this was not large and within few hours I&amp;rsquo;d created two drawings. The cluster will primarily be used  as Kubernetes cluster so I designed all the plates shaped as heptagons, the top and base plates also have Raspberry Pi logo. For attaching plates together allowing enough space for airflow I chose 35mm standoffs with M4 thread to accept M4 I gave holes a 4.2mm radius. For mounting the Pi the board has 2.7mm holes to accept M2.5 thread screws so I mirror these on the plate with 2.7mm radius and use 5mm standoff to lift slightly. I added a rectangular hole behin Pi on each shelf to allow for internal PoE cable routing. The rack dimensions when assembled, widest points the heptagon is 210mm and (6x3mm=)18mm plates plus (5x40mm)=200 standoffs gives total height of 218mm.
&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/cut_plates.jpeg&#34; alt=&#34;Laser cut plates&#34;&gt;&lt;/p&gt;
&lt;p&gt;I got the plates cut and they looked better than expected except the standoff holes looked rather large. I checked the the calipers and noticed my planned 4.2mm holes were 8.4mm and my 2.7mm holes 5.4mm. Seems I had entered diameter as value for circle radius parameter. Luckily I hadn&amp;rsquo;t ordered the standoffs, nuts or bolts. It was easy to switched from M4 to M6 for the between layer standoffs but as the Pi board only accepts M2.5 I kept these and added a washer to prevent bolt going straight through the mount hole on the shelf.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/radius_diameter.jpeg&#34; alt=&#34;Oops radius != diameter&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/first_assembly.jpeg&#34; alt=&#34;First assembly&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;rack-cabling&#34;&gt;Rack Cabling&lt;/h2&gt;
&lt;p&gt;To keep the internal rack cabling tidy I decided to create each cable with custom length. The PoE standards require category 3 cable or better for 802.3af (upto 15watt) and category 5 cable for better for 802.3at (upto 30watt). The Raspberry Pi NIC can operate at 1Gb/s, category 5 is cable rated for 100Mb/s, category 5e is cable rated for 1Gb/s, category 6 is cable rated for 10Gb/s. To operate the Pi&amp;rsquo;s over PoE at full potential I use category 5e cable and crimped RJ45 connectors.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/custom_cables.jpeg&#34; alt=&#34;Custom cable lengths&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;finished-rack&#34;&gt;Finished Rack&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/rack_full.jpeg&#34; alt=&#34;Pi Rack Full&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/rack_side.jpeg&#34; alt=&#34;Pi Rack Side&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;hardware-bill-of-materials&#34;&gt;Hardware Bill of materials&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;35mm M6 Standoffs (Bag 50) £30&lt;/li&gt;
&lt;li&gt;10mm M6 screws (Bag 100) £6.45&lt;/li&gt;
&lt;li&gt;M6 hexagonal nuts (Bag 250) £6&lt;/li&gt;
&lt;li&gt;5mm M2.5 Standoffs (Bag 20) £5&lt;/li&gt;
&lt;li&gt;6mm M2.5 screws (Bag 100) £3.50&lt;/li&gt;
&lt;li&gt;Custom laser cut aluminium plates £60&lt;/li&gt;
&lt;li&gt;&amp;lt;1m Cat5e cable&lt;/li&gt;
&lt;li&gt;RJ45 cable crimping tool kit £20&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Total rack parts cost ~£130&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4x Raspberry Pi 4B WITH 8GB RAM (4X £73.50=) £294&lt;/li&gt;
&lt;li&gt;4x Raspberry Pi PoE HAT (4x £18=) £72&lt;/li&gt;
&lt;li&gt;4x 128GB SanDisk Extreme (4x £24=) £96&lt;/li&gt;
&lt;li&gt;4x Arcanite 128GB USB 3.1 Flash Drive (4x £20=) £80&lt;/li&gt;
&lt;li&gt;Netgear GS305P £45&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Total cluster parts cost ~£717&lt;/p&gt;

        
      </description>
    </item>
    
  </channel>
</rss>
