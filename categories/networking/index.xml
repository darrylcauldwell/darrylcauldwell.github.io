<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>networking on </title>
    <link>https://darrylcauldwell.com/categories/networking/</link>
    <description>Recent content in networking on </description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Wed, 12 Aug 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://darrylcauldwell.com/categories/networking/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Avi Load Balancer</title>
      <link>https://darrylcauldwell.com/post/avi/</link>
      <pubDate>Wed, 12 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/avi/</guid>
      <description>
        
          &lt;p&gt;Here we will explore an example of how an application can be secured using NSX Advanced Load Balancer. The application is deployed to Cloud Foundation on-premises and is extended to other geographic regions using both Amazon Web Services and Azure public clouds. The application is two tier, Ant Media servers provide a user facing presentation tier and MooseFS servers provide storage.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/avi-1.jpg&#34; alt=&#34;Multi-Cloud Ant Media&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;what-is-a-layer-7-load-balancer&#34;&gt;What is a layer 7 load balancer?&lt;/h1&gt;
&lt;p&gt;Traditional load balancers operate at layer 4 and offers traffic management of transactions at the network protocol layer (TCP/UDP). A layer 4 load balancer distributes traffic to pool members with a load balancing algorithm (i.e. round-robin) and by calculating the best server based on fewest connections and fastest server response time.&lt;/p&gt;
&lt;p&gt;The Avi Load Balancer operates at layer 7 which means you can make routing decisions on various characteristics of the HTTP/HTTPS header, the content of the message, the URL type, and information in cookies. A device that performs Layer 7 load balancing is sometimes referred to as a reverse proxy server or in Kubernetes terminology an ingress controller.&lt;/p&gt;
&lt;h1 id=&#34;avi-load-balancer-architecture&#34;&gt;Avi Load Balancer Architecture&lt;/h1&gt;
&lt;p&gt;The Avi Load Balancer (ALB) uses a software-defined architecture that separates the central control plane (Controller) from the distributed data plane (Service Engines).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/avi-2.jpg&#34; alt=&#34;Avi Load Balancer Architecture&#34;&gt;&lt;/p&gt;
&lt;p&gt;The Controller acts as a single point of intelligence, management, and control for the data plane. This control plane can be self-hosted or consumed as managed software-as-a-service (SaaS). When deploying the software-as-a-service architecture model the data plane remains deployed in each environment.&lt;/p&gt;
&lt;p&gt;The Service Engines represent the data-plane and provide full-featured, enterprise-grade load balancers, WAF, or analytics that manage and secure application traffic, and collect real-time telemetry from the traffic flows. Virtual Services are deployed to the data-plane which provide virtual IP address to a pool of application servers. Pools maintain the list of servers assigned to them and perform health monitoring, load balancing, persistence, web application firewall and SSL offload. A typical virtual service will point to one pool; however, more advanced configurations may have a virtual service content switching across multiple pools.&lt;/p&gt;
&lt;p&gt;The Controllers need to continually exchange information securely with Service Engines. The Service Engines hold communications channel with controller over TCP port 22 (SSH), 8443 (HTTPS) and UDP port 123 (NTP).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/avi-3.jpg&#34; alt=&#34;Service Engine to Cntroller Communications&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;web-application-firewall&#34;&gt;Web Application Firewall&lt;/h1&gt;
&lt;p&gt;A web application firewall (or WAF) filters, monitors, and blocks HTTP traffic to and from a web application. A network firewall filters traffic at Layer 4 a web application firewall filters traffic at Layer 7. A web application firewall is a type of reverse-proxy, protecting the server from exposure by having clients pass through the WAF before reaching the server. The capability to inspecting HTTP traffic allows filtering rules which can prevent attacks stemming from web application security flaws, such as SQL injection, cross-site scripting (XSS), file inclusion, and security misconfigurations.&lt;/p&gt;
&lt;h1 id=&#34;avi-load-balancer-iwaf&#34;&gt;Avi Load Balancer iWAF&lt;/h1&gt;
&lt;p&gt;The Avi Load Balancer provides capability to configure Virtual Services with Intelligent Web Application Firewall (iWAF) capabilities.&lt;/p&gt;
&lt;p&gt;The WAF configuration is stored as profile and policy objects. These objects can be logically associated to one or many virtual services. Using this approach, an application specific profile and policy can be created and maintained which is logically to any Virtual Services in any cloud.&lt;/p&gt;
&lt;p&gt;The WAF can be set to either detect or enforce a ruleset. In detection mode the policy will evaluate the request and log request matching ruleset, in enforcement mode the policy will evaluate the request, block and log request matching ruleset. Paranoia mode can be set for each WAF policy which defines its rigidity. Specific rules are enabled or disabled based on the set paranoia mode.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/avi-4.jpg&#34; alt=&#34;Avi Load Balancer iWAF&#34;&gt;&lt;/p&gt;
&lt;p&gt;The iWAF comes with a ruleset which provides signatures to provide Core Rule Set (CRS) protection, support for compliance regulations such as PCI DSS, HIPAA, and GDPR.&lt;/p&gt;
&lt;h1 id=&#34;demonstrating-waf-ruleset&#34;&gt;Demonstrating WAF Ruleset&lt;/h1&gt;
&lt;p&gt;Core Rule Set protection primarily consists of regular expressions, and it decides for each request whether it is legitimate, an attack, or an information leak. There are various commercial and open source tools to test if WAFs are working. The testing tools can target many aspects of the WAF ruleset. It is possible to perform basic test using a web browser and look to target CRS rule 920350 ‘Host header is numeric IP address’. To be valid the Host header field must be sent in all HTTP/1.1 request messages. When the rule is enabled any traffic to IP address would be caught and only access via FQDN would be allowed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/avi-5.jpg&#34; alt=&#34;CRS rule 920350&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Within the default ‘System-WAF-Policy’ this signature is disabled by default. We cannot edit the default policy but if we duplicate it and name like ‘ANT-Media-WAF-Policy’ and change policy mode to Enforcement. Ensure the CRS 920350 signature check is disabled (default).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;With this policy applied to a Virtual Service we can direct browser to website via IP or FQDN and web page is displayed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When we update the policy and enable the CRS 920350 signature check requests via IP are blocked and web page can only be accessed via FQDN.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;demonstrating-waf-logging-and-exception&#34;&gt;Demonstrating WAF Logging and Exception&lt;/h1&gt;
&lt;p&gt;When a WAF policy is attached to a virtual service, specific WAF logs are generated. When WAF blocks traffic the request details get logged.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/avi-6.jpg&#34; alt=&#34;WAF Block Detected Log Graph&#34;&gt;&lt;/p&gt;
&lt;p&gt;It maybe that a rule is implemented which has unintended consequences and is blocking traffic we want to allow. Clicking on the + sign at the end of each log entry will expand the panel to provide more details. If we look through the extended details, we can see specific WAF signature which caused the log entry. There is also a button to add an WAF policy exception directly from the log view.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/avi-7.jpg&#34; alt=&#34;WAF Block Detected Log Detail&#34;&gt;&lt;/p&gt;
&lt;p&gt;This opens a wizard where we can confirm the details of the exception.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/avi-8.jpg&#34; alt=&#34;Add Exception&#34;&gt;&lt;/p&gt;
&lt;p&gt;Making an exception in this way alters the behaviour of policy immediately, in our example to allow access via IP address. In the policy itself the exception is clearly marked which can allow retrospective security change records to be created.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/avi-9.jpg&#34; alt=&#34;Exception Highlighted&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;lab-hybrid-cloud-networking&#34;&gt;Lab Hybrid Cloud Networking&lt;/h1&gt;
&lt;p&gt;The lab configuration used for testing was comprised of a Cloud Foundation on-premises cloud,  AWS and Azure. The Cloud Foundation deployment hosts an Ubuntu VM with SSL CPN client installed and is configured to maintain VPN to both AWS and Azure. Static routing is configured, and Virtual Machines and Virtual Services on the Cloud Foundation network can communicate with Virtual Machines and Virtual Services in both public clouds.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/avi-10.jpg&#34; alt=&#34;Multi-Cloud Site-to-Site VPNs&#34;&gt;&lt;/p&gt;
&lt;p&gt;Multi-cloud and hybrid cloud are closely related but are not the same. A multi-cloud approach would be hosting some services on-premises and other services in public cloud and the services operate independently. A hybrid-cloud approach would be hosting services across multiple clouds but having them integrated, communicating and exchanges data with each other. When integrating services applying consistent configuration is key, integrating services across multiple clouds is challenging.&lt;/p&gt;
&lt;h1 id=&#34;demonstrating-hybrid-cloud&#34;&gt;Demonstrating Hybrid Cloud&lt;/h1&gt;
&lt;p&gt;Another use case we can explorer is where an on-premises application is made available in its local geography but also made available in other geographies. Provide application proxy services adjacent to clients to improve performance and security.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/avi-11.jpg&#34; alt=&#34;Edge Proxy&#34;&gt;&lt;/p&gt;
&lt;p&gt;For this scenario we can deploy Virtual Service on-premises which has business application as pool members. We can deploy additional Virtual Services in public cloud which have on-premises Virtual Service as pool member. The network routing across private and public clouds is all within VPN tunnel.&lt;/p&gt;
&lt;p&gt;The separation of NSX Advanced Load Balancer control and data plane allows us to manage the Virtual Services and Web Application Firewalls deployed to multiple clouds as a hybrid-cloud. The WAF profile and policy are defined as logical objects within the control plane. These objects then have logical association to Virtual Services deployed to private and public cloud data planes. We can apply the same policy as in previous test to the Virtual Services across clouds and test the WAF rules function exactly the same.&lt;/p&gt;
&lt;h1 id=&#34;application-latency-analysis&#34;&gt;Application Latency Analysis&lt;/h1&gt;
&lt;p&gt;When operating web applications, it is essential to understand where network latency is introduced to the user experience. Out of the box when a Virtual Service is deployed you can a very easy to understand view of end-to-end latency.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/avi-12.jpg&#34; alt=&#34;Application Latency Analysis&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;traffic-capture&#34;&gt;Traffic Capture&lt;/h1&gt;
&lt;p&gt;When operating web applications, it good to understand the traffic flowing towards it. Typically to achieve this packet capture is required to be turned on at network side,  for whole VM or specific VM NIC. For NSX Advanced Load Balancer any Virtual Service can have packet capture turned on and packets captured as they flow through the Virtual Service which can be used to analysed to get a security insight.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/avi-13.jpg&#34; alt=&#34;Traffic Capture&#34;&gt;&lt;/p&gt;
&lt;p&gt;The packets are captured in PCAP Next Generation file format ( *.pcapng files ) so any analyser can be used to open and view.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/avi-14.jpg&#34; alt=&#34;Traffic Capture Analysis&#34;&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Introduction To Kubernetes Cluster Networking with Antrea</title>
      <link>https://darrylcauldwell.com/post/antrea/</link>
      <pubDate>Wed, 12 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/antrea/</guid>
      <description>
        
          &lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/antrea-logo.png&#34; alt=&#34;Antrea Logo&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-is-antrea&#34;&gt;What is Antrea&lt;/h2&gt;
&lt;p&gt;Antrea is Container Network Interface (CNI) plugin for Kubernetes. The Antrea CNI leverages Open vSwitch (OVS) to provides an overlay network and security services for a Kubernetes cluster. The overlay encapsulation uses Virtual Extensible LAN (VXLAN) by default, although Generic Network Virtualization Encapsulation (GENEVE), Generic Routing Encapsulation (GRE) or Stateless Transport Tunneling (STT) encapsulation can be configured. Antrea is an open source project hosted on GitHub &lt;a href=&#34;https://github.com/vmware-tanzu/antrea&#34;&gt;here&lt;/a&gt;. Using OVS also introduces other standard network management capabilities such NetFlow, sFlow, IP Flow Information Export (IPFIX) and Remote SPAN (RSPAN).&lt;/p&gt;
&lt;h2 id=&#34;architecture&#34;&gt;Architecture&lt;/h2&gt;
&lt;p&gt;Antrea is composed of several components deployed as Pods in the kube-system namespace of the cluster. The Antrea Conroller monitors the addition and deletion of objects by watching the Kubernetes API. A daemonset is deployed which druns Antrea Agent on each node the Agent applys flow configuration to Open vSwitch (OVS).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/antrea-architecture.png&#34; alt=&#34;Antrea Logo&#34;&gt;&lt;/p&gt;
&lt;p&gt;The network configration required by Kubernetes is configured in a collection of Open vSwitch (OVS) flow tables. To make it easy to interpret each type of network traffic is classified and each traffic classification is stored in its own flow table.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/antrea-ovs-pipeline.svg&#34; alt=&#34;OVS Pipeline&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;creating-a-simple-antrea-lab&#34;&gt;Creating A Simple Antrea Lab&lt;/h2&gt;
&lt;p&gt;We can create a lab to look at Antrea anywhere we can run some VMs so on either a public cloud or a homelab. Create three Ubuntu 19.10 VMs with 2x CPU, 4GB RAM and 50GB vHDD, use Netplan to configure NIC to static IP addressing like.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat /etc/netplan/01-netcfg.yaml 

network:
  version: 2
  renderer: networkd
  ethernets:
    ens160:
      addresses: 
      - 192.168.1.27/24
      gateway4: 192.168.1.254
      nameservers:
          search:
          - darrylcauldwell.com
          addresses: 
          - 192.168.1.10
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Antrea requires Kubernetes 1.16 or later. Install Docker, Open vSwitch and Kubernetes by running the following.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt update -y
sudo apt upgrade -y
sudo apt install docker.io python apt-transport-https openvswitch-switch -y
sudo gpasswd -a $USER docker
sudo systemctl start docker
sudo systemctl enable docker
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
sudo apt-add-repository &amp;quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&amp;quot;
sudo apt-get update
sudo swapoff -a 
sudo sed -i &#39;/ swap / s/^\(.*\)$/#\1/g&#39; /etc/fstab
sudo apt-get install -y kubelet=1.16.4-00 kubeadm=1.16.4-00 kubectl=1.16.4-00
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To form the overlay network Antrea requires an IP address block to allocate IP addresses (&amp;ndash;pod-network-cidr). We can use kubeadm to configure the IP address block and bootstrap the cluster by running the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo kubeadm init --pod-network-cidr=172.16.0.0/16
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In order to easily run kubectl from the master we can copy the kube config to our user profile by running the following.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With the control plane initiatlised  we can now get the cluster token from the master we can use this to add the two worker Nodes to the cluster.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo kubeadm join 192.168.1.27:6443 --token 4bp2f0.ppfjk7mfee89j2kd \
&amp;gt;     --discovery-token-ca-cert-hash sha256:5e79610f28840c15be46895340c4a5535b9a0d003741ed657961891a05987ccd 
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;All the Antrea components are containerized and can be installed using the Kubernetes deployment manifest. We can apply the default Antrea manifest supplied in GitHub repository which will deploy all necessary components.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl apply -f https://raw.githubusercontent.com/vmware-tanzu/antrea/master/build/yamls/antrea.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The Antrea components are deployed to the kube-system Namespace. The components include Deploymentm, Pods, Daemonset and ConfigMap.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get all --namespace kube-system
kubectl get configmap --namespace kube-system
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The ConfigMap defines antrea-agent.conf this is used by Daemonset on each Node to configure Open vSwitch for use with CNI we can see some key information like.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl describe configmap antrea-config-tm7bht9mg6

Integration Bridge name (default: br-int)
DataPath type (default: system)
Interface name to communicate with host (default: gw0)
Tunnel (Encapsulation) type (default: vxlan)
MTU value (default: 1450)
Service CIDR (default 10.96.0.0/12)
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;exploring-the-open-vswitch-overlay-network&#34;&gt;Exploring The Open vSwitch Overlay Network&lt;/h2&gt;
&lt;p&gt;With Antrea in place we can look at exploring the overlay network it has put in place. We create a Kubernetes Namespace and deploy an simple stateless application like &lt;a href=&#34;https://kubernetes.io/docs/tutorials/stateless-application/guestbook/&#34;&gt;guestbook&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
kubectl config set-context --current --namespace=development
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-deployment.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-service.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-deployment.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-service.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-deployment.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-service.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When running we can view the Pods IP addressing.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get pods --output wide

NAME                            READY   STATUS    RESTARTS   AGE     IP           NODE              NOMINATED NODE   READINESS GATES
frontend-6cb7f8bd65-25qv4       1/1     Running   0          5m47s   172.16.2.4   antrea-worker-2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
frontend-6cb7f8bd65-mn5jk       1/1     Running   0          5m47s   172.16.1.5   antrea-worker-1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
frontend-6cb7f8bd65-nr9zk       1/1     Running   0          5m47s   172.16.2.5   antrea-worker-2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
redis-master-7db7f6579f-4wdnj   1/1     Running   0          5m51s   172.16.2.2   antrea-worker-2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
redis-slave-7664787fbc-5xrvj    1/1     Running   0          5m49s   172.16.1.4   antrea-worker-1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
redis-slave-7664787fbc-nj6m5    1/1     Running   0          5m49s   172.16.2.3   antrea-worker-2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can see that IPs from 172.16.1.0/24 are issued to Pods running on Node antrea-worker-1 and 172.16.2.0/24 are issued to Pods running on Node antrea-worker-2. To facilitate communications between Pods the antrea-agent configures flows on Open vSwitch on each Node. If we connect to a Antrea Agent container we can see that an OVS bridge is created called br-int and the bridge has a vxlan tunnel port called tun0 and a gateway port called gw0. The internal port gw0 is allocated the role of gateway of the Node&amp;rsquo;s subnet and is allocated the first IP address in subnet.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl config set-context --current --namespace=kube-system
kubectl get pods --selector=component=antrea-agent --output wide

NAME                 READY   STATUS    RESTARTS   AGE   IP             NODE              NOMINATED NODE   READINESS GATES
antrea-agent-czksb   2/2     Running   0          14m   192.168.1.28   antrea-worker-1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
antrea-agent-gwkmr   2/2     Running   0          14m   192.168.1.27   antrea-master     &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
antrea-agent-x9xjk   2/2     Running   0          14m   192.168.1.29   antrea-worker-2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;

kubectl exec -it antrea-agent-czksb -c antrea-ovs bash

ovs-vsctl list-br
ovs-vsctl show

3cf39eec-f64c-49ed-ad86-48f203b215a4
    Bridge br-int
        Port &amp;quot;tun0&amp;quot;
            Interface &amp;quot;tun0&amp;quot;
                type: vxlan
                options: {key=flow, remote_ip=flow}
        Port &amp;quot;coredns--886217&amp;quot;
            Interface &amp;quot;coredns--886217&amp;quot;
        Port &amp;quot;redis-sl-b32512&amp;quot;
            Interface &amp;quot;redis-sl-b32512&amp;quot;
        Port &amp;quot;coredns--3d5851&amp;quot;
            Interface &amp;quot;coredns--3d5851&amp;quot;
        Port &amp;quot;gw0&amp;quot;
            Interface &amp;quot;gw0&amp;quot;
                type: internal
        Port &amp;quot;frontend-1ee3a9&amp;quot;
            Interface &amp;quot;frontend-1ee3a9&amp;quot;
    ovs_version: &amp;quot;2.11.1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can also see that ArpResponderTable (20) and L3ForwardingTable (70) have flow records relating to the pod network.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl config set-context --current --namespace=kube-system
kubectl exec -it antrea-agent-czksb -c antrea-ovs bash
ovs-ofctl dump-flows br-int | grep 172.16.2

 cookie=0xf70e020000000000, duration=820.733s, table=20, n_packets=1, n_bytes=42, idle_age=585, priority=200,arp,arp_tpa=172.16.2.1,arp_op=1 actions=move:NXM_OF_ETH_SRC[]-&amp;gt;NXM_OF_ETH_DST[],mod_dl_src:aa:bb:cc:dd:ee:ff,load:0x2-&amp;gt;NXM_OF_ARP_OP[],move:NXM_NX_ARP_SHA[]-&amp;gt;NXM_NX_ARP_THA[],load:0xaabbccddeeff-&amp;gt;NXM_NX_ARP_SHA[],move:NXM_OF_ARP_SPA[]-&amp;gt;NXM_OF_ARP_TPA[],load:0xac100201-&amp;gt;NXM_OF_ARP_SPA[],IN_PORT

 cookie=0xf70e020000000000, duration=820.733s, table=70, n_packets=649, n_bytes=63996, idle_age=0, priority=200,ip,nw_dst=172.16.2.0/24 actions=dec_ttl,mod_dl_src:06:d1:bb:d3:bc:fa,mod_dl_dst:aa:bb:cc:dd:ee:ff,load:0x1-&amp;gt;NXM_NX_REG1[],load:0x1-&amp;gt;NXM_NX_REG0[16],load:0xc0a8011d-&amp;gt;NXM_NX_TUN_IPV4_DST[],resubmit(,105)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can look to test the IP routing and connectivity between Pods on same Node and also between Nodes,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl config set-context --current --namespace=development
kubectl get pods --output wide | grep frontend

frontend-6cb7f8bd65-25qv4       1/1     Running   0          12m   172.16.2.4   antrea-worker-2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
frontend-6cb7f8bd65-mn5jk       1/1     Running   0          12m   172.16.1.5   antrea-worker-1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
frontend-6cb7f8bd65-nr9zk       1/1     Running   0          12m   172.16.2.5   antrea-worker-2   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;

kubectl exec -it frontend-6cb7f8bd65-25qv4 -- ping -c 1 172.16.2.5
kubectl exec -it frontend-6cb7f8bd65-25qv4 -- ping -c 1 172.16.1.5
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;exploring-network-policy&#34;&gt;Exploring Network Policy&lt;/h2&gt;
&lt;p&gt;Antrea implements NetworkPolicy using OVS Flows. Flows are organized in tables, and they are applied on each node by the Antrea agent. Before applying a network policy, we can check flow table IngressDefault (100) and IngressRuleTable (90)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl config set-context --current --namespace=kube-system
kubectl get pods | grep antrea-agent

kubectl exec -it antrea-agent-czksb -c antrea-agent ovs-ofctl dump-flows br-int | grep table=100

cookie=0xcac6000000000000, duration=9103.902s, table=100, n_packets=102, n_bytes=9696, priority=0 actions=resubmit(,105)
&lt;/code&gt;&lt;/pre&gt;&lt;pre&gt;&lt;code&gt;kubectl exec -it antrea-agent-czksb -c antrea-agent ovs-ofctl dump-flows br-int | grep table=90

cookie=0xcac6000000000000, duration=4059.159s, table=90, n_packets=32493, n_bytes=6556290, priority=210,ct_state=-new+est,ip actions=resubmit(,105)

cookie=0xcac6000000000000, duration=4059.159s, table=90, n_packets=1634, n_bytes=122104, priority=210,ip,nw_src=172.16.1.1 actions=resubmit(,105)

cookie=0xcac6000000000000, duration=4059.159s, table=90, n_packets=37, n_bytes=3768, priority=0 actions=resubmit(,100)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can confirm that frontend can ping the backend.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl config set-context --current --namespace=development
kubectl get pods -o wide
kubectl exec -it frontend-6cb7f8bd65-25qv4 -- ping -c 1 172.16.2.27
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can create a network policy to deny all ingress.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt;/tmp/deny-all.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny
spec:
  podSelector: {}
  policyTypes:
  - Ingress
EOF

kubectl create -f /tmp/deny-all.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If we test again we can test and ensure that the policy is applied and the frontend can no longer ping backend.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl exec -it frontend-6cb7f8bd65-25qv4 -- ping -c 1 172.16.2.27
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can then check IngressDefault (100) flow table and see that our network policy has added action to drop.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl config set-context --current --namespace=kube-system

kubectl exec -it antrea-agent-czksb -c antrea-agent ovs-ofctl dump-flows br-int | grep table=100
 cookie=0xcac6000000000000, duration=3.427s, table=100, n_packets=0, n_bytes=0, priority=200,ip,reg1=0xa actions=drop

 cookie=0xcac6000000000000, duration=3.427s, table=100, n_packets=0, n_bytes=0, priority=200,ip,reg1=0x8 actions=drop

 cookie=0xcac6000000000000, duration=9226.746s, table=100, n_packets=137, n_bytes=12966, priority=0 actions=resubmit(,105)
&lt;/code&gt;&lt;/pre&gt;
        
      </description>
    </item>
    
    <item>
      <title>Introduction To Kubernetes Cluster Networking with NSX-T</title>
      <link>https://darrylcauldwell.com/post/k8s-nsxt/</link>
      <pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/k8s-nsxt/</guid>
      <description>
        
          &lt;p&gt;When developing a cloud native application using Docker containers she soon needs to understand how Docker containers communicate. In previous &lt;a href=&#34;https://darrylcauldwell.com/post/docker-networking&#34;&gt;post&lt;/a&gt; I looked at how Docker containers communicate on a single host. When the developer wants to scaleout capacity of the hosting across multiple hosts or increase abailability she might look at deploying this on a Kubernetes cluster. The move from single Docker host to multiple hosts managed as Kubernetes Cluster introduces changes to the container networking model. The four distinct networking problems a Kubernetes Cluster needs to address:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Highly-coupled container-to-container communications&lt;/li&gt;
&lt;li&gt;Pod-to-Pod communications&lt;/li&gt;
&lt;li&gt;Pod-to-Service communications&lt;/li&gt;
&lt;li&gt;External-to-Service communications: this is covered by services&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-is-a-kubernetes-pod&#34;&gt;What Is A Kubernetes Pod&lt;/h2&gt;
&lt;p&gt;A Docker container is great for deploying a single atomic unit of software. This model can become a bit cumbersome when you want to run multiple pieces of software together. You often see this when developers create Docker images that use &lt;a href=&#34;https://docs.docker.com/config/containers/multi-service_container/&#34;&gt;supervisord&lt;/a&gt; as an entrypoint to start and manage multiple processes. Many have found that it is instead more useful to deploy those applications in groups of containers that are partially isolated and partially share an environment. It is possible to configure Docker to control the level of sharing between groups of containers by creating a parent container and manage the lifetime of those containers, however this is administratively complex. Kubernetes provides an abstraction called Pods for just this use case.&lt;/p&gt;
&lt;p&gt;A Kubernetes Pod implements a &amp;lsquo;pause&amp;rsquo; container as the managing parent container, the Pod also contains one or more of  application containers. The &amp;lsquo;pause&amp;rsquo; container serves as the basis of Linux namespace sharing in the Pod the other containers are starterd within that namespace. Sharing a namespace includes sharing network stack and other resources such as volumes. Sharing a network namespace means containers within a Pod share an IP address and all containers within a Pod can all reach each other’s ports on localhost.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-pod.png&#34; alt=&#34;Kubernets Pod&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-is-a-kubernetes-service&#34;&gt;What Is A Kubernetes Service&lt;/h2&gt;
&lt;p&gt;Typically a Kubernetes Deployment is used to When a Kubernetes Pod is deployed Pods. A Deployment describes a desired state it can create and destroy Pods dynamically. While each Pod gets its own IP address, the set of Pods running can change. A Kubernetes Service (sometimes called a micro-service) is an abstraction which defines a logical set of Pods. The Kubernetes API provides Service discovery to Pods, it also offers a method of exposing Services via network port or load balancer to external systems.&lt;/p&gt;
&lt;h2 id=&#34;what-is-container-networking-interface-cni&#34;&gt;What Is Container Networking Interface (CNI)&lt;/h2&gt;
&lt;p&gt;Container-centric infrastructure needs a network and this network must be dynamic. Container networking is designed to be plugable, the Container Networking Interface is a defined &lt;a href=&#34;https://github.com/containernetworking/cni/blob/master/SPEC.md&#34;&gt;specification&lt;/a&gt;. Various open source projects and vendors provide CNI compliant plugins which provide dynamic networking solution for containers.&lt;/p&gt;
&lt;h2 id=&#34;what-is-nsx-t-container-plugin-ncp&#34;&gt;What Is NSX-T Container Plugin (NCP)&lt;/h2&gt;
&lt;p&gt;The NSX-T Container Plug-in (NCP) provides a CNI plugin and an integration with container orchestrators such as Kubernetes and OpenShift.&lt;/p&gt;
&lt;p&gt;NSX-T bring advanced features which can enrich Kubernetes cluster networking &lt;a href=&#34;https://blogs.vmware.com/networkvirtualization/2017/03/kubecon-2017.html/&#34;&gt;including&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fine-grained traffic control and monitoring&lt;/li&gt;
&lt;li&gt;Fine-grained security policy (firewall rules)&lt;/li&gt;
&lt;li&gt;Automated creation of network topology&lt;/li&gt;
&lt;li&gt;Integration with enterprise networking&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The main component of NCP runs in a container and communicates with NSX Manager and with the Kubernetes control plane. NCP monitors changes to containers and other resources and manages networking resources such as logical ports, switches, routers, and security groups for the containers by calling the NSX API.&lt;/p&gt;
&lt;h2 id=&#34;nsx-t-container-plug-in-release-25&#34;&gt;NSX-T Container Plug-in Release 2.5&lt;/h2&gt;
&lt;p&gt;I looked at the &lt;a href=&#34;http://darrylcauldwell.com/nsx-openshift/&#34;&gt;NCP initially for integration with OpenShift&lt;/a&gt; in mid-2018. I am revisiting this now to refresh my understanding and explore some of the new features introduced with the &lt;a href=&#34;https://blogs.vmware.com/networkvirtualization/2019/09/nsx-t-2-5-what-is-new-for-kubernetes.html/&#34;&gt;2.5 release&lt;/a&gt; including:&lt;/p&gt;
&lt;h3 id=&#34;policy-api-object-support&#34;&gt;Policy API Object Support&lt;/h3&gt;
&lt;p&gt;Prior to the 2.5 release all NSX objects which the NCP interacted with had to be created via the Advanced Networking &amp;amp; Security tab in the UI or the old imperative APIs. The imperative API was harder than it could have been to control programatically so with the NSX-T 2.4 release VMware introduced a new intent-based Policy API and corresponding Simplified UI. The NCP now supports either the imperative or the intent-based API,  to use the intent-based API a new parameter in the NCP configmap (ncp.ini) policy_nsxapi needs to be set to True.&lt;/p&gt;
&lt;h3 id=&#34;simplified-installation&#34;&gt;Simplified Installation&lt;/h3&gt;
&lt;p&gt;Another change I am interested in exploring is the simplified installation. In the past, an admin had to login to every k8s node and perform multiple steps to bootstrap it. She had to install the NSX CNI Plug-in and OpenVSwitch, to create OVS bridge and to add one vNic to the bridge. The 2.5 release introduces a second DaemonSet nsx-ncp-bootstrap Pod this now handles the deployment and lifecycle management of these components and we don’t need to login to every node. This should make it easier to scale out a cluster with additional nodes.&lt;/p&gt;
&lt;h2 id=&#34;lab-hardware-configuration&#34;&gt;Lab Hardware Configuration&lt;/h2&gt;
&lt;p&gt;To explore Kubernetes networking and the NCP I am using my homelab. My homelab has a very simple physical network namely a single subnet (192.168.1.0/24) with DHCP enabled and which has default route to the internet. Connected to the physical network are three Intel NUCs each has two 1G NIC an onboard and an additional 1G USB3 NIC.&lt;/p&gt;
&lt;h2 id=&#34;lab-vsphere-configuration&#34;&gt;Lab vSphere Configuration&lt;/h2&gt;
&lt;p&gt;The hosts run vSphere 6.7 Update 3 and have the onboard NIC configure as a vSphere Standard Switch hosting Management and vSAN VMkernels and the USB3 NIC is unused. The hosts are added to a vCenter appliance (192.168.1.13) and formed into a VSAN enabled cluster. The cluster also hosts a Windows 2019 Server VM running Active Directory (192.168.1.10) this also acts as DNS server and NTP source for lab.&lt;/p&gt;
&lt;h2 id=&#34;lab-nsx-t-configuration&#34;&gt;Lab NSX-T Configuration&lt;/h2&gt;
&lt;p&gt;An extra-small NSX Manager appliance (192.168.1.14) is deployed.  All esxi hosts are configured as Transport Nodes using vusb0 interface to Transport Zone named &amp;lsquo;overlayTransport&amp;rsquo;. A medium sized NSX Edge called &amp;lsquo;nsxEdge&amp;rsquo; is deployed which is a member of &amp;lsquo;overlayTransport&amp;rsquo; and &amp;lsquo;vlanTransport&amp;rsquo; Transport Zones. A Edge Cluster named &amp;lsquo;edgeCluster&amp;rsquo; and add &amp;lsquo;nsxEdge&amp;rsquo; is a member.&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-ip-address-space&#34;&gt;Kubernetes IP Address Space&lt;/h2&gt;
&lt;p&gt;A Kubernetes Cluster requires all Pods on a node to communicate with all Pods on all nodes in the cluster without NAT these are refered to as CluterIP. To support this a range of IP addresses must be defined to be issued to Pods and Services within a cluster. Even though the range is used for both Pods and Services, it is called the Pod address range. The last /20 of the Pod address range is used for Services. A /20 range has 212 = 4096 addresses. So 4096 addresses are used for Services, and the rest of the range is used for Pods.&lt;/p&gt;
&lt;p&gt;The address range I will be using to issue ClusterIP for this lab cluster is 10.0.0.0/16.&lt;/p&gt;
&lt;p&gt;As well as internal communicatins using ClusterIP some parts of your application may need to be exposed as a Service to be accessible on an externally routable IP address. There are two methods for exposing Service onto an externally routable IP address, NodePort and LoadBalancer. Source NAT is used for translating private ClusterIP address to a public routable address.&lt;/p&gt;
&lt;p&gt;The external address range I will be using to issue ExternalIP for this lab cluster is 172.16.0.0/16.&lt;/p&gt;
&lt;h2 id=&#34;ncp---nsx-object-identification&#34;&gt;NCP - NSX Object Identification&lt;/h2&gt;
&lt;p&gt;There can be many objects deployed within NSX-T, the NCP needs to understand which of these objects to interact with. The NSX objects which the NCP needs to interact with include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Overlay transport zone&lt;/li&gt;
&lt;li&gt;Tier-0 logical router&lt;/li&gt;
&lt;li&gt;Logical switch to connect the node VMs&lt;/li&gt;
&lt;li&gt;IP Block for internal ClusterIP addressing&lt;/li&gt;
&lt;li&gt;IP Pool for ExternalIP addressing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The NCP configuration is stored in ncp.ini file. It is possible to put the UUID of NSX objects in the ncp.ini but this an administrative pain. The mapping of which NSX objects for NCP to interact with is better achieved by applying tags to the appropriate NSX objects.&lt;/p&gt;
&lt;p&gt;An NSX-T instance can support multiple Kubernetes clusters to ensure correct object mapping a cluster name is used. The cluster name is specified in NCP configuration and the appopriate NSX objects must have tag of same name applied.&lt;/p&gt;
&lt;p&gt;For this lab environment I am configuring with cluster name &amp;lsquo;pandora&amp;rsquo;.&lt;/p&gt;
&lt;h2 id=&#34;deploy-and-configure-tier-0&#34;&gt;Deploy and Configure Tier-0&lt;/h2&gt;
&lt;p&gt;The NCP deploys application centric network topology at the top of that topology is a Tier-0 router which provides uplink to the physical network.&lt;/p&gt;
&lt;p&gt;Use the Policy UI to deploy a Tier-0 Logical Router with High-Availability mode Active-Passive. In order the NCP knows the correct Tier-0 Logical Router a tag like this needs to be applied:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Tag&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Scope&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;pandora&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ncp/cluster&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;When applied it should look like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-tier0.png&#34; alt=&#34;NSX-T Tier-0 Router&#34;&gt;&lt;/p&gt;
&lt;p&gt;To communicate with the physical network the Tier-0 requires an uplink IP address.  In order to add uplink IP address we require creating a network segment backed by VLAN.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-physical-segment.png&#34; alt=&#34;VLAN Backed Segment&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can now configure an interface on the Tier-0 with IP address 192.168.1.17.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-tier0-uplink.png&#34; alt=&#34;NSX-T Tier-0 Uplink&#34;&gt;&lt;/p&gt;
&lt;p&gt;To enable ExternalIP routing from physical network add a static route to physical router directing 172.16.0.0/16 to 192.168.1.17.&lt;/p&gt;
&lt;h3 id=&#34;nsx-ip-block-for-internal-clusterip&#34;&gt;NSX IP Block for internal ClusterIP&lt;/h3&gt;
&lt;p&gt;NSX-T has an inbuilt capability for IP Management, in which we can allocate blocks of IP Addresses and create IP Pools.&lt;/p&gt;
&lt;p&gt;The NCP requires an IP Block for issuing internal ClusterIP. Create the 10.0.0.0/16 IP address block named &amp;lsquo;k8s-1-internal&amp;rsquo; with tags applied like this:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Tag&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Scope&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;pandora&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ncp/no_snat&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pandora&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ncp/cluster&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;When applied it should look like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-internal-block.png&#34; alt=&#34;Internal Block&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;nsx-ip-pool-for-externalip&#34;&gt;NSX IP Pool for ExternalIP&lt;/h2&gt;
&lt;p&gt;The NCP requires an IP Pool for issuing internal ExternalIP. First create an IP Block named &amp;lsquo;k8s-1-external&amp;rsquo; with CIDR 172.16.0.0/16. This IP Block is not accessed directly by NCP so does not need any tags.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-external-block.png&#34; alt=&#34;External Block&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once the IP Block is in place create an IP Pool named &amp;lsquo;k8s-1-loadbalancer&amp;rsquo; which has a subnet issued from IP Block &amp;lsquo;k8s-1-external&amp;rsquo; which is sized at 128 the IP Pool.  The External IP Pool can be shared but should at least have tag applied like this:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Tag&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Scope&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ncp/external&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;When applied it should look like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-external-pool.png&#34; alt=&#34;External Block Tag&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;node-logical-switch&#34;&gt;Node Logical Switch&lt;/h2&gt;
&lt;p&gt;Network connectivity to the containers running in Kubernetes is provided by a NSX-T logical switch segment which is often referred to as the node logical switch. For this create a new segment called &amp;lsquo;node-logical-switch&amp;rsquo; within the &amp;lsquo;overlayTransportZone&amp;rsquo; connected to no gateway.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-node-logical-switch.png&#34; alt=&#34;Node Logical Switch&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-master-vm&#34;&gt;Kubernetes Master VM&lt;/h2&gt;
&lt;p&gt;Create a Ubuntu 18.04 VM with 2x CPU, 4GB RAM and 50GB vHDD named &amp;lsquo;k8s-master&amp;rsquo;. The VM should have two vNIC one which is used to communicate with NSX API and which will host Kubernetes API. The second connected to the node logical switch which will have the Open vSwitch (OVS) bridge configured to give connectivity to the Pods. In my lab first connected to &amp;lsquo;VM Network&amp;rsquo; enumerates as ens160 and the scond connected to &amp;lsquo;node-logical-switch&amp;rsquo; enumerates as ens192.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat /etc/netplan/01-netcfg.yaml 

network:
  version: 2
  renderer: networkd
  ethernets:
    ens160:
      addresses: 
      - 192.168.1.27/24
      gateway4: 192.168.1.254
      nameservers:
          search:
          - darrylcauldwell.com
          addresses: 
          - 192.168.1.10
    ens192: {}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In order the NCP know which vNIC to configure the VM vNIC connected &amp;lsquo;node-logical-switch&amp;rsquo; creates a segment port object in NSX-T this port must have tag applied like this:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Tag&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Scope&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;k8s-master&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ncp/node_name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pandora&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ncp/cluster&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;When applied it should look like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-segment-port-tags.png&#34; alt=&#34;Segment Port Tags&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;install-and-configure-kubernetes&#34;&gt;Install and Configure Kubernetes&lt;/h2&gt;
&lt;p&gt;Docker and Kubernetes require installation do this by running the following.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt update -y
sudo apt upgrade -y
sudo apt install docker.io python apt-transport-https -y
sudo gpasswd -a $USER docker
sudo systemctl start docker
sudo systemctl enable docker
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
sudo apt-add-repository &amp;quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&amp;quot;
sudo apt-get update
sudo swapoff -a 
sudo sed -i &#39;/ swap / s/^\(.*\)$/#\1/g&#39; /etc/fstab
sudo apt-get install -y kubelet=1.16.4-00 kubeadm=1.16.4-00 kubectl=1.16.4-00
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Initialize the Kubernetes cluster by running the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo kubeadm init
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In order to easily run kubectl as a user we need to copy the cluster configuration to the user profile, do this by running the following.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can now connect to cluster and check the state of the nodes, do this by running the following on the Master node.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-init-get-nodes.png&#34; alt=&#34;Init Get Nodes&#34;&gt;&lt;/p&gt;
&lt;p&gt;The status of each Node will show &amp;lsquo;NotReady&amp;rsquo;,  we can get more details of why it is in this state by running the following on the Master node.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl describe nodes k8s-master | grep Conditions -A9
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-init-describe-node.png&#34; alt=&#34;Init Describe Nodes&#34;&gt;&lt;/p&gt;
&lt;p&gt;With this we can see this reason for the error is &amp;lsquo;NetworkPluginNotReady&amp;rsquo; that the cni plugin is not initiated.&lt;/p&gt;
&lt;h2 id=&#34;install-and-configure-nsx-container-plug-in-ncp&#34;&gt;Install and Configure NSX Container Plug-in (NCP)&lt;/h2&gt;
&lt;p&gt;The NSX Container Plug-in (NCP) provides integration between NSX-T and Kubernetes, it is a containerised application which manages communicates between NSX Manager and the Kubernetes control plane. The NSX Container Plug-in (NCP) application runs in Kubernetes it is supplied as .zip download. The configuration of the NCP applications is maintained in a Kubernetes manifest file.&lt;/p&gt;
&lt;p&gt;The NCP application ships a container image file we could deploy this to a container registry but here we will just upload the image file to VM and import the image to the local docker repository. The default container image name specified in the manifest is nsx-ncp so we can apply that as tag on the image.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo docker load -i /home/ubuntu/nsx-ncp-ubuntu-2.5.1.15287458.tar
sudo docker image tag registry.local/2.5.1.15287458/nsx-ncp-ubuntu nsx-ncp
sudo docker images | grep ncp
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Prior to the 2.5 release there were multiple  manifest files each targetting a specific area of configuration.  These are now merged into a single manifest file with multiple sections with multiple resource specifications the sections can be identified by the separator &lt;code&gt;---&lt;/code&gt;. The resources are created in the order they appear in the file.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Resource Kind&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Resource Name&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Comments&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CustomResourceDefinition&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;nsxerrors.nsx.vmware.com&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CustomResourceDefinition&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;nsxlocks.nsx.vmware.com&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Namespace&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;nsx-system&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ServiceAccount&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ncp-svc-account&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ClusterRole&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ncp-cluster-role&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ClusterRole&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ncp-patch-role&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ClusterRoleBinding&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ncp-cluster-role-binding&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ClusterRoleBinding&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ncp-patch-role-binding&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ServiceAccount&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;nsx-node-agent-svc-account&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ClusterRole&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;nsx-node-agent-cluster-role&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ClusterRoleBinding&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;nsx-node-agent-cluster-role-binding&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ConfigMap&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;nsx-ncp-config&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Must update Kubernetes API and NSX API parameters&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Deployment&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;nsx-ncp&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ConfigMap&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;nsx-node-agent-config&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Must update Kubernetes API and NSX API parameters&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DaemonSet&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;nsx-ncp-bootstrap&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DaemonSet&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;nsx-node-agent&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For this lab I will use mostly default configuration from the supplied manifest file template. The settings I change from what is in template are the environment specifics such as details for connecting to NSX API including whether to use imperative API or intent-based API, the &amp;lsquo;cluster&amp;rsquo; name and the Node vNIC on which the OVS bridge gets created on.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[nsx_v3]
policy_nsxapi = True

nsx_api_managers = 192.168.1.14
nsx_api_user = admin
nsx_api_password = VMware1!

insecure = True

[coe]
cluster = pandora

[k8s]
apiserver_host_ip = 192.168.1.27
apiserver_host_port = 6443

[nsx_kube_proxy]
ovs_uplink_port = ens192
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once the manifest file is updated and docker image in local registry we can apply the NCP manifest. Applying the manifest takes a couple of minutes while as it creates various Pods we can watch their creation to view progress.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl apply --filename /home/ubuntu/ncp-ubuntu.yaml
kubectl get pods --output wide --namespace nsx-system --watch
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-deploy-ncp.png&#34; alt=&#34;Deploy NCP&#34;&gt;&lt;/p&gt;
&lt;p&gt;If all has gone well we can take a look at the objects created within the namespace.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get all --namespace nsx-system
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-deployed-ncp.png&#34; alt=&#34;Deployed NCP&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now its running we can check health of the NCP, the NCP has a &lt;a href=&#34;https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.5/ncp-kubernetes/GUID-EA8E6CEE-36F4-423C-AD1E-DD6421A5FB1C.html&#34;&gt;CLI&lt;/a&gt; where we can run various commands including health checks.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl exec -it nsx-ncp-6978b9cb69-dj4k2 --namespace nsx-system -- /bin/bash 
nsxcli
get ncp-k8s-api-server status
get ncp-nsx status
exit
exit
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-ncp-health.png&#34; alt=&#34;NCP Health&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;nsx-policy-ui-object-creation&#34;&gt;NSX Policy UI Object Creation&lt;/h2&gt;
&lt;p&gt;With NSX integration when we deploy a Kubernetes namespace the NCP creates a corresponding segment, IP Pool (from internet 10.0.0.0/8 range), subnet and Tier-1 router.  If you open NSX Manager and view one of the object categories which should have objects created. Then create two Kubernetes namespaces one called development and one called production.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create --filename https://k8s.io/examples/admin/namespace-dev.json
kubectl create --filename https://k8s.io/examples/admin/namespace-prod.json
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you refresh view in NSX Manager you will see the new objects appear in the Policy UI.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-tier1s.png&#34; alt=&#34;NSX-T Tier-1&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can then remove these two namespaces the NCP removes the corresponding segment, IP Pool (from internet 10.0.0.0/8 range), subnet and Tier-1 router.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl delete -f https://k8s.io/examples/admin/namespace-dev.json
kubectl delete -f https://k8s.io/examples/admin/namespace-prod.json
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;ncp-bootstrap-scaleout-cluster&#34;&gt;NCP Bootstrap Scaleout Cluster&lt;/h2&gt;
&lt;p&gt;We started this lab with a single node cluster to look at the nsx-ncp-bootstrap Daemonset. When Nodes are added to the cluster this should install and configure the Node with NCP.&lt;/p&gt;
&lt;p&gt;Create a second and third VM with same hardware configuration as k8s-master but name these k8s-worker-1 / k8s-worker-2 and give IPs 192.168.1.28  / 192.168.1.29.  Ensure the NSX segment port attached to  VMs is tagged correctly. Ensure the NCP docker image is uploaded, imported to local docker registry and has tag applied.&lt;/p&gt;
&lt;p&gt;To add the additional Node to the cluster first step is to create a token on master.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm token create --print-join-command
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-token.png&#34; alt=&#34;Token&#34;&gt;&lt;/p&gt;
&lt;p&gt;We use the output of command from the master to add the additional worker nodes to the cluster. The nsx-ncp-bootstap and nsx-node-agent are DaemonSets this ensures that all Nodes run a copy of a Pod. When we add the worker nodes to the cluster we can see the nsx-ncp-bootstrap Pods initialize and configure the Node and the nsx-node-agent Pods initialize.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get pods -o wide --namespace nsx-system --watch
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-ncp-scale-out.png&#34; alt=&#34;NCP Scale Out&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;ip-assignments&#34;&gt;IP Assignments&lt;/h2&gt;
&lt;p&gt;When a Pod is deployed it can be exposed as a Service, the service&lt;/p&gt;
&lt;p&gt;If we deploy an simple stateless application example like &lt;a href=&#34;https://kubernetes.io/docs/tutorials/stateless-application/guestbook/&#34;&gt;guestbook&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
kubectl config set-context --current --namespace=development
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-deployment.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-service.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-deployment.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-service.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-deployment.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-service.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When we configures the lab we created NSX managed IP Block 10.0.0.0/16. When created the development namespace got allocated a /24 subnet from this block. If we view the Pods get IP in correct range.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get pods --output wide
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-internal-ip-allocation.png&#34; alt=&#34;Internal IP Allocation&#34;&gt;&lt;/p&gt;
&lt;p&gt;As well as Pod deployments the guestbook application installation also creates Service resources. A Service is an abstraction which defines a logical set of Pods. Our application runs three frontend pods and two Redis slaves the Service provides virtual IP. The kube-proxy is responsible for implementing the virtual IP for Services. We can see the ClusterIP are issued from the last /20 of the Pod address range. If we look in more detail at the Service resource we can see the three internal IP addresses behind it.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get services --namespace development
kubectl cluster-info dump | grep -m 1 service-cluster-ip-range
kubectl describe service frontend
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-clusterip.png&#34; alt=&#34;Cluster IP&#34;&gt;&lt;/p&gt;
&lt;p&gt;To facilitate communications between Pods the NCP is configuring the Open vSwitch on each Node. The Open vSwitch user space daemon runs on a container named nsx-ovs within the nsx-node-agent Pods. The Open vSwitch bridge name can be specified in the ncp.ini but defaults to br-int. If we connect to this container we can view the Open vSwitch flows .&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl exec -it nsx-node-agent-llf2c -c nsx-ovs bash --namespace nsx-system
ovs-ofctl dump-flows br-int
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-ovs-flows.png&#34; alt=&#34;OVS Flows&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can then remove the namespace and all objects within.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl delete -f https://k8s.io/examples/admin/namespace-dev.json
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;nsx-loadbalancer&#34;&gt;NSX Loadbalancer&lt;/h2&gt;
&lt;p&gt;NSX provides an load balancer capability to Kubernetes we can use this by creating service resource with type LoadBalancer.  If we create a simple replicaset of five pods and expose this we can see it gets issued with IP Address from the IP Pool tagged with ncp/external = True. We can also see this in NSX Loadbalancer configuration.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
kubectl config set-context --current --namespace=development
kubectl apply -f https://k8s.io/examples/service/load-balancer-example.yaml
kubectl get replicasets
kubectl expose deployment hello-world --type=LoadBalancer --name=hello-world-nsx-lb
kubectl get services hello-world-nsx-lb --watch

NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
hello-world-nsx-lb   LoadBalancer   10.110.64.115   &amp;lt;pending&amp;gt;     8080:32091/TCP   7s
hello-world-nsx-lb   LoadBalancer   10.110.64.115   172.16.0.13   8080:32091/TCP   7s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can then remove the namespace and all objects within.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl delete -f https://k8s.io/examples/admin/namespace-dev.json
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;kubernetes-pods-micro-segmentation&#34;&gt;Kubernetes Pods Micro-Segmentation&lt;/h2&gt;
&lt;p&gt;One of the great usecases for NSX with vSphere has been the is the Distributed Firewall which protects VM workload at the Hypervisor layer. When we extend NSX into the Kubernetes container we also extend the Distributed Firewall capability. We can create firewall rules which contain members of groups, the groups can be dynamically populated by use of NSX tags. Kubernetes objects can have labels attached a label attached to a Pod is reflected in NSX as a tag on the segement port of the Pod.&lt;/p&gt;
&lt;p&gt;A simple test might be to deploy a two tier app where frontend can talk to backend but frontend cannot talk to other frontend. If we create a NSX group called Web with membership criteria Segement Port, Tag, Equals web Scope secgroup. We then create a NSX DFW rule with Web as source and destination and action of drop.&lt;/p&gt;
&lt;p&gt;With this in place we can ping test from one frontend pod to another frontend and backend and see this works.  We can then apply the label to the three frontend web Pods so they become members of the NSX group and are affected by the firewall rule.  With these in place we can retest ping from one frontend pod to another frontend and see that this is now blocked.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
kubectl config set-context --current --namespace=development
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-deployment.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-deployment.yaml
kubectl get pods --output wide --watch

NAME                            READY   STATUS    RESTARTS   AGE   IP         NODE           NOMINATED NODE   READINESS GATES
frontend-6cb7f8bd65-4mctt       1/1     Running   0          13m   10.0.6.4   k8s-worker-1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
frontend-6cb7f8bd65-8wkhr       1/1     Running   0          13m   10.0.6.2   k8s-worker-1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
frontend-6cb7f8bd65-rtgc9       1/1     Running   0          13m   10.0.6.3   k8s-worker-1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
redis-master-7db7f6579f-zlx26   1/1     Running   0          3s    10.0.6.5   k8s-worker-1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;

kubectl exec -it frontend-6cb7f8bd65-4mctt ping 10.0.6.2
PING 10.0.6.2 (10.0.6.2): 56 data bytes
64 bytes from 10.0.6.2: icmp_seq=0 ttl=64 time=0.083 ms

kubectl exec -it frontend-6cb7f8bd65-4mctt ping 10.0.6.5
PING 10.0.6.5 (10.0.6.5): 56 data bytes
64 bytes from 10.0.6.5: icmp_seq=0 ttl=64 time=3.191 ms

kubectl label pod frontend-6cb7f8bd65-4mctt secgroup=web
kubectl label pod frontend-6cb7f8bd65-8wkhr secgroup=web
kubectl label pod frontend-6cb7f8bd65-rtgc9 secgroup=web

kubectl exec -it frontend-6cb7f8bd65-4mctt ping 10.0.6.2
PING 10.0.6.2 (10.0.6.2): 56 data bytes
^C--- 10.0.6.2 ping statistics ---
2 packets transmitted, 0 packets received, 100% packet loss
command terminated with exit code 1

kubectl exec -it frontend-6cb7f8bd65-4mctt ping 10.0.6.5
PING 10.0.6.5 (10.0.6.5): 56 data bytes
64 bytes from 10.0.6.5: icmp_seq=0 ttl=64 time=5.672 ms
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can then remove the namespace and all objects within.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl delete -f https://k8s.io/examples/admin/namespace-dev.json
&lt;/code&gt;&lt;/pre&gt;
        
      </description>
    </item>
    
    <item>
      <title>Introduction To Docker Container Networking</title>
      <link>https://darrylcauldwell.com/post/docker-networking/</link>
      <pubDate>Mon, 18 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/docker-networking/</guid>
      <description>
        
          &lt;p&gt;When developing a cloud native application using Docker containers understanding network connectivity between containers and non-Docker workloads such as other servers and cloud APIs is essential. Docker’s networking subsystem is pluggable using drivers, several drivers exist by default, and others can be added easily.&lt;/p&gt;
&lt;h2 id=&#34;linking-containers-legacy&#34;&gt;Linking Containers (legacy)&lt;/h2&gt;
&lt;p&gt;Linking containers by name is a simple method of enabling communications between containers. To link a an app server to a DB we simply reference the db container name in the command we execute to run app server container.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -d --name my_mongodb mongo
docker run -d --link my_mongodb --name my_nodeapp -it node
docker ps -l

    CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
    5dc2aa4d8542        node                &amp;quot;docker-entrypoint.s…&amp;quot;   7 seconds ago       Up 6 seconds                            my_nodeapp
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In order to facilitate communications Docker automatically creates environment variables in the target container. It also exposes all environment variables originating from Docker from the source container. If we view the IP address of the two containers and then view the environment variables of the app server we can see the details for the db server.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker inspect --format &#39;{{ .ID }} - {{ .Name }} - {{ .NetworkSettings.IPAddress }}&#39; my_mongodb
docker inspect --format &#39;{{ .ID }} - {{ .Name }} - {{ .NetworkSettings.IPAddress }}&#39; my_nodeapp

docker exec my_nodeapp env

    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
    HOSTNAME=a88e6e3f1107
    MY_MONGODB_PORT=tcp://172.17.0.2:27017
    MY_MONGODB_PORT_27017_TCP=tcp://172.17.0.2:27017
    MY_MONGODB_PORT_27017_TCP_ADDR=172.17.0.2
    MY_MONGODB_PORT_27017_TCP_PORT=27017
    MY_MONGODB_PORT_27017_TCP_PROTO=tcp
    MY_MONGODB_NAME=/my_nodeapp/my_mongodb
    MY_MONGODB_ENV_GOSU_VERSION=1.11
    MY_MONGODB_ENV_JSYAML_VERSION=3.13.0
    MY_MONGODB_ENV_GPG_KEYS=E162F504A20CDF15827F718D4B7C549A058F8B6B
    MY_MONGODB_ENV_MONGO_PACKAGE=mongodb-org
    MY_MONGODB_ENV_MONGO_REPO=repo.mongodb.org
    MY_MONGODB_ENV_MONGO_MAJOR=4.2
    MY_MONGODB_ENV_MONGO_VERSION=4.2.1
    NODE_VERSION=13.1.0
    YARN_VERSION=1.19.1
    HOME=/root
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Container linking is a legacy method of connecting containers it is much more likely today that communications on single host will use a bridge network.&lt;/p&gt;
&lt;h2 id=&#34;container-networking-with-a-bridge-driver&#34;&gt;Container Networking With A Bridge Driver&lt;/h2&gt;
&lt;p&gt;A bridge network can be used on Docker host to enable communications between containers. The first step is to create a custom bridge network on the Docker host which the containers will connect to. Each Docker network gets assigned a CIDR range and has a IPAM capability, we can view these by inspecting the network.&lt;/p&gt;
&lt;p&gt;The default bridge network is present on all Docker hosts, if you do not specify a different network, new containers are automatically connected to the default bridge network. More than likely you will want to separate containers to different networks and so will want to create multiple networks and attach different containers to each.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker network create --driver bridge my_isolated_network
docker network ls
docker network inspect my_isolated_network

    [
        {
            &amp;quot;Name&amp;quot;: &amp;quot;my_isolated_network&amp;quot;,
            &amp;quot;Id&amp;quot;: &amp;quot;76e440d68a136ffe1ab9a4dd4977a7dc0499167814de14bf4bedfaffb646197b&amp;quot;,
            &amp;quot;Created&amp;quot;: &amp;quot;2019-11-18T09:59:09.8310474Z&amp;quot;,
            &amp;quot;Scope&amp;quot;: &amp;quot;local&amp;quot;,
            &amp;quot;Driver&amp;quot;: &amp;quot;bridge&amp;quot;,
            &amp;quot;EnableIPv6&amp;quot;: false,
            &amp;quot;IPAM&amp;quot;: {
                &amp;quot;Driver&amp;quot;: &amp;quot;default&amp;quot;,
                &amp;quot;Options&amp;quot;: {},
                &amp;quot;Config&amp;quot;: [
                    {
                        &amp;quot;Subnet&amp;quot;: &amp;quot;172.18.0.0/16&amp;quot;,
                        &amp;quot;Gateway&amp;quot;: &amp;quot;172.18.0.1&amp;quot;
                    }
                ]
            },
            &amp;quot;Internal&amp;quot;: false,
            &amp;quot;Attachable&amp;quot;: false,
            &amp;quot;Ingress&amp;quot;: false,
            &amp;quot;ConfigFrom&amp;quot;: {
                &amp;quot;Network&amp;quot;: &amp;quot;&amp;quot;
            },
            &amp;quot;ConfigOnly&amp;quot;: false,
            &amp;quot;Containers&amp;quot;: {},
            &amp;quot;Options&amp;quot;: {},
            &amp;quot;Labels&amp;quot;: {}
        }
    ]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When networks are created you can start containers in those bridge networks.  The containers in same isolated network can then communicate by referencing name. When containers are running and connected we can inspect the network again and get the container IP address assignments etc.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -d --net=my_isolated_network --name my_mongodb mongo
docker run -d --net=my_isolated_network --name my_nodeapp -it node
docker network inspect my_isolated_network

    [
        {
            &amp;quot;Name&amp;quot;: &amp;quot;my_isolated_network&amp;quot;,
            &amp;quot;Id&amp;quot;: &amp;quot;76e440d68a136ffe1ab9a4dd4977a7dc0499167814de14bf4bedfaffb646197b&amp;quot;,
            &amp;quot;Created&amp;quot;: &amp;quot;2019-11-18T09:59:09.8310474Z&amp;quot;,
            &amp;quot;Scope&amp;quot;: &amp;quot;local&amp;quot;,
            &amp;quot;Driver&amp;quot;: &amp;quot;bridge&amp;quot;,
            &amp;quot;EnableIPv6&amp;quot;: false,
            &amp;quot;IPAM&amp;quot;: {
                &amp;quot;Driver&amp;quot;: &amp;quot;default&amp;quot;,
                &amp;quot;Options&amp;quot;: {},
                &amp;quot;Config&amp;quot;: [
                    {
                        &amp;quot;Subnet&amp;quot;: &amp;quot;172.18.0.0/16&amp;quot;,
                        &amp;quot;Gateway&amp;quot;: &amp;quot;172.18.0.1&amp;quot;
                    }
                ]
            },
            &amp;quot;Internal&amp;quot;: false,
            &amp;quot;Attachable&amp;quot;: false,
            &amp;quot;Ingress&amp;quot;: false,
            &amp;quot;ConfigFrom&amp;quot;: {
                &amp;quot;Network&amp;quot;: &amp;quot;&amp;quot;
            },
            &amp;quot;ConfigOnly&amp;quot;: false,
            &amp;quot;Containers&amp;quot;: {
                &amp;quot;996b590f206dded3db3a653e554167e046fe3852f55a78d6d983eaa0f7f7fc3d&amp;quot;: {
                    &amp;quot;Name&amp;quot;: &amp;quot;my_mongodb&amp;quot;,
                    &amp;quot;EndpointID&amp;quot;: &amp;quot;e78b23529f29b2a6f5291f06fb1875cac9958888ca0eb95cae814dc8b631613a&amp;quot;,
                    &amp;quot;MacAddress&amp;quot;: &amp;quot;02:42:ac:12:00:02&amp;quot;,
                    &amp;quot;IPv4Address&amp;quot;: &amp;quot;172.18.0.2/16&amp;quot;,
                    &amp;quot;IPv6Address&amp;quot;: &amp;quot;&amp;quot;
                },
                &amp;quot;eebe0a3bf7116eab4731ef68da445f24c15b185efc8d7cea20937736702b93f8&amp;quot;: {
                    &amp;quot;Name&amp;quot;: &amp;quot;my_nodeapp&amp;quot;,
                    &amp;quot;EndpointID&amp;quot;: &amp;quot;6903d08b5942c71b5afd09caf535d4abb818ded97742fe324d54bb6396b595a5&amp;quot;,
                    &amp;quot;MacAddress&amp;quot;: &amp;quot;02:42:ac:12:00:03&amp;quot;,
                    &amp;quot;IPv4Address&amp;quot;: &amp;quot;172.18.0.3/16&amp;quot;,
                    &amp;quot;IPv6Address&amp;quot;: &amp;quot;&amp;quot;
                }
            },
            &amp;quot;Options&amp;quot;: {},
            &amp;quot;Labels&amp;quot;: {}
        }
    ]

&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The docker daemon implements an embedded DNS server (127.0.0.11) which provides built-in service discovery for any container created with a valid name. The containers are able to use this by docker overlay three crucial /etc files inside the container with virtual files. This arrangement allows Docker to do clever things like keep resolv.conf up to date across all containers.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker exec my_nodeapp mount

    ...
    /dev/sda1 on /etc/resolv.conf type ext4 (rw,relatime,data=ordered)
    /dev/sda1 on /etc/hostname type ext4 (rw,relatime,data=ordered)
    /dev/sda1 on /etc/hosts type ext4 (rw,relatime,data=ordered)
    ...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If we view the DNS settings of one of the containers we see that it is set by default to 127.0.0.11.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker exec -it my_nodeapp cat /etc/resolv.conf

    nameserver 127.0.0.11
    options ndots:0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can therefore ping the database container by using its container name from the app server.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker exec -it my_nodeapp ping -c 1 my_mongodb

    PING my_mongodb (172.18.0.2) 56(84) bytes of data.
    64 bytes from my_mongodb.my_isolated_network (172.18.0.2): icmp_seq=1 ttl=64 time=0.177 ms

    --- my_mongodb ping statistics ---
    1 packets transmitted, 1 received, 0% packet loss, time 0ms
    rtt min/avg/max/mdev = 0.177/0.177/0.177/0.000 ms
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;It can be a relatively common requirement for me to have different container names for same application function in different environments e.g. dev_mongodb, qa_mongodb, prod_mongodb. While I could configuring connection strings in the app server it might be more useful to give the database a connection alias e.g. mongodb. Docker run command has flag &amp;ndash;network-alias so we can use a common alias between environments.&lt;/p&gt;
&lt;p&gt;If we run the two containers and specify an alias for the database we can see we can ping the alias name from app container.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -d --net=my_isolated_network --name my_mongodb --network-alias mongodb mongo 
docker run -d --net=my_isolated_network --name my_nodeapp -it node
docker exec -it my_nodeapp ping -c 1 mongodb

    PING mongodb (172.18.0.2) 56(84) bytes of data.
    64 bytes from my_mongodb.my_isolated_network (172.18.0.2): icmp_seq=1 ttl=64 time=0.136 ms

    --- mongodb ping statistics ---
    1 packets transmitted, 1 received, 0% packet loss, time 0ms
    rtt min/avg/max/mdev = 0.136/0.136/0.136/0.000 ms
&lt;/code&gt;&lt;/pre&gt;
        
      </description>
    </item>
    
    <item>
      <title>NSX-T for Planespotter</title>
      <link>https://darrylcauldwell.com/post/nsx-planespotter/</link>
      <pubDate>Thu, 14 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-planespotter/</guid>
      <description>
        
          &lt;p&gt;Moving towards application centric infrastructure suitable for microservices applications requires a good example application to work with.  I saw an NSX demo presented by &lt;a href=&#34;https://github.com/yfauser&#34;&gt;Yves Fauser&lt;/a&gt; where he used just such an application called planespotter.&lt;/p&gt;
&lt;p&gt;My first task towards understanding this more was to get this setup in my homelab and look at it in context of NSX-T.&lt;/p&gt;
&lt;h2 id=&#34;homelab-core&#34;&gt;Homelab Core&lt;/h2&gt;
&lt;p&gt;My homelab networking is very simple, I have a domestic BT broadband router, this connects a Cisco SB200 8-port L2 only switch. Each port connected to ESXi is configured with MTU 9216 and has the VLAN passed untagged.&lt;/p&gt;
&lt;p&gt;Connected to this are a pair of Intel NUCs each has onboard 1GB NIC and two USB 1GB NICs.  The lab is used for other things so the on-board vmnic0 NIC is assigned to a VSS which has default &amp;lsquo;VM Network&amp;rsquo; portgroup. This leaves both vusb0 and vusb1 free to be used for N-VDS.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/planespotter-esxi.png&#34; alt=&#34;ESXi Networking&#34;&gt;&lt;/p&gt;
&lt;p&gt;The NUCs are single socket, dual core i5s running at 1.8GHz and each has only 32GB of RAM, these currently run vSphere 6.7 Update 1 and shared storage is all-flash VSAN with no data protection.&lt;/p&gt;
&lt;h2 id=&#34;nsx-t-base&#34;&gt;NSX-T Base&lt;/h2&gt;
&lt;p&gt;Download OVA&amp;rsquo;s and perform initial deployment of NSX Manager,  NSX Controller and NSX Edge.  As lab only has a single L2 all get deployed with all vNICs connected to the portgroup &amp;lsquo;VM Network&amp;rsquo;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/planespotter-edge.png&#34; alt=&#34;NSX-T Base&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;nsx-t-routing&#34;&gt;NSX-T Routing&lt;/h2&gt;
&lt;p&gt;One of the biggest differences between NSX for vSphere and NSX-T is the routing architecture. The services are split between service router (SR) and distributed router (DR), the service router functions are run on the NSX edge and the distributed router (DR) is a kernel module running on the ESXi hosts. My lab setup uses defaults for all transit switches, it is importanrt to understand the relationship when we look at packets flow through these various hops.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/planespotter-edge-arch.png&#34; alt=&#34;NSX-T Routing&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;planespotter&#34;&gt;Planespotter&lt;/h2&gt;
&lt;p&gt;The planespotter application is made up of various microservices and database. For this NSX-T setup each is installed on a VM. The application can be installed by following &lt;a href=&#34;https://github.com/darrylcauldwell/planespotter/blob/master/docs/vm_deployment/README.md&#34;&gt;this guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/planespotter-logical-switches.png&#34; alt=&#34;NSX-T Logical Switches&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;planespotter-fe-to-api-traceflow&#34;&gt;Planespotter FE to API Traceflow&lt;/h2&gt;
&lt;p&gt;One neat feature of NSX-T and geneve is to inject data into the header and use this to trace flows. The traceflow feature helps inspect the path of a packet as it travels from one logical port to a single or multiple logical ports.&lt;/p&gt;
&lt;p&gt;So if we select the port connected to planespotter frontend and port connected to planespotter api, we get a nice visual represenation of the path.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/planespotter-traceflow.png&#34; alt=&#34;NSX-T Logical Switches&#34;&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Controlling NSX-T with Terraform</title>
      <link>https://darrylcauldwell.com/post/nsx-terraform/</link>
      <pubDate>Mon, 07 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-terraform/</guid>
      <description>
        
          &lt;p&gt;Hashicorp Terraform is a tool which enables you to safely and predictably create, change, and improve infrastructure by defining its configuration as code.&lt;/p&gt;
&lt;p&gt;VMware NSX-T is a product which enables software defined network infrastructure.&lt;/p&gt;
&lt;p&gt;The Terraform NSX-T provider allows us to deliver and maintain NSX-T configuration as code.&lt;/p&gt;
&lt;p&gt;This is a walkthrough of how in a very few commands you can begin to control NSX-T configuration using NSX-T.&lt;/p&gt;
&lt;p&gt;If starting from scratch &lt;a href=&#34;https://learn.hashicorp.com/terraform/getting-started/install.html&#34;&gt;install a simple terraform server&lt;/a&gt;,  on CentOS / RHEL you would use these commands.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;yum update -y
yum install wget net-tools unzip -y
cd /tmp
wget https://releases.hashicorp.com/terraform/0.11.11/terraform_0.11.11_linux_amd64.zip
mkdir /terraform
unzip terraform_0.11.11_linux_amd64.zip -d /terraform
cd /terraform
./terraform --version
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once we have this installed we can configure the  NSX-T Manager connection details as variables in a reusable variables file.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat &amp;gt; /terraform/variables.tf &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; &amp;#39;__EOF__&amp;#39;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;variable &amp;#34;nsx_manager&amp;#34; {}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;variable &amp;#34;nsx_username&amp;#34; {}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;variable &amp;#34;nsx_password&amp;#34; {}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;__EOF__&lt;/span&gt;

cat &amp;gt; /terraform/terraform.tfvars &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; &amp;#39;__EOF__&amp;#39;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;nsx_manager = &amp;#34;192.168.1.15&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;nsx_username = &amp;#34;admin&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;nsx_password = &amp;#34;VMware1!&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;__EOF__&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can then create a very basic Terraform configuration file which uses these variables and then performs a simple action like creating a NSX IP Set.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat &amp;gt; /terraform/nsx.tf &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; &amp;#39;__EOF__&amp;#39;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;provider &amp;#34;nsxt&amp;#34; {
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  host                  = &amp;#34;${var.nsx_manager}&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  username              = &amp;#34;${var.nsx_username}&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  password              = &amp;#34;${var.nsx_password}&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  allow_unverified_ssl  = true
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  max_retries           = 10
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  retry_min_delay       = 500
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  retry_max_delay       = 5000
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  retry_on_status_codes = [429]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;resource &amp;#34;nsxt_ip_set&amp;#34; &amp;#34;ip_set1&amp;#34; {
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  description  = &amp;#34;IP Set provisioned by Terraform&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  display_name = &amp;#34;IP Set&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  tag {
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    scope = &amp;#34;color&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    tag   = &amp;#34;blue&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  }
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  ip_addresses = [&amp;#34;1.1.1.1&amp;#34;, &amp;#34;2.2.2.2&amp;#34;]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;__EOF__&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With this in place we can initialize Terraform and get it to pull down the NSX-T provider.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;./terraform init
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If all has gone well Terraform should initialize successfully.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;We can now look at the changes our Terraform configuration file will make to NSX.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;./terraform plan
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We should see the single resource defined in the Terraform configuration file.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Now we have verified it does what we hope we can then look to apply the change.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;./terraform apply
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Not as this is changing configuration we get asked to confirm action.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Once ran successfully we can then check in NSX GUI and confirm the IP Set is created.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;If we would like to launch and destroy application specific NSX network infrastructure when we deploy our application. We would do this with a CI/CD pipeline tool like Jenkins will walk through this. If starting from scratch &lt;a href=&#34;https://wiki.jenkins.io/display/JENKINS/Installing+Jenkins+on+Red+Hat+distributions&#34;&gt;install a simple jenkins server&lt;/a&gt;, on CentOS / RHEL you would use these commands.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;yum update -y
yum install wget net-tools unzip -y
wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat-stable/jenkins.repo
rpm --import https://jenkins-ci.org/redhat/jenkins-ci.org.key
yum install jenkins java -y
service jenkins start
chkconfig jenkins on
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once installed and running we would normally load the &lt;a href=&#34;https://wiki.jenkins.io/display/JENKINS/Terraform+Plugin&#34;&gt;Terraform plugin&lt;/a&gt;,  however this does not presently work as Terraform prompts for confirmation.  There is a &lt;a href=&#34;https://github.com/jenkinsci/terraform-plugin/pull/4/commits/47d6d3da54dd2cc437c1efb5df89cdccdb0f3eb0&#34;&gt;pending pull request to fix this&lt;/a&gt; until this gets merged we need a workaround.&lt;/p&gt;
&lt;p&gt;To workaround this issue we can still control Terraform with Jenkins by calling a shell script from within Jenkins job.  To do this we will create a folder and give Jenkins user account permissions by running following.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;mkdir /terraform
sudo usermod -a -G root jenkins
chmod -R g+w /terraform
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can then create a Jenkins job with Build contents which creates a Terraform file and applies this.  A simple example would be.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cd /terraform

cat &amp;gt; /terraform/nsx.tf &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; &amp;#39;__EOF__&amp;#39;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;provider &amp;#34;nsxt&amp;#34; {
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  host                  = &amp;#34;192.168.1.15&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  username              = &amp;#34;admin&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  password              = &amp;#34;VMware1!&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  allow_unverified_ssl  = true
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  max_retries           = 10
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  retry_min_delay       = 500
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  retry_max_delay       = 5000
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  retry_on_status_codes = [429]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;resource &amp;#34;nsxt_ip_set&amp;#34; &amp;#34;ip_set1&amp;#34; {
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  description  = &amp;#34;IP Set provisioned by Terraform&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  display_name = &amp;#34;IP Set&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  tag {
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    scope = &amp;#34;color&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    tag   = &amp;#34;blue&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  }
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  ip_addresses = [&amp;#34;1.1.1.1&amp;#34;, &amp;#34;2.2.2.2&amp;#34;]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;__EOF__&lt;/span&gt;

./terraform init

./terraform apply -auto-approve

rm -f nsx.tf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This is an overly simple example and more likely we would pull a config file from distributed source control such as github.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>NSX-T for OpenShift</title>
      <link>https://darrylcauldwell.com/post/nsx-openshift/</link>
      <pubDate>Sat, 28 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-openshift/</guid>
      <description>
        
          &lt;p&gt;While looking at the various documentation sets I found it difficult to understand the NSX-T and OpenShift integration. A lot was masked by configuration performed by Ansible scripts. Here I try and record my understanding of the technology and then work through getting this running in a capacity constrained lab environment.&lt;/p&gt;
&lt;h2 id=&#34;nsx-t&#34;&gt;NSX T&lt;/h2&gt;
&lt;p&gt;NSX-T (NSX Transformers) can provide network virtualization for multi-hypervisor environments, including both vSphere and KVM. It is also designed to address emerging application frameworks and architectures that have heterogeneous endpoints and technology stacks such as OpenStack, Red Hat OpenShift, Pivotal Cloud Foundry, Kubernetes, and Docker. NSX-V (NSX for vSphere) Manager integrates into vCenter and leverages a vSphere dvSwitch to form an overlay. NSX-T Manager can be used with vSphere it does not integrate with vCenter or dvSwitch, instead NSX is managed via its API, and its overlay is formed by each member having Open vSwitch (OVS) installed.&lt;/p&gt;
&lt;h2 id=&#34;red-hat-openshift&#34;&gt;Red Hat OpenShift&lt;/h2&gt;
&lt;p&gt;OpenShift helps you to develop, deploy, and manage container-based applications. It provides you with a self-service platform to create, modify, and deploy applications on demand, thus enabling faster development and release life cycles. OpenShift is built around a core of application containers powered by Docker, with orchestration and management provided by Kubernetes.&lt;/p&gt;
&lt;h2 id=&#34;container-networking-framework-background&#34;&gt;Container Networking Framework Background&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/docker/libnetwork/blob/master/docs/design.md&#34;&gt;Libnetwork&lt;/a&gt; is the canonical implementation Container Network Model (CNM) which formalizes the steps required to provide networking for containers while providing an abstraction that can be used to support multiple network drivers. Libnetwork provides an interface between the Docker daemon and network drivers. Container Network Model (CNM) is designed to support the Docker runtime engine only.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/containernetworking/cni&#34;&gt;Container Network Interface&lt;/a&gt; (CNI), consists of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Container Network Interface (CNI) supports integration with any container runtime.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-cni.jpeg&#34; alt=&#34;Container Network Interface (CNI) Integration&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;vmware-nsx-and-kubernetes-integration&#34;&gt;VMware NSX and Kubernetes Integration&lt;/h2&gt;
&lt;p&gt;VMware provide an &lt;a href=&#34;https://my.vmware.com/group/vmware/details?downloadGroup=NSX-T-PKS-221&amp;amp;productId=673&#34;&gt;NSX Container Plugin package&lt;/a&gt; which contains the required modules to integrate NSX-T with Kubernetes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NSX Container Plugin (NCP) - is a container image which watches the Kubernetes API for changes to Kubernetes Objects (namespaces, network policies, services etc.). It calls the NSX API to creates network constructs based on object addition and changes.&lt;/li&gt;
&lt;li&gt;NSX DaemonSet
&lt;ul&gt;
&lt;li&gt;NSX Node Agent - is a container image which manages the container network interface&lt;/li&gt;
&lt;li&gt;NSX Kube-Proxy - is a container image which replaces the native distributed east-west load balancer in Kubernetes with the NSX load-balancer based on Open vSwitch (OVS).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;NSX Container Network Interface (CNI) - is an executable which allow the integration of NSX into Kubernetes.&lt;/li&gt;
&lt;li&gt;Open vSwitch&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-ncp.jpeg&#34; alt=&#34;NSX and Kubernetes Integration&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;nsx-for-openshift&#34;&gt;NSX For OpenShift&lt;/h2&gt;
&lt;p&gt;NSX implements a discreet network topology per Kubernetes namespace. NSX maps logical network elements like logical switches and distributed logical router to Kubernetes namespaces. Each of those network topologies can be directly routed, or privately addressed and behind NAT.&lt;/p&gt;
&lt;h2 id=&#34;nsx-for-openshift-homelab&#34;&gt;NSX For OpenShift Homelab&lt;/h2&gt;
&lt;p&gt;For the rest of this blog post I am aiming to create a NSX OpenShift integration. I aiming for two namespaces, each with a logical router and three subnets. The namespaces will use private address ranges and the tier-0 router will provide SNAT connectivity to the routed network.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-topology.jpeg&#34; alt=&#34;NSX Topology&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;starting-point-homelab-configuration&#34;&gt;Starting point homelab configuration&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;1GbE Switch (Layer 2 only)
&lt;ul&gt;
&lt;li&gt;VLAN 0 - CIDR 192.168.1.0/24&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;vSphere vCenter Appliance 6.7&lt;/li&gt;
&lt;li&gt;3x vSphere ESXi 6.7 Update 1 hosts (Intel NUC - 3x 1.8GHz CPU &amp;amp; 32GB RAM)
&lt;ul&gt;
&lt;li&gt;Onboard NIC is connected to a vSphere Standard Switch&lt;/li&gt;
&lt;li&gt;USB3 NIC is unused and will be used for NSX&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;VSAN&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following resources are required&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Small NSX-T Manager is a VM sized 8GB vRAM, 2x vCPU and 140GB vHDD&lt;/li&gt;
&lt;li&gt;Small NSX Controller is a VM sized 8GB vRAM, 2x vCPU and 120GB vHDD&lt;/li&gt;
&lt;li&gt;Small NSX Edge is a VM sized 4GB vRAM, 2x vCPU and 120GB vHDD&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;nsx-management-plane&#34;&gt;NSX Management Plane&lt;/h2&gt;
&lt;p&gt;Deploy a small NSX unifed appliance specifying the nsx-manager role. Once deployed link this to vCenter, to do this add vCenter in &amp;lsquo;Fabric / Compute Manager&amp;rsquo;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-compute-manager.jpeg&#34; alt=&#34;NSX-T Management Plane&#34;&gt;&lt;/p&gt;
&lt;p&gt;With the manager in place we now need to create the management plane, to do this we need to install the management plane agent (MPA) on each host so they are added as usable Fabric Nodes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-nodes.jpeg&#34; alt=&#34;NSX-T Fabric Nodes&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;tunnel-endpoint-ip-pool&#34;&gt;Tunnel Endpoint IP Pool&lt;/h2&gt;
&lt;p&gt;We create an IP pool one for the Transort Nodes to communicate for my scenario the three ESXi hosts and an edge will all participate so I create an IP Pool with four addresses. Navigate to Inventory &amp;gt; Groups &amp;gt; IP Pools and click add.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-ip-pool.png&#34; alt=&#34;NSX-T IP Pool&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;nsx-control-plane&#34;&gt;NSX Control Plane&lt;/h2&gt;
&lt;p&gt;In order to create an overlay network we need an NSX Controller to manage the hosts. NSX Controllers serve as the central control point got all hosts, logical switches, and logical routers.&lt;/p&gt;
&lt;p&gt;While NSX Manager can deploy and configure NSX Controllers the size cannot be selected. As lab is resource constrained I only want a small NSX Controller, the &amp;lsquo;NSX Controller for VMware ESXi&amp;rsquo; is a separate OVA download where size can be selected.&lt;/p&gt;
&lt;p&gt;Once the controller appliance is deployed we need to facilitate communications between it and nsx manager.  To do this open an SSH session with admin user to NSX Manager and run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;get certificate api thumbprint
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Open an SSH session to NSX Controller with admin user and run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;join management-plane &amp;lt;NSX-Manager&amp;gt; username admin thumbprint &amp;lt;NSX-Managers-thumbprint&amp;gt;

set control-cluster security-model shared-secret

initialize control-cluster
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-mgr-ctrl-thumb.jpeg&#34; alt=&#34;NSX-T Controller&#34;&gt;&lt;/p&gt;
&lt;p&gt;This should then be viewable in NSX Manager&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-control-cluster.jpeg&#34; alt=&#34;NSX-T Controller Cluster&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;overlay-transport-zone&#34;&gt;Overlay Transport Zone&lt;/h2&gt;
&lt;p&gt;All the virtual network objects will need to communicate across an overlay network. To faciliate this the three esxi hosts and edges need to be part of an Overlay Transport Zone.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-transport.jpeg&#34; alt=&#34;NSX-T Transport Zone&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once we have a Transport Zone we can add our NSX fabric nodes as transport nodes. Navigate menu to Select Fabric &amp;gt; Transport Nodes and click Add.  A wizard will open on the general tab select first Node (host), give appropriate name for that host and select the openshift transport zone.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-transport-node.jpeg&#34; alt=&#34;NSX-T Transport Node&#34;&gt;&lt;/p&gt;
&lt;p&gt;Change to N-VDS tab, create N-VDS for openshift, select default NIOC, select default hostswitch Uplink profile, select transport IP Pool and enter Physical NIC identifier for Uplink-1.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-host-vds.jpeg&#34; alt=&#34;NSX-T Transport Zone N-VDS&#34;&gt;&lt;/p&gt;
&lt;p&gt;In order that the NSX Container Plugin can find the correct NSX objects all of the NSX objects created require a tag applying. For this lab build I am using tag dc-openshift. Navigate within NSX Manager to Fabric &amp;gt; Transport Zones, select overlay network then Actions &amp;gt; Manage Tags and apply tag.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Scope = ncp/cluster and Tag = dc-openshift
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-ncp-tags.jpeg&#34; alt=&#34;NSX-T Openshift Tags&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;vlan-transport-zone&#34;&gt;VLAN Transport Zone&lt;/h2&gt;
&lt;p&gt;As well as connecting to the overlay network the Edges running Tier-0 routing functions also needs to be able to connect to the physical network. This connectivity is achieved by using a Transport Zone of type VLAN.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-vlan-transport.png&#34; alt=&#34;NSX-T VLAN Transport Zone&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;nsx-edge&#34;&gt;NSX Edge&lt;/h2&gt;
&lt;p&gt;We need some way for the logical container overlay network to communicate with the physical network. AN NSX Edge can host services which provide this connectivity.&lt;/p&gt;
&lt;p&gt;The NSX Edge has 4 network adapters, the first is used by the management network, the other 3 interfaces (fp-eth0, fp-eth1 and fp-eth2) can then be used for connecting to overlay networks or for routing. Within my lab I have a single flat physical network so all NSX Edge interfaces connect to the same Port Group.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;GUI Reference&lt;/th&gt;
&lt;th&gt;VM vNIC&lt;/th&gt;
&lt;th&gt;NIC&lt;/th&gt;
&lt;th&gt;Lab Function&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Managewment&lt;/td&gt;
&lt;td&gt;Network adapter 1&lt;/td&gt;
&lt;td&gt;eth0&lt;/td&gt;
&lt;td&gt;Management&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Datapath #1&lt;/td&gt;
&lt;td&gt;Network adapter 2&lt;/td&gt;
&lt;td&gt;fp-eth0&lt;/td&gt;
&lt;td&gt;Overlay&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Datapath #2&lt;/td&gt;
&lt;td&gt;Network adapter 3&lt;/td&gt;
&lt;td&gt;fp-eth1&lt;/td&gt;
&lt;td&gt;Uplink&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Datapath #3&lt;/td&gt;
&lt;td&gt;Network adapter 4&lt;/td&gt;
&lt;td&gt;fp-eth2&lt;/td&gt;
&lt;td&gt;Unused&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-edge.jpeg&#34; alt=&#34;NSX-T Add Edge&#34;&gt;&lt;/p&gt;
&lt;p&gt;The NSX Edge needs to participate in the Overlay Transport Zone so we need to first configure this as Transport Node.  This is very similar process to how we setup ESXi hosts as Transport Nodes except on N-VDS tab we add to both overlay and vlan transport zones,  we use the edge-vm Uplink profile and for Virtual NIC select appropriate NIC as per table above.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-edge-nvds.png&#34; alt=&#34;NSX-T Edge N-VDS&#34;&gt;&lt;/p&gt;
&lt;p&gt;In order we can deploy Tier-0 router the Edge needs to be a member of an Edge Cluster.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-edge-cluster.jpeg&#34; alt=&#34;NSX-T Add Edge Cluster&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;tier-0-router&#34;&gt;Tier-0 Router&lt;/h2&gt;
&lt;p&gt;Once the Edge Cluster is created we can create the tier-0 router.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-tier0.jpeg&#34; alt=&#34;NSX-T Add Edge Tier-0 Router&#34;&gt;&lt;/p&gt;
&lt;p&gt;In my lab I have 192.168.1.0 /24 and will be using the 172.16.0.0 /16 address space for NSX. I would like to use network address translation (NAT) and allocate a separate SNAT IP on the 192.168.1.0 network for each OpenShift namespace on the 172.16.0.0 network.  To achieve this I need to configure a redistribution criteria of type Tier-0 NAT.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-tier0-route-redist.jpeg&#34; alt=&#34;NSX-T Add Edge Tier-0 Route Redist&#34;&gt;&lt;/p&gt;
&lt;p&gt;The next step requires an NSX Logical Switch so we create that.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-logical-switch.jpeg&#34; alt=&#34;NSX-T Add Logical Switch&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can now configure the Router Port,  selecting the Transport Node and Logical Switch.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-tier0-route-port.jpeg&#34; alt=&#34;NSX-T Add Tier-0 Router Port&#34;&gt;&lt;/p&gt;
&lt;p&gt;This will be used by OpenShift to once created navigate to Actions &amp;gt; Manage Tags and apply tag.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Scope = ncp/cluster and Tag = dc-openshift
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-ncp-tags.jpeg&#34; alt=&#34;NSX-T Add NCP Tags&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;ip-block-kubernetes-pods&#34;&gt;IP Block Kubernetes Pods&lt;/h2&gt;
&lt;p&gt;In order to create the topology we are aiming for we need to create an IP Blocks for each of our two namespaces.  Within each IP Block we need to create the three subnets. In the end you should end up with something which looks like this, and all IP Block needs to have the ncp/cluster tag.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-ddi-blocks.jpeg&#34; alt=&#34;NSX-T Add NCP Tags&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;ip-pool-snat&#34;&gt;IP Pool SNAT&lt;/h2&gt;
&lt;p&gt;We create an IP pool for the tier-0 router to issue SNAT and provide external (floating) IPs to OpenShift.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-snat-pool.jpeg&#34; alt=&#34;NSX-T SNAT Pool&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once created add the following two tags,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Scope = ncp/cluster and Tag = dc-openshift
Scope = ncp/external and Tag = true
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;red-hat-openshift-origin&#34;&gt;Red Hat OpenShift Origin&lt;/h2&gt;
&lt;p&gt;OpenShift Origin is a computer software product from Red Hat for container-based software deployment and management. It is a supported distribution of Kubernetes using Docker containers and DevOps tools for accelerated application development.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift.jpeg&#34; alt=&#34;Openshift Stack&#34;&gt;&lt;/p&gt;
&lt;p&gt;OpenShift Origin is the upstream community project used in &lt;a href=&#34;https://www.openshift.com/products/online/&#34;&gt;OpenShift Online&lt;/a&gt;, &lt;a href=&#34;https://www.openshift.com/products/dedicated/&#34;&gt;OpenShift Dedicated&lt;/a&gt;, and &lt;a href=&#34;https://www.openshift.com/products/container-platform/&#34;&gt;OpenShift Container Platform (formerly known as OpenShift Enterprise)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;VMware provides &lt;a href=&#34;https://github.com/vmware/nsx-integration-for-openshift&#34;&gt;Red Hat Ansible playbooks for installing NSX-T for OpenShift Container Platform&lt;/a&gt;. However, OpenShift Container Platform is a licensed product and this deploys a scaled-out deployment. Neither of these lend itself to a home lab deployment, my goal for the rest of this blog post is to detail the steps I follow for a cutdown installation.&lt;/p&gt;
&lt;h2 id=&#34;create-openshift-origin-base-vm&#34;&gt;Create OpenShift Origin Base VM&lt;/h2&gt;
&lt;p&gt;The OpenShift Container Platform is Red Hat Enterprise Linux based, I don&amp;rsquo;t have a Red Hat Enterprise Linux subscription license. As such I created a CentOS 7 (64-bit) virtual machine, as the library versions are the same, so binaries that work on one will work on the other.&lt;/p&gt;
&lt;p&gt;Each OpenShift node needs to be managed and also provide connectivity to NSX, it is possible to perform these two functions on same vNIC however, I give my VM two vNICs one for management on VLAN backed dvPortgroup and one for NSX on VXLAN backed dvPortgroup. I used the CentOS minimal installation ISO set static IP address on management vNIC, and create DNS A &amp;amp; PTR records for this.&lt;/p&gt;
&lt;p&gt;Once built I run following commands to install Docker, some other basic tools and apply latest patches.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt; /etc/yum.repos.d/docker.repo &amp;lt;&amp;lt; &#39;__EOF__&#39;
[docker]
name=Docker Repository
baseurl=https://yum.dockerproject.org/repo/main/centos/7/
enabled=1
gpgcheck=1
gpgkey=https://yum.dockerproject.org/gpg
__EOF__
yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
yum install -y git open-vm-tools wget docker-engine net-tools python-pip
pip install docker-py
systemctl enable docker.service
yum update -y
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;default-kubernetes-service-addresses&#34;&gt;Default Kubernetes Service Addresses&lt;/h2&gt;
&lt;p&gt;OpenShift leverages the Kubernetes concept of a pod, which is one or more containers deployed together on one host, and the smallest compute unit that can be defined, deployed, and managed. A Kubernetes service address serves as an internal load balancer. It identifies a set of replicated pods in order to proxy the connections it receives to them. Services are assigned an IP address and port pair that, when accessed, proxy to an appropriate backing pod. These service addresses are assigned and managed by OpenShift. By default they are assigned out of the 172.30.0.0/16 network.&lt;/p&gt;
&lt;p&gt;To setup our environment we can configure the Docker daemon with an insecure registry parameter of 172.30.0.0/16.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl start docker
touch /etc/docker/daemon.json
cat &amp;gt; /etc/docker/daemon.json &amp;lt;&amp;lt; &#39;__EOF__&#39;
{
&amp;quot;insecure-registries&amp;quot;: [
    &amp;quot;172.30.0.0/16&amp;quot;
    ]
}
__EOF__
systemctl daemon-reload
systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;add-openshift-client&#34;&gt;Add OpenShift Client&lt;/h1&gt;
&lt;p&gt;The OpenShift client is used to manage the OpenShift installation and configuration it is supplied as a package. Download this, unpack and add to runtime path.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /tmp
wget https://github.com/openshift/origin/releases/download/v3.10.0-rc.0/openshift-origin-client-tools-v3.10.0-rc.0-c20e215-linux-64bit.tar.gz
tar -xvf /tmp/openshift-origin-client-tools-v3.10.0-rc.0-c20e215-linux-64bit.tar.gz -C /bin
mv /bin/openshift* /home/openshift
echo &#39;PATH=$PATH:/home/openshift&#39; &amp;gt; /etc/profile.d/oc-path.sh
chmod +x /etc/profile.d/oc-path.sh
. /etc/profile
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;start-openshift-origin-as-all-in-one-cluster&#34;&gt;Start OpenShift Origin as all-in-one Cluster&lt;/h2&gt;
&lt;p&gt;For next steps we need a basic OpenShift stack. Rather than build something custom we can simply start a local OpenShift all-in-one cluster with a configured registry, router, image streams, and default templates, by running the following command (where openshift.darrylcauldwell.com is the FQDN which points to IP address of management interface of your VM),&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc cluster up --public-hostname=openshift.darrylcauldwell.com
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We should also be able to logon and see all of the OpenShift services listed&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc login -u system:admin
oc get services --all-namespaces
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;NAMESPACE&lt;/th&gt;
&lt;th&gt;NAME&lt;/th&gt;
&lt;th&gt;TYPE&lt;/th&gt;
&lt;th&gt;CLUSTER-IP&lt;/th&gt;
&lt;th&gt;EXTERNAL-IP&lt;/th&gt;
&lt;th&gt;PORT(S)&lt;/th&gt;
&lt;th&gt;AGE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;default&lt;/td&gt;
&lt;td&gt;docker-registry&lt;/td&gt;
&lt;td&gt;ClusterIP&lt;/td&gt;
&lt;td&gt;172.30.1.1&lt;/td&gt;
&lt;td&gt;none&lt;/td&gt;
&lt;td&gt;5000/TCP&lt;/td&gt;
&lt;td&gt;9m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;default&lt;/td&gt;
&lt;td&gt;kubernetes&lt;/td&gt;
&lt;td&gt;ClusterIP&lt;/td&gt;
&lt;td&gt;172.30.0.1&lt;/td&gt;
&lt;td&gt;none&lt;/td&gt;
&lt;td&gt;443/TCP,53/UDP,53/TCP&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;default&lt;/td&gt;
&lt;td&gt;router&lt;/td&gt;
&lt;td&gt;ClusterIP&lt;/td&gt;
&lt;td&gt;172.30.88.3&lt;/td&gt;
&lt;td&gt;none&lt;/td&gt;
&lt;td&gt;80/TCP,443/TCP,1936/TCP&lt;/td&gt;
&lt;td&gt;9m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-dns&lt;/td&gt;
&lt;td&gt;kube-dns&lt;/td&gt;
&lt;td&gt;ClusterIP&lt;/td&gt;
&lt;td&gt;172.30.0.2&lt;/td&gt;
&lt;td&gt;none&lt;/td&gt;
&lt;td&gt;53/UDP,53/TCP&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openshift-apiserver&lt;/td&gt;
&lt;td&gt;api&lt;/td&gt;
&lt;td&gt;ClusterIP&lt;/td&gt;
&lt;td&gt;172.30.85.121&lt;/td&gt;
&lt;td&gt;none&lt;/td&gt;
&lt;td&gt;443/TCP&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openshift-web-console&lt;/td&gt;
&lt;td&gt;webconsole&lt;/td&gt;
&lt;td&gt;ClusterIP&lt;/td&gt;
&lt;td&gt;172.30.83.178&lt;/td&gt;
&lt;td&gt;none&lt;/td&gt;
&lt;td&gt;443/TCP&lt;/td&gt;
&lt;td&gt;9m&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We should also be able to see all of the OpenShift pods listed&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc get pod --all-namespaces
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;NAMESPACE&lt;/th&gt;
&lt;th&gt;NAME&lt;/th&gt;
&lt;th&gt;READY&lt;/th&gt;
&lt;th&gt;STATUS&lt;/th&gt;
&lt;th&gt;RESTARTS&lt;/th&gt;
&lt;th&gt;AGE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;default&lt;/td&gt;
&lt;td&gt;docker-registry-1-4l59n&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;default&lt;/td&gt;
&lt;td&gt;persistent-volume-setup-grm9s&lt;/td&gt;
&lt;td&gt;0/1&lt;/td&gt;
&lt;td&gt;Completed&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;default&lt;/td&gt;
&lt;td&gt;router-1-5xtqg&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-dns&lt;/td&gt;
&lt;td&gt;kube-dns-bj5cq&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;11m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-proxy&lt;/td&gt;
&lt;td&gt;kube-proxy-9l8ql&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;11m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-system&lt;/td&gt;
&lt;td&gt;kube-controller-manager-localhost&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-system&lt;/td&gt;
&lt;td&gt;kube-scheduler-localhost&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-system&lt;/td&gt;
&lt;td&gt;master-api-localhost&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-system&lt;/td&gt;
&lt;td&gt;master-etcd-localhost&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;11m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openshift-apiserver&lt;/td&gt;
&lt;td&gt;openshift-apiserver-ptk5j&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;11m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openshift-controller-manager&lt;/td&gt;
&lt;td&gt;openshift-controller-manager-vg7gm&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openshift-core-operators&lt;/td&gt;
&lt;td&gt;openshift-web-console-operator-78ddf7cbb7-r8dhd&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openshift-web-console&lt;/td&gt;
&lt;td&gt;webconsole-847bc4ccc4-hgsv4&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Once running we can open browser to OpenShift Origin&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;https://openshift.darrylcauldwell.com:8443/console/catalog
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Default credentials username &amp;lsquo;system&amp;rsquo; password &amp;lsquo;admin&amp;rsquo;&lt;/p&gt;
&lt;h2 id=&#34;nsx-t-open-vswitch&#34;&gt;NSX-T Open vSwitch&lt;/h2&gt;
&lt;p&gt;The NSX-T Container Plug-in (NCP) relies on Open vSwitch (OVS) providing a bridge to the NSX Logical Switch. VMware provide an Open vSwitch (OVS)  in the &lt;a href=&#34;https://my.vmware.com/web/vmware/details?downloadGroup=NSX-T-PKS-220&amp;amp;productId=673&#34;&gt;NSX Container Plugin 2.2.0&lt;/a&gt;, package.  Download expand and copy to OpenShift VM /tmp folder. Once uploaded install the following packages.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install -y /tmp/nsx-container-2.2.0.8740202/OpenvSwitch/rhel74_x86_64/kmod-openvswitch-2.9.1.8614397.rhel74-1.el7.x86_64.rpm
yum install -y /tmp/nsx-container-2.2.0.8740202/OpenvSwitch/rhel74_x86_64/openvswitch-2.9.1.8614397.rhel74-1.x86_64.rpm
yum install -y /tmp/nsx-container-2.2.0.8740202/OpenvSwitch/rhel74_x86_64/openvswitch-kmod-2.9.1.8614397.rhel74-1.el7.x86_64.rpm
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once installed start the Open vSwitch&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;service openvswitch start
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once the Open vSwitch is running we can create a bridge network interface, and then connect this to the VM network interface located on the NSX-T Logical Switch. You can do this by running the following command (where eno33559296 is the devicename of NIC on NSX Logical Switch),&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ovs-vsctl add-br br-int
ovs-vsctl add-port br-int eno33559296 -- set Interface eno33559296 ofport_request=1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;These connections are created with link state DOWN in order to use them we need to set link status is up for both,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ip link set br-int up
ip link set eno33559296 up
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Update the network configuration file to ensure that the network interface is up after a reboot.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vi /etc/sysconfig/network-scripts/ifcfg-eno33559296
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Ensure has a line reading,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ONBOOT=yes
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;nsx-t-container-network-interface-cni&#34;&gt;NSX-T Container Network Interface (CNI)&lt;/h2&gt;
&lt;p&gt;The NSX-T Container Plug-in (NCP) provides integration between NSX-T and container orchestrators such as Kubernetes. The installation files are in same package as the NSX Open vSwitch (OVS). Install using command.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install -y /tmp/nsx-container-2.2.0.8740202/Kubernetes/rhel_x86_64/nsx-cni-2.2.0.8740202-1.x86_64.rpm
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;nsx-t-container-plug-in-ncp-replicationcontroller-rc&#34;&gt;NSX-T Container Plug-in (NCP) ReplicationController (RC)&lt;/h2&gt;
&lt;p&gt;There are a few accounts used for rights assignments, the project, users and roles are defined in NCP RBAC file. To create the users within the project run,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc login -u system:admin
oc create -f /tmp/nsx-container-2.2.0.8740202/nsx-ncp-rbac.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The RBAC creates two service account users, the tokens for these are required by NCP in folder /etc/nsx-ujo. This gets mounted as config-volume and these tokens used for authentication.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc project nsx-system
mkdir -p /etc/nsx-ujo
SVC_TOKEN_NAME=&amp;quot;$(oc get serviceaccount ncp-svc-account -o yaml | grep -A1 secrets | tail -n1 | awk {&#39;print $3&#39;})&amp;quot;
oc get secret $SVC_TOKEN_NAME -o yaml | grep &#39;token:&#39; | awk {&#39;print $2&#39;} | base64 -d &amp;gt; /etc/nsx-ujo/ncp_token
NODE_TOKEN_NAME=&amp;quot;$(oc get serviceaccount nsx-node-agent-svc-account -o yaml | grep -A1 secrets | tail -n1 | awk {&#39;print $3&#39;})&amp;quot;
oc get secret $NOD_TOKEN_NAME -o yaml | grep &#39;token:&#39; | awk {&#39;print $2&#39;} | base64 -d &amp;gt; /etc/nsx-ujo/node_agent_token
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The pods which NSX-T Container Plug-in (NCP) ReplicationController (RC) run in need to use the host networking so we need to allow then this right by loading the NCP Security Context Constraints for NCP and NSX Node Agent.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc apply -f /tmp/nsx-container-2.2.0.8740202/Kubernetes/rhel_x86_64/ncp-os-scc.yml
oc adm policy add-scc-to-user ncp-scc -z ncp-svc-account
oc adm policy add-scc-to-user ncp-scc -z nsx-node-agent-svc-account
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Edit the ReplicationController (RC) YML file,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vi /tmp/nsx-container-2.2.0.8740202/Kubernetes/ncp-rc.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Ensure the following lines are configured thus,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;serviceAccountName: ncp-svc-account
apiserver_host_port = 8443
apiserver_host_ip = 192.168.1.20
nsx_api_managers = 192.168.1.15
insecure = True
nsx_api_user = admin
nsx_api_password = VMware1!
cluster = dc-openshift
adaptor = openshift
enable_snat = True
tier0_router = 0d772616-4c44-47ae-ac9e-06f3c0222211
overlay_tz = 5eeefd4c-bd7d-4871-9eba-d7ed02394dec
container_ip_blocks = 562c85de-8675-4bb2-b211-3f95a6342e0e, f225d518-2fe3-4f8d-a476-a4697bff3ea6
external_ip_pools = d5095d53-c7f8-4fcd-9fad-3032afd080a4
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The NSX-T Container Plug-in (NCP) is a docker image which we import into the local registry.  The image is referenced by later script by different tag name so we add an additional tag.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker load -i /tmp/nsx-container-2.2.0.8740202/Kubernetes/nsx-ncp-rhel-2.2.0.8740202.tar
docker image tag registry.local/2.2.0.8740202/nsx-ncp-rhel nsx-ncp
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then we can create NSX ReplicationController&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc project nsx-system
oc create -f /tmp/nsx-container-2.2.0.8740202/Kubernetes/ncp-rc.yml
oc describe rc/nsx-ncp
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We should now see the container running within pod namespace nsx-system.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc get pod --all-namespaces
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If all has gone well we can now connect to the NCP container and use the &lt;a href=&#34;https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.ncp_openshift.doc/GUID-12F44CD5-0518-41C3-BB14-5507224A5D60.html&#34;&gt;nsxcli&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc exec -it nsx-ncp-6k5t2 nsxcli
get ncp-nsx status
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;nsx-t-container-plug-in-ncp-node-agent-daemonset-ds&#34;&gt;NSX-T Container Plug-in (NCP) Node Agent DaemonSet (DS)&lt;/h2&gt;
&lt;p&gt;Edit the nsx-node-agent-ds.yml file,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vi /tmp/nsx-container-2.2.0.8740202/Kubernetes/rhel_x86_64/nsx-node-agent-ds.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Ensure the following is set,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;serviceAccountName: nsx-node-agent-svc-account
cluster = dc-openshift
apiserver_host_port = 8443
apiserver_host_ip = 192.168.1.20
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once updated create the Node Agent Daemonset (DS),&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc login -u system:admin
oc apply -f /tmp/nsx-container-2.2.0.8740202/Kubernetes/rhel_x86_64/nsx-node-agent-ds.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Check the Node Agent Daemonset is there,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc describe daemonset.apps/nsx-node-agent
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We should also be able to see all of the OpenShift pods listed including our two NSX ones.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc get pod --all-namespaces
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;NAMESPACE&lt;/th&gt;
&lt;th&gt;NAME&lt;/th&gt;
&lt;th&gt;READY&lt;/th&gt;
&lt;th&gt;STATUS&lt;/th&gt;
&lt;th&gt;RESTARTS&lt;/th&gt;
&lt;th&gt;AGE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;default&lt;/td&gt;
&lt;td&gt;docker-registry-1-4l59n&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;default&lt;/td&gt;
&lt;td&gt;persistent-volume-setup-grm9s&lt;/td&gt;
&lt;td&gt;0/1&lt;/td&gt;
&lt;td&gt;Completed&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;default&lt;/td&gt;
&lt;td&gt;router-1-5xtqg&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-dns&lt;/td&gt;
&lt;td&gt;kube-dns-bj5cq&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-proxy&lt;/td&gt;
&lt;td&gt;kube-proxy-9l8ql&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-system&lt;/td&gt;
&lt;td&gt;kube-controller-manager-localhost&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-system&lt;/td&gt;
&lt;td&gt;kube-scheduler-localhost&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-system&lt;/td&gt;
&lt;td&gt;master-api-localhost&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-system&lt;/td&gt;
&lt;td&gt;master-etcd-localhost&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nsx-system&lt;/td&gt;
&lt;td&gt;nsx-ncp-9m2jl&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nsx-system&lt;/td&gt;
&lt;td&gt;nsx-node-agent-jlt5t&lt;/td&gt;
&lt;td&gt;2/2&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;4m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openshift-apiserver&lt;/td&gt;
&lt;td&gt;openshift-apiserver-ptk5j&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openshift-controller-manager&lt;/td&gt;
&lt;td&gt;openshift-controller-manager-vg7gm&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openshift-core-operators&lt;/td&gt;
&lt;td&gt;openshift-web-console-operator-78ddf7cbb7-r8dhd&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openshift-web-console&lt;/td&gt;
&lt;td&gt;webconsole-847bc4ccc4-hgsv4&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;testing&#34;&gt;Testing&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;oc create namespace my-first
oc logs nsx-ncp-9m2jl | grep ERROR
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;nsx_ujo.k8s.ns_watcher Failed to create NSX topology for project my-first: Unexpected error from backend manager ([&amp;lsquo;192.168.1.15&amp;rsquo;]) for Allocate subnet from IP block&lt;/p&gt;
&lt;p&gt;more commands for working OpenShift here&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://static.rainfocus.com/vmware/vmworldus17/sess/148924638739800152Do/finalpresentationPDF/NET1522BU_FORMATTED_FINAL_1507910147966001nlDx.pdf&#34;&gt;https://static.rainfocus.com/vmware/vmworldus17/sess/148924638739800152Do/finalpresentationPDF/NET1522BU_FORMATTED_FINAL_1507910147966001nlDx.pdf&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Controlling vSphere &amp; NSX-V With Python</title>
      <link>https://darrylcauldwell.com/post/nsx-python/</link>
      <pubDate>Mon, 15 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-python/</guid>
      <description>
        
          &lt;p&gt;My former colleagues had made me aware of pyVmomi an open source library which VMware provide and mostly maintain for managing vSphere, so its here I shall start. Since then NSX for vSphere has also an open source library NSX RAML Client provided by VMware so I&amp;rsquo;ll then move to that.&lt;/p&gt;
&lt;p&gt;I am performing this learning exercise in my home lab  using is vSphere 6.5, vSAN 6.5, NSX6.3, with Python 2.7.10, although this should work the same with other versions.&lt;/p&gt;
&lt;p&gt;Install pyVmomi and open vCenter connection and then initiate an interactive python environment&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;git clone https://github.com/vmware/pyvmomi.git
sudo pip install -r ~/pyvmomi/requirements.txt
python
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Import the pyVim, pyVmomi &amp;amp; SSL libraries we are using,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; ssl
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pyVim &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; connect
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pyVmomi &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; vim, vmodl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Open connection to vCenter then gather contents as an object&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;vcenter &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; connect&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;SmartConnect(
    host&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;192.168.1.13&amp;#39;&lt;/span&gt;,
    user&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;administrator@vsphere.local&amp;#39;&lt;/span&gt;,
    pwd&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;VMware1!&amp;#39;&lt;/span&gt;,
    port&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;443&lt;/span&gt;,
    sslContext&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;ssl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_create_unverified_context()
)
content &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; vcenter&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;RetrieveContent()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;basic-get-of-information&#34;&gt;Basic get of information&lt;/h1&gt;
&lt;p&gt;Once we have vCenter Object Model as content object we can output any part of this data&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;content.about.fullName
&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;VMware vCenter Server 6.5.0 build-5178943&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can also explore the Object Model which is well descrived here in the &lt;a href=&#34;http://pubs.vmware.com/vsphere-65/topic/com.vmware.wssdk.apiref.doc/right-pane.html&#34;&gt;vSphere SDK API Docs&lt;/a&gt; and when we know what we want to look for we can search and display anything we like, for example the list of virtual machines.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;objView &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; content&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;viewManager&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;CreateContainerView(content&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rootFolder,[vim&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;VirtualMachine],True)
vmList &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; objView&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;view
objView&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Destroy()
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt;  vm &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; vmList:
    vm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;summary&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;config&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;basic-put-of-configuration-information&#34;&gt;Basic put of configuration information&lt;/h1&gt;
&lt;p&gt;As well as getting information from the Object Model we can just as easily apply configuration to items within (assuming the account we connect with has sufficient rights),  for example if we gather the list of hosts and set a advanced option on all of them.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;objView &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; content&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;viewManager&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;CreateContainerView(content&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rootFolder,[vim&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;HostSystem],True)
hostList &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; objView&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;view
objView&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Destroy()
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; host &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; hostList:
    optionManager &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; host&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;configManager&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;advancedOption
    option &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; vim&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;option&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;OptionValue(key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;VSAN.ClomRepairDelay&amp;#39;&lt;/span&gt;, value&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;long(&lt;span style=&#34;color:#ae81ff&#34;&gt;120&lt;/span&gt;))
    optionManager&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;UpdateOptions(changedValue&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[option])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;nsx-for-vsphere&#34;&gt;NSX for vSphere&lt;/h1&gt;
&lt;p&gt;So we have the pyVmomi library for vSphere, in addition to this VMware have provided open source library for &lt;a href=&#34;https://github.com/vmware/nsxramlclient&#34;&gt;NSX for vSphere&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll first make sure the packages are installed along with the additional packages for managing NSX.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;sudo pip install nsxramlclient pyvim pyvmomi lxml requests
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The NSX for vSphere REST API changes with each version, so in order to use the nsxramlclient library we will need a RAML file specific to version of NSX-V we are connecting to. The RAML file also produces nice &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/vmware/nsxraml/blob/6.3/html-version/nsxvapi.html&#34;&gt;dynamic documentation of the NSX APIs&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;git clone https://github.com/vmware/nsxraml
cd nsxraml
git checkout 6.3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So now we can try and connect and get some information about anything described in the API document, like NSX Controllers.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; nsxramlclient.client &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; NsxClient
nsx_manager &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;192.168.1.18&amp;#34;&lt;/span&gt;
nsx_username &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;admin&amp;#34;&lt;/span&gt;
nsx_password &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;VMware1!VMware1!&amp;#34;&lt;/span&gt;
nsxraml_file &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;nsxvapi.raml&amp;#39;&lt;/span&gt;
nsx_manager_session &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; NsxClient(nsxraml_file, nsx_manager, nsx_username, nsx_password)
nsx_controllers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nsx_manager_session&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;nsxControllers&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;read&amp;#39;&lt;/span&gt;)
nsx_controllers
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;When it comes to putting and posting information getting the formatting right can be a challenge. To this end with the library it is possible to create a template python dictionary using extract_resource_body_example.  Once we have this we can display the output structure but more usefully we can also substitute values into the template.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;new_ls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nsx_manager_session&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;extract_resource_body_example(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;logicalSwitches&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;create&amp;#39;&lt;/span&gt;)
nsx_manager_session&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;view_body_dict(new_ls)
new_ls[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;virtualWireCreateSpec&amp;#39;&lt;/span&gt;][&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;TestLogicalSwitch1&amp;#39;&lt;/span&gt;
new_ls[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;virtualWireCreateSpec&amp;#39;&lt;/span&gt;][&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;description&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;TestLogicalSwitch1&amp;#39;&lt;/span&gt;
new_ls[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;virtualWireCreateSpec&amp;#39;&lt;/span&gt;][&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;tenantId&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Tenant1&amp;#39;&lt;/span&gt;
new_ls[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;virtualWireCreateSpec&amp;#39;&lt;/span&gt;][&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;controlPlaneMode&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;UNICAST_MODE&amp;#39;&lt;/span&gt;
nsx_manager_session&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;view_body_dict(new_ls)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once we have out body template correctly describing what we want we can post this and if all goes to plan create a new Logical Switch. In this example I am passing in the scopeId (transport zone) manually to keep it a simple example.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;new_ls_response &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nsx_manager_session&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;create(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;logicalSwitches&amp;#39;&lt;/span&gt;, uri_parameters&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;scopeId&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;vdnscope-1&amp;#39;&lt;/span&gt;}, request_body_dict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;new_ls)
nsx_manager_session&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;view_response(new_ls_response)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;It looks like with our new found friend the VMware Python libraries we can easily create and deploy a VMware configuration &amp;lsquo;infrastructure as code&amp;rsquo;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Docker Networking</title>
      <link>https://darrylcauldwell.com/post/docker-net/</link>
      <pubDate>Tue, 08 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/docker-net/</guid>
      <description>
        
          &lt;p&gt;Hear I describe Docker networking from the very basic through to extending it by pluggin in alternative drivers. In order to follow along with this post you will require Docker capability on your laptop to do this follow &lt;a href=&#34;https://docs.docker.com/engine/installation/&#34;&gt;these instructions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Mostly will be using &lt;a href=&#34;https://www.alpinelinux.org/&#34;&gt;Alpine Linux&lt;/a&gt; as it is very small so downloads quickly and doesn&amp;rsquo;t consume many resources.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker pull alpine
docker images
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Output from images command should list all images you hold locally including Alpine Linux&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;REPOSITORY  TAG     IMAGE ID        CREATED         SIZE
alpine      latest  baa5d63471ea    2 weeks ago     4.803 MB
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once we have the Alpine Linux image local we can then launch it,  if we do this with interactive and simulated TTY options we get presented with the Linux shell&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker run --interactive --tty alpine sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;From here we can view the docker containers view of the networking,  we see here it has allocated an rfc1918 address and that it can ping outbound.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;/ &lt;span style=&#34;color:#75715e&#34;&gt;# ip addr&lt;/span&gt;
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu &lt;span style=&#34;color:#ae81ff&#34;&gt;65536&lt;/span&gt; qdisc noqueue state UNKNOWN qlen &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
    valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
    valid_lft forever preferred_lft forever
16: eth0@if17: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&amp;gt; mtu &lt;span style=&#34;color:#ae81ff&#34;&gt;1500&lt;/span&gt; qdisc noqueue state UP 
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.2/16 scope global eth0
    valid_lft forever preferred_lft forever
    inet6 fe80::42:acff:fe11:2/64 scope link 
    valid_lft forever preferred_lft forever
/ &lt;span style=&#34;color:#75715e&#34;&gt;# ping 8.8.8.8&lt;/span&gt;
PING 8.8.8.8 &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;8.8.8.8&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;56&lt;/span&gt; data bytes
&lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt; bytes from 8.8.8.8: seq&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; ttl&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;61&lt;/span&gt; time&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;43.770 ms
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To detach the tty without exiting the shell, use the escape sequence Ctrl+p + Ctrl+q more details &lt;a href=&#34;https://docs.docker.com/engine/reference/commandline/attach/&#34;&gt;here&lt;/a&gt;.  Once exited you should still be able to see the container in the running state.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker ps -a

CONTAINER ID    IMAGE   COMMAND     CREATED             STATUS          PORTS   NAMES
5099910cc9d0    alpine  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sh&amp;#34;&lt;/span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt; minutes ago       Up &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt; minutes            elegant_torvalds
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you need to get back to the running shell you can use either the container id or the name, these two commands are equivelant.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker attach 5099910cc9d0
docker attach elegant_torvalds
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So now if we start a second instance of Apline Linux in container&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker run --interactive --tty alpine sh
docker ps -a

CONTAINER ID    IMAGE   COMMAND     CREATED             STATUS          PORTS   NAMES
e96c3e67410c    alpine  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sh&amp;#34;&lt;/span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt; seconds ago       Up &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt; seconds            suspicious_pare
5099910cc9d0    alpine  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sh&amp;#34;&lt;/span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt; minutes ago       Up &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt; minutes            elegant_torvalds
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can attach and detach to each,  we notice that the second instance has been assigned next IP 172.17.0.3 and that it can ping the first instance 172.17.0.2.&lt;/p&gt;
&lt;p&gt;If we then detach the containers and view what docker networking shows, we see that Docker uses the concept of Drivers and that we are using the default bridge driver.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker network ls

NETWORK ID          NAME                DRIVER
64481f7454d1        bridge              bridge              
a8f3b81d9991        host                host                
6ccbf074e447        none                null         
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can explore the properties of the bridge network a little more, and see the IP addresses it has issued to the containers.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker network inspect bridge

&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;bridge&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;64481f7454d138e43558359d1e41bb96564769bfcccc547b42b2a37b873c3a73&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Scope&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;local&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Driver&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;bridge&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;EnableIPv6&amp;#34;&lt;/span&gt;: false,
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;IPAM&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Driver&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Options&amp;#34;&lt;/span&gt;: null,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Config&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
                &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Subnet&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;172.17.0.0/16&amp;#34;&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;,
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Internal&amp;#34;&lt;/span&gt;: false,
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Containers&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{}&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;,
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Options&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;com.docker.network.bridge.default_bridge&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;com.docker.network.bridge.enable_icc&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;com.docker.network.bridge.enable_ip_masquerade&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;com.docker.network.bridge.host_binding_ipv4&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;0.0.0.0&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;com.docker.network.bridge.name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;docker0&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;com.docker.network.driver.mtu&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1500&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;,
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Labels&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{}&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So we can see that docker created a shared bridge, but what happens if you want to isolate containers on the same host well we can create a different bridge network we see it allocates the 172.18.0.0/16 CIDR.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker network create --driver bridge my-bridge-network
docker network inspect my-bridge-network

&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;my-bridge-network&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;5fec8c172a6c323a616c54938815b42d6baab04e816d7a5bb9113a0e914c52a4&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Scope&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;local&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Driver&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;bridge&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;EnableIPv6&amp;#34;&lt;/span&gt;: false,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;IPAM&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Driver&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;,
            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Options&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{}&lt;/span&gt;,
            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Config&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
                    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Subnet&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;172.18.0.0/16&amp;#34;&lt;/span&gt;,
                    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Gateway&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;172.18.0.1/16&amp;#34;&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Internal&amp;#34;&lt;/span&gt;: false,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Containers&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{}&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Options&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{}&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Labels&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{}&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So if we create a new container now on this new network, we see it gets IP address on 172.18.0.0/16 and it cannot ping containers on 172.17.0.0/16.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker run --interactive --tty --net&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;my-bridge-network alpine sh
/ &lt;span style=&#34;color:#75715e&#34;&gt;# ipaddr&lt;/span&gt;
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu &lt;span style=&#34;color:#ae81ff&#34;&gt;65536&lt;/span&gt; qdisc noqueue state UNKNOWN qlen &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
    valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
    valid_lft forever preferred_lft forever
25: eth0@if26: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&amp;gt; mtu &lt;span style=&#34;color:#ae81ff&#34;&gt;1500&lt;/span&gt; qdisc noqueue state UP 
    link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff
    inet 172.18.0.2/16 scope global eth0
    valid_lft forever preferred_lft forever
    inet6 fe80::42:acff:fe12:2/64 scope link 
    valid_lft forever preferred_lft forever
/ &lt;span style=&#34;color:#75715e&#34;&gt;# ping 172.17.0.3&lt;/span&gt;
PING 172.17.0.3 &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;172.17.0.3&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;56&lt;/span&gt; data bytes
^C
--- 172.17.0.3 ping statistics ---
&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt; packets transmitted, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; packets received, 100% packet loss
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We have focussed on the shipped bridge driver, however since &lt;a href=&#34;https://github.com/docker/libnetwork&#34;&gt;LibNetwork project&lt;/a&gt; got integrated into docker the driver became plugable. Some of the Docker networking ecosystems, include Weave, Nuage, Cisco, Microsoft, Calico, Midokura, and VMware.&lt;/p&gt;
&lt;p&gt;Upto now we have looked at networking between containers on the same host by bridging to the network interface card. You will probably want to extend the layer 2 networking capacibility so containers on different hosts can communicate. To do this we would use &lt;a href=&#34;https://docs.docker.com/swarm/networking/&#34;&gt;Docker Swarm&lt;/a&gt; which includes the multi-host networking feature and allows the creation of custom container networks which span multiple Docker hosts. By default it does this by use of the &amp;lsquo;overlay&amp;rsquo; network driver&lt;/p&gt;
&lt;p&gt;An interesting option to use as a Docker networking driver is &lt;a href=&#34;https://github.com/projectcalico/calico-containers&#34;&gt;Project Calico for Containers&lt;/a&gt; as the overlay networking expands across more and more hosts it can experience performance issues. Project Calico aims to over come this issue but allow container connectivity across hosts by having each container route directly at Layer 3 rather than Layer 2.  It does this by having a calico-node container installed and running on each host which manages the network routing etc.&lt;/p&gt;
&lt;p&gt;As in my previous post about &lt;a href=&#34;https://darrylcauldwell.com/install-vic/&#34;&gt;vSphere Integrated Containers&lt;/a&gt; its interesting to see VMware integrating with Docker and I&amp;rsquo;d expect it won&amp;rsquo;t be too long until we discover how NSX will plug into Docker Networking.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Safely Lockdown NSX Distributed Firewall (DFW) Ruleset</title>
      <link>https://darrylcauldwell.com/post/nsx-dfw-lockdown/</link>
      <pubDate>Tue, 05 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-dfw-lockdown/</guid>
      <description>
        
          &lt;p&gt;A common dilemma when developing a solution with firewall is whether to change the Default rule to Deny at the start and develop the ruleset as part of development or leave the Default rule to Allow and secure it later. In modern agile teams its best to develop the ruleset as part of development ensuring the ruleset is tested with the product as introducing it later could well invalidate every bit of testing performed.&lt;/p&gt;
&lt;p&gt;If however you find yourself in the situation where a NSX firewall solution is deployed with the Default rule to Allow and your asked to implement a ruleset to cover the traffic and change default to Deny. This is one possible solution to capture the required configuration.&lt;/p&gt;
&lt;h2 id=&#34;enable-default-rule-logging&#34;&gt;Enable Default Rule Logging&lt;/h2&gt;
&lt;p&gt;In order we can capture the active traffic we can first enable Logging on the default rule.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/dFW-EnableLogging.jpg&#34; alt=&#34;NSX Enable Logging&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can then operate the environment normally for a period of time which captures all business processes. This maybe a day, a week, a month or more.&lt;/p&gt;
&lt;h2 id=&#34;centralize-logging&#34;&gt;Centralize Logging&lt;/h2&gt;
&lt;p&gt;NSX data plane logging is written to the VMkernel.log files,  therefore if a logical firewall rule log is generated for a vNIC of a VM it is written to the ESX host log file which it was residing at that time.&lt;/p&gt;
&lt;p&gt;The distributed firewall configuration can apply to the whole vCenter and all objects within. You must therefore configure the remote syslog server for each host in each cluster that has firewall enabled. The remote syslog server is specified in the &lt;em&gt;Syslog.global.logHost&lt;/em&gt; attribute. My preference is to use vRealize Log Insight for centralized syslog.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/dFW-EnableRemoteLogging.jpg&#34; alt=&#34;NSX Remote Logging&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;identifying-traffic-hitting-default-allow&#34;&gt;Identifying Traffic Hitting Default Allow&lt;/h2&gt;
&lt;p&gt;When we browse the firewall log entries we find that in order to narrow our search to the correct rule we need to establish the &lt;em&gt;vmw_nsx_firewall_ruleid&lt;/em&gt; for the default layer 3 rule which we are logging. The &lt;em&gt;vmw_nsx_firewall_ruleid&lt;/em&gt; is not displayed via the NSX Firewall GUI but can be easily got from the NSX REST API by running the GET method on this URL&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;https://&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;nsx-mgr-ip&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;/api/4.0/firewall/globalroot-0/config?ruleType&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;LAYER3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/dFW-DefaultRuleID.jpg&#34; alt=&#34;NSX Default Rule ID&#34;&gt;&lt;/p&gt;
&lt;p&gt;We see in this example the &lt;em&gt;rule id=&amp;ldquo;1001&amp;rdquo;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Once we have this we can create a vRealize Log Insight query to list all logs generated by this rule.  To make this query easier to view I remove all columns except timestamp and vmw_nsx_firewall.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/dFW-LI-Query.jpg&#34; alt=&#34;Query Firewall Events Log Insight &#34;&gt;&lt;/p&gt;
&lt;p&gt;From this data we can then identify what traffic would be blocked if we changed the default rule to Deny. We can work through this data, identify valid traffic flows. We can then put in explicit allow rules for the valid traffic. When we are happy no traffic is being logged by the Default Rule we can change it to Deny.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Deploying NSX-V With Ansible</title>
      <link>https://darrylcauldwell.com/post/nsx-install-ansible/</link>
      <pubDate>Wed, 08 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-install-ansible/</guid>
      <description>
        
          &lt;p&gt;Here I will describe the steps taken to deploy NSX Manager using the &lt;a href=&#34;https://github.com/vmware/nsxansible&#34;&gt;Ansible NSX Module&lt;/a&gt;. The NSX Ansible module is written by VMware and is provided opensource on GitHub. To work properly this depends on the &lt;a href=&#34;https://github.com/vmware/nsxraml&#34;&gt;NSX RAML Specification&lt;/a&gt;, &lt;a href=&#34;https://github.com/vmware/nsxramlclient&#34;&gt;NSX RAML Python Client&lt;/a&gt;, &lt;a href=&#34;https://github.com/vmware/pyvmomi&#34;&gt;vSphere API Python Bindings&lt;/a&gt; and the &lt;a href=&#34;https://www.vmware.com/support/developer/ovf/&#34;&gt;OVF Tool&lt;/a&gt; all being installed on the Ansible server.&lt;/p&gt;
&lt;p&gt;The following assumes you have a working Ansible installation already, and a vSphere environmentto install NSX to. If you don&amp;rsquo;t yet have these you can see how I performed my &lt;a href=&#34;%7B%7Bsite.url%7D%7D/how-to-setup-an-ansible-test-lab-for-windows-managed-nodes-custom-windows-modules/&#34;&gt;Ansible Installation&lt;/a&gt; in this earlier blog post.&lt;/p&gt;
&lt;h2 id=&#34;nsx-raml-specification&#34;&gt;NSX RAML Specification&lt;/h2&gt;
&lt;p&gt;As the NSX REST API changes with each release the REST API Markup Language (RAML) specification for NSX is provided as a different branch of the GitHub repository.  In my environment I will be using NSX 6.2.2 so first I ensure the git client is installed to my Ansible server and then I use this to take a clone of the correct branch.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;yum install git-all
git clone -b 6.2.2 https://github.com/vmware/nsxraml.git /nsxraml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;nsx-raml-python-client&#34;&gt;NSX RAML Python Client&lt;/h2&gt;
&lt;p&gt;The NSX RAML python client is series of functions which can be used standalone or in our case called by the Ansible NSX module.  To install these we need to ensure the &amp;lsquo;Python Package Manger&amp;rsquo; and some other tools are installed and available on our Ansible server.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;yum install python-pip gcc libxslt-devel python-devel pyOpenSSL
pip install --upgrade pip
pip install lxml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once thes pre-requiste components are in place we can then install the NSX RAML python client. Similar to the RAML specification, the client functions are also dependant on version. The version which works with NSX RAML Specification 6.2.2 is Python client 1.0.4.  To install this version from Python package manager we use.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;pip install nsxramlclient&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1.0.4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you have NSX Manager already deployed you can create a session to this using the python client.  First start python by running&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;python
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;then you can run the commands similar to this to create a session.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; nsxramlclient.client &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; NsxClient
nsxraml_file &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;/nsxraml/nsxvapi.raml&amp;#39;&lt;/span&gt;
nsxmanager &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;nsx.darrylcauldwell.local&amp;#39;&lt;/span&gt;
nsx_username &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;admin&amp;#39;&lt;/span&gt;
nsx_password &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;VMware!&amp;#39;&lt;/span&gt;
client_session &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; NsxClient(nsxraml_file, nsxmanager, nsx_username, nsx_password, debug&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;False)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;you can then see the session by running the following command in python session&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;client_session
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-install-ansible-RAMLclient.jpg&#34; alt=&#34;NSX RAML Client&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;vmware-vsphere-api-python-bindings&#34;&gt;VMware vSphere API Python Bindings&lt;/h2&gt;
&lt;p&gt;As well as the NSX Python client the Ansible NSX Module also depends on the VMware vSphere API Python Bindings (pyVmomi. pyVmomi is the Python SDK for the VMware vSphere API that allows you to manage ESX, ESXi, and vCenter. This is similarly installed with the python package manager using command like.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;pip install pyvmomi
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;ovf-tool&#34;&gt;OVF Tool&lt;/h2&gt;
&lt;p&gt;The final thing to be installed for the NSX Module to operate correctly is the VMware OVF Tool. The OVF tool for Linux version 4.1.0 is &lt;a href=&#34;https://my.vmware.com/group/vmware/details?downloadGroup=OVFTOOL410&amp;amp;productId=491#&#34;&gt;available here&lt;/a&gt;, please note a VMware login is required to get this.&lt;/p&gt;
&lt;p&gt;Once downloaded to the Ansbile server, we need to ensure it has execute attribute and then execute it to start the install, the commands to do this for the current version 4.1.0 are.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;chmod +x VMware-ovftool-4.1.0-2459827-lin.x86_64.bundle
./VMware-ovftool-4.1.0-2459827-lin.x86_64.bundle
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-install-ansible-ovftool.jpg&#34; alt=&#34;OVFTool&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once the install is running you have to agree to EULA, to get to the end of the text, hold down the Space bar. When prompted &lt;em&gt;Do you agree?&lt;/em&gt; type yes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-install-ansible-ovftool2.jpg&#34; alt=&#34;OVFTool&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once OVF Tool is installed we can use SCP to copy the NSX OVA, VMware-NSX-Manager-6.2.2-3604087.ova, to Ansible server I placed this in folder named /OVAs.&lt;/p&gt;
&lt;h2 id=&#34;ansible-nsx-module&#34;&gt;Ansible NSX Module&lt;/h2&gt;
&lt;p&gt;We already have Git installed for dowlnloading the NSX RAML Specification so we can use this to clone the NSX Ansible repository.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;git clone https://github.com/vmware/nsxansible.git /nsxansible
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;deploy-nsx-manager&#34;&gt;Deploy NSX Manager&lt;/h2&gt;
&lt;p&gt;The NSX Module comes supplied with some example playbooks for performing common tasks, we’ll first take a copy of the example to deploy NSX Manager&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cp /nsxansible/test_deploynsxova.yml /nsxansible/darryl_deploynsxova.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can then edit the contents to match the variables we plan to deploy in environment. While most of playbook contents are environmental specific varibles its worth noting that we run this module against the Ansible server itself as this is where the OVA and ovftool are located so hosts: value will always be localhost.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-install-ansible-NSXmgr.jpg&#34; alt=&#34;Deploy NSX Manager&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once we have our environmental specific entries set we can execute the playbook with Ansible.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;ansible-playbook darryl_deploynsxova.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-install-ansible-playbook.jpg&#34; alt=&#34;Deploy NSX Manager Playbook&#34;&gt;&lt;/p&gt;
&lt;p&gt;We see this deploys ‘NSX Manager’ and configures the setting specified in the playbook.&lt;/p&gt;
&lt;h2 id=&#34;deploy-nsx-manager-and-register-with-vcenter-and-sso&#34;&gt;Deploy NSX Manager and register with vCenter and SSO&lt;/h2&gt;
&lt;p&gt;As with any Ansible playbook we can put common variables in a central location and call these from playbooks. An example is provided called answerfile-deployLab.yml. Variable names are overlapped between parts of the NSX Modules, in order that they are unique in the central answer file the names vary but its very easy to match these.&lt;/p&gt;
&lt;p&gt;An example of my playbook to deploy NSX Manager and then register this with vCenter and SSO.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-install-ansible-mgr-cfg.jpg&#34; alt=&#34;Deploy and Configure NSX Manager Playbook&#34;&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Ansibile NSX Configuration - Infrastructure As Code</title>
      <link>https://darrylcauldwell.com/post/nsx-ansible-iac/</link>
      <pubDate>Tue, 07 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-ansible-iac/</guid>
      <description>
        
          &lt;p&gt;With the rise of DevOps culture and the ethos of automate all the things. Using test driven development techniques to apply application configuration declaratively has become widespread. The ever closer working relations of developers and infrastructure operations staff to become an application centric delivery and management unit has lead to many benefits.  One of the benefits is the shared empathy for each others pain points and understanding of how each approaches tackling these. It is clear that managing configuration as code has many benefits for consistent repeated delivery of applications.&lt;/p&gt;
&lt;p&gt;Tools which are commonly used for deploying applications with declarative configuration include Puppet, Chef and Ansible. The processes for distributed source control are also mature and with Git, Bitbucket and Apache Subversion there is a DVCS for most use-cases. As we work as one delivery team sharing tools and repositories is natural as with this we can look to deploy our infrastructure and application configuration with a single tool and avoid issues with interoperability and hand off.&lt;/p&gt;
&lt;p&gt;VMware NSX offers many things to many people with this rich feature set comes complexity in configuration.  At VMworld Europe 2015 I was introduced to Yves Fauser and attended his session &lt;a href=&#34;https://vmworld2015.lanyonevents.com/connect/sessionDetail.ww?SESSION_ID=4972&amp;amp;tclass=popup&#34;&gt;‘NET4972 – Incorporating VMware NSX in your DevOps Toolchain – Network Programmability with Python and Ansible‘.&lt;/a&gt;  He had created an [NSX RAML specification]((&lt;a href=&#34;http://github.com/vmware/nsxraml),&#34;&gt;http://github.com/vmware/nsxraml),&lt;/a&gt; a &lt;a href=&#34;http://github.com/vmware/nsxramlclient.&#34;&gt;Python NSX RAML client&lt;/a&gt; he then brings these altogether into a usable form by way of an &lt;a href=&#34;https://github.com/vmware/nsxansible&#34;&gt;NSX Ansible module.&lt;/a&gt;  The &lt;a href=&#34;https://github.com/vmware/nsxansible&#34;&gt;Ansible module&lt;/a&gt; offers examples which give the ability to NSX Manager, configure NSX to vCenter integration, configure VXLAN, deploy NSX Controllers and then deploy some logical switches.&lt;/p&gt;
&lt;p&gt;I explorered this capability in [this follow up post.]({{ site.url }}/deploy-nsx-from-ansible)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Configuring NSX Edge to DLR OSPF</title>
      <link>https://darrylcauldwell.com/post/nsx-edge-ospf/</link>
      <pubDate>Mon, 23 May 2016 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-edge-ospf/</guid>
      <description>
        
          &lt;p&gt;As part of study for VCIX-NV I’ve given myself task of exploring in my new home lab all parts of NSX which I&amp;rsquo;m still not fully comfortable.  One of these things is OSPF,  to investigate this I came up with a test scenario and then worked through the steps to achieve a solution to meet the scenario design.&lt;/p&gt;
&lt;h2 id=&#34;scenario-design&#34;&gt;Scenario Design&lt;/h2&gt;
&lt;p&gt;We will replicate a typical development environment secured with an Edge Gateway.  Within the environment the developers will have ability to add and remove logical switches to the DLR and the automatic updating of the Edge Gateway routing table.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-test.jpg&#34; alt=&#34;NSX Edge OSPF Test&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;deploy-edge-transit-logical-switch-and-distributed-logical-router&#34;&gt;Deploy Edge, Transit Logical Switch and Distributed Logical Router&lt;/h2&gt;
&lt;p&gt;The lab has VXLAN capability during my NSX core home lab configuration, so has hosts prepared, VNI pool and deployed Controller.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll first deploy the Transit logical switch.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-ls.jpg&#34; alt=&#34;NSX Edge Logical Switch&#34;&gt;&lt;/p&gt;
&lt;p&gt;Create Edge Gateway with an Uplink interface on the OOB network  (192.168.1.43) and on the Transit network (192.168.2.1) .&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-edge-deploy.jpg&#34; alt=&#34;NSX Edge Deploy&#34;&gt;&lt;/p&gt;
&lt;p&gt;Create DLR initially with a single uplink on Transit network (192.168.2.254) , do not configure a default gateway.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-dlr.jpg&#34; alt=&#34;NSX Edge DLR&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;configure-ospf-area&#34;&gt;Configure OSPF Area&lt;/h2&gt;
&lt;p&gt;An OSPF area is a logical collection of OSPF networks, routers, and links that have the same area identification. The simulated development environment we are using RFC1918 address space in order that the IP address schema can overlap as such our OSPF routing will be internal so we will create an OSPF area which is type Stub.&lt;/p&gt;
&lt;p&gt;On Edge Gateway Enable Dynamic Routing Configuration on the Transit interface.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-dynamic.jpg&#34; alt=&#34;NSX Edge OSPF Dynamic&#34;&gt;&lt;/p&gt;
&lt;p&gt;Then we need to enable OSPF&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-enable.jpg&#34; alt=&#34;NSX Edge OSPF Enable&#34;&gt;&lt;/p&gt;
&lt;p&gt;We require a stub OSPF area, and usefully Area ID 51 is configured already so we&amp;rsquo;ll use this.  We just need to associate this with the Transit interface in order it can communicate with DLR.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-mapping.jpg&#34; alt=&#34;NSX Edge OSPF Mapping&#34;&gt;&lt;/p&gt;
&lt;p&gt;Repeat the steps used for configuring OSPF on the Edge on the DLR, when enabling OSPF enter the forwarding address to match the Transit network IP interface and the protocol address to be another available IP address on the Transit network.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-ProFwd.jpg&#34; alt=&#34;NSX Edge OSPF Protocol Forward&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;route-distribution&#34;&gt;Route Distribution&lt;/h2&gt;
&lt;p&gt;The default Route Redistribution configuration for DLR is to distribute information about networks it is connected to.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-RouteDistribute.jpg&#34; alt=&#34;OSPF Route Redistribution&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we now check the Edge Gateway routing table we can see all routing is of type C (directly connected) or S (static entry for default gateway).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-RouteTable.jpg&#34; alt=&#34;OSPF Route Table&#34;&gt;&lt;/p&gt;
&lt;p&gt;If I now simulate a developer creating a new Logical Switch for network 192.168.3.0/24 which is attached to DLR as internal interface 192.168.3.1.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-test1DLR.jpg&#34; alt=&#34;OSPF Test 1 DLR&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can then check the Edge Gateway routing table again.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-test1DLR.jpg&#34; alt=&#34;OSPF Test 1 Edge&#34;&gt;&lt;/p&gt;
&lt;p&gt;We see a new route added type O (ospf derived) N2 (OSPF NSSA external type 2).  Which directs this traffic to the DLR interface on Transit network.&lt;/p&gt;
&lt;p&gt;If we then simulate the developer adding the test2 and test3 networks. We see the other routes populated.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-test2-3.jpg&#34; alt=&#34;OSPF Test 2-3&#34;&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>NSX Backup and Restore</title>
      <link>https://darrylcauldwell.com/post/nsx-backup-restore/</link>
      <pubDate>Fri, 20 May 2016 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-backup-restore/</guid>
      <description>
        
          &lt;p&gt;As part of study for VCIX-NV I&amp;rsquo;ve given myself task of exploring in my new home lab all parts of NSX which I don&amp;rsquo;t use at work. One of these things is NSX Backup and Restore,  its not that we don&amp;rsquo;t use these function but that its relatively high impacting if it goes wrong.  To investigate this I came up with a list of networking configuration to test backup and restore of and the steps to I followed to perform each backup and restore type.&lt;/p&gt;
&lt;h2 id=&#34;scenario-1---distributed-firewall&#34;&gt;Scenario 1 - Distributed Firewall&lt;/h2&gt;
&lt;p&gt;To backup and restore distributed firewall rules is relatively straight forwards to do.  I&amp;rsquo;m starting with no rules created.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-BlankRuleset.jpg&#34; alt=&#34;NSX Firewall Blank Ruleset&#34;&gt;&lt;/p&gt;
&lt;p&gt;Then create a new section.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-BandR.jpg&#34; alt=&#34;NSX Firewall New Section&#34;&gt;&lt;/p&gt;
&lt;p&gt;Add some rules for example any to WEB on HTTP Service and WEB to DB on SQL Service.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-testRules.jpg&#34; alt=&#34;NSX Firewall New Rules&#34;&gt;&lt;/p&gt;
&lt;p&gt;Click Export and save these as a file.&lt;/p&gt;
&lt;p&gt;Delete the WEB to DB rule.&lt;/p&gt;
&lt;p&gt;To perform a restore we do this from the Saved Configuration tab,  we see that there are only auto saved backups.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-Restore1.jpg&#34; alt=&#34;NSX Firewall Auto-Saved&#34;&gt;&lt;/p&gt;
&lt;p&gt;In order to restore from the file backup we need to import this into the Saved Configurations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-Restore2.jpg&#34; alt=&#34;NSX Firewall Import Saved&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now we can change back to the Configuration tab and use &amp;lsquo;Load Saved Configuration&amp;rsquo; option to restore the exported file based backup.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-Restore3.jpg&#34; alt=&#34;NSX Firewall Restore&#34;&gt;&lt;/p&gt;
&lt;p&gt;The deleted rule then gets added back.&lt;/p&gt;
&lt;h2 id=&#34;scenario-2---nsx-manager&#34;&gt;Scenario 2 - NSX Manager&lt;/h2&gt;
&lt;p&gt;To backup and restore NSX Manager it depends on having access to a FTP or SFTP to store the configuration in.&lt;/p&gt;
&lt;p&gt;Previously I&amp;rsquo;d created a CentOS7 virtual machine template,  I deployed one of these and then followed this procedure to configure FTP.&lt;/p&gt;
&lt;p&gt;Configure the FTP server settings in NSX.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-nsxBackup.jpg&#34; alt=&#34;NSX Manager Configuration Backup&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once backup location is configured click Backup to create a backup.&lt;/p&gt;
&lt;p&gt;Check the FTP server and it should have a folder called nsx created and within a file prefixed with nsx followed by the date and time stamp.&lt;/p&gt;
&lt;p&gt;Make a change to NSX configuration,  for example assign the NSX Manager the primary role.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-PrimaryRole.jpg&#34; alt=&#34;NSX Manager Assign Primary Role&#34;&gt;&lt;/p&gt;
&lt;p&gt;Check role is applied.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-Primary.jpg&#34; alt=&#34;NSX Manager Is Primary&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we now restore the configuration from the NSX Backup we should see the NSX Manager revert to running the Standalone role.&lt;/p&gt;
&lt;p&gt;Select the backup created prior to making the configuration change and click Restore.  The NSX Manager will restore and then restart the Management Services so will take a minute or two. We can then check the role is reverted to Standalone.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-Standalone.jpg&#34; alt=&#34;NSX Manager Is Standalone&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;scenario-3---vsphere-distributed-switch&#34;&gt;Scenario 3 - vSphere Distributed Switch&lt;/h2&gt;
&lt;p&gt;I have in lab a vSphere Distributed Switch configured with some Distributed Portgroups and Logical Switches created on.  Otherwise you might need to create some configuration.&lt;/p&gt;
&lt;p&gt;Use the context menu of the vDistributed Switch to Export the configuration to a file.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-Export-vDS.jpg&#34; alt=&#34;Export VDS&#34;&gt;&lt;/p&gt;
&lt;p&gt;This creates a compressed zip file,  most files within this are not readable except for an xml file which appears to be a manifest of the zip file.&lt;/p&gt;
&lt;p&gt;To test that the configuration is restored add an additional distributed portgroup named TestRestore1.&lt;/p&gt;
&lt;p&gt;Use the context menu of the vDistributed Switch to Restore the configuration from the file.  Interesting we find that the Portgroup created post backup is not removed.&lt;/p&gt;
&lt;p&gt;If we export the configuration again to include the new TestRestore1 Portgroup. Then delete the TestRestore1 Portgroup. Then Restore the configuration we find the TestRestore1 Portgroup is added back.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-TestRestore.jpg&#34; alt=&#34;Restored VDS&#34;&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>NSX-V Load Balancer As A Service LBaaS</title>
      <link>https://darrylcauldwell.com/post/nsx-lbaas/</link>
      <pubDate>Fri, 20 May 2016 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-lbaas/</guid>
      <description>
        
          &lt;p&gt;As part of study for VCIX-NV I&amp;rsquo;ve given myself task of exploring in my homelab all parts of NSX which I don&amp;rsquo;t use
at work. One of these things is NSX Load Balancer as a Service (LBaaS), to investigate this I came up with a test
scenario and then worked through the steps to achieve a solution to meet the design.&lt;/p&gt;
&lt;h2 id=&#34;load-balancer-scenariodesign&#34;&gt;Load Balancer Scenario Design&lt;/h2&gt;
&lt;p&gt;I started by reading the &lt;a href=&#34;http://pubs.vmware.com/NSX-62/index.jsp#com.vmware.nsx.admin.doc/GUID-412635AE-1F2C-4CEC-979F-CC5B5D866F53.html&#34;&gt;NSX Administration Guide&lt;/a&gt; section covering load balancer configuration.&lt;/p&gt;
&lt;p&gt;Here is what I plan to achieve my client and browser will sit on the 192.168.1.0/24 out of band network.  I&amp;rsquo;ll create a new WEB VLAN 1000 to host 192.168.2.0/24 network,  the Edge Gateway will serve this network with DHCP and DHCP will issue gateway IP to be its own address,  two web server VMs will be assigned to this network.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-onearm.jpg&#34; alt=&#34;One Armed Load Balancer&#34;&gt;&lt;/p&gt;
&lt;p&gt;A destination network address translation (DNAT) rule will be configured to pass this OOB network IP address to the load balancerd address on the WEB network. The load balancer will round robin traffic between the two web servers. Port 80 will be monitored on web servers and when apache is stopped on one server all traffic should direct to the active server. The Edge firewall will be configured to block all traffic except for HTTP to the web servers.&lt;/p&gt;
&lt;h2 id=&#34;configure-underlay&#34;&gt;Configure Underlay&lt;/h2&gt;
&lt;p&gt;Create VLAN 1000 on physical switch and ensure this is allowed in the trunk presented to the virtual distributed switch ports.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-VLAN1000-Assignment.jpg&#34; alt=&#34;VLAN 1000 Assignment&#34;&gt;&lt;/p&gt;
&lt;p&gt;Create a distributed portgroup for VLAN1000 and OOB.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-vDS-OOB-and-WEB.jpg&#34; alt=&#34;Distributed Portgroup&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;deploy-edge-gateway&#34;&gt;Deploy Edge Gateway&lt;/h2&gt;
&lt;p&gt;Create a Compact NSX Edge Gateway with an &lt;em&gt;Uplink&lt;/em&gt; interface on the OOB with IP address 192.168.1.40 and &lt;em&gt;Internal&lt;/em&gt; interface on the WEB - VLAN1000 with IP address 192.168.2.254 .  Specify 192.168.1.254 as default gateway on the OOB interface. Ensure firewall is set to accept as default policy.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-EdgeSummary.jpg&#34; alt=&#34;Edge Deployment Summary&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;create-dnat-rule&#34;&gt;Create DNAT Rule&lt;/h2&gt;
&lt;p&gt;As 192.168.2.0/24 is not a network my home router is aware of I must create a Destination NAT rule on 192.168.1.0/24 network to map to the 192.168.2.0/24 address.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-DNAT.jpg&#34; alt=&#34;Destination NAT&#34;&gt;&lt;/p&gt;
&lt;p&gt;Ensure this is created by connecting SSH session to the Edge and viewing the NAT configuration by running&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;show nat
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-NAT-CLI.jpg&#34; alt=&#34;Destination NAT&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;configure-web-server-dhcp&#34;&gt;Configure Web Server DHCP&lt;/h2&gt;
&lt;p&gt;Enable DHCP and create a DHCP scope 192.168.2.10-192.168.2.50.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-DHCP.jpg&#34; alt=&#34;DHCP&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;create-web-servers&#34;&gt;Create Web Servers&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;ll look to create two test web servers each will host a single web page with its actual IP address on.  When I connect to load balanced IP the page contents returned will flip between the two servers.&lt;/p&gt;
&lt;p&gt;I created a &lt;a href=&#34;https://darrylcauldwell.com/post/vsphere-vm-templates&#34;&gt;CentOS 7 VM template&lt;/a&gt; previously in lab and so for this test scenario I provisioned two new VMs from this named web1 and web2.  I left both as DHCP and setup the web server using following commands.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;yum install httpd -y
service httpd start
ifconfig | grep &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;inet&amp;#34;&lt;/span&gt; &amp;amp;gt; /var/www/html/test.html
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;configure-load-balancer&#34;&gt;Configure Load Balancer&lt;/h2&gt;
&lt;p&gt;Enable the load balance function on the Edge.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-LoadBalancer-Enable.jpg&#34; alt=&#34;DHCP&#34;&gt;&lt;/p&gt;
&lt;p&gt;Create a load balancer application profile called Web and mark is for HTTP traffic.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-LoadBalancer-AppProfile.jpg&#34; alt=&#34;Load Balancer AppProfile&#34;&gt;&lt;/p&gt;
&lt;p&gt;Add web1 and web2 virtual machines to a load balancer pool with monitor port 80.  If adding as VM object as per screenshot then you might have IPv6 issue see section below for solution,  alternatively enter IP address.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-LoadBalancer-PoolMembers.jpg&#34; alt=&#34;Load Balancer Pool Membership&#34;&gt;&lt;/p&gt;
&lt;p&gt;Create a load balancer virtual server to link all the parts into a VIP.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-WebLB.jpg&#34; alt=&#34;Load Balancer Virtual Server&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once this is created you will be able to browse to http://192.168.1.40/test.html on the OOB network this will return a web page displaying IP address.  Refreshing the page will display a similar page but with alternate VM IP address.&lt;/p&gt;
&lt;h2 id=&#34;nsx-loadbalancer-ipv6-issue&#34;&gt;NSX LoadBalancer IPv6 Issue&lt;/h2&gt;
&lt;p&gt;I found when I configured this initially by selecting VM objects in the Virtual Server the loadbalancer tried to configure all IP addresses including the IPv6 addresses which are present by default in Centos7.  This caused a error in LoadBalancer and wouldn&amp;rsquo;t allow me to proceed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-LoadBalance-IPv6.jpg&#34; alt=&#34;Load Balancer Virtual Server&#34;&gt;&lt;/p&gt;
&lt;p&gt;I disabled IPv6 on both CentOS7 VMs by adding the following to /etc/sysctl.conf:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;net.ipv6.conf.all.disable_ipv6 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
net.ipv6.conf.default.disable_ipv6 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then run the following to commit the changes.  When VMs had only IPv4 addresses the process works without error.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;sysctl -p
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;secure-solution&#34;&gt;Secure Solution&lt;/h2&gt;
&lt;p&gt;Up to now we have had the Edge firewall default rule set to allow,  one of the first tasks we would want to do once web load balancing is working is to secure the solution.&lt;/p&gt;
&lt;p&gt;To do this we create a new rule explicitly allowing HTTP traffic to the IP address it will enter the Edge on,  and once in place change default rule to Deny.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-LB-Firewall.jpg&#34; alt=&#34;Load Balancer Firewall&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once in place test you still have access to the web page.&lt;/p&gt;
&lt;h2 id=&#34;load-balance-monitoring&#34;&gt;Load Balance Monitoring&lt;/h2&gt;
&lt;p&gt;When creating the Loadbalance configuration we specified to monitor port 80 was available in order to include the server in the balanced set. Before proceeding here ensure that when you connect to 192.168.1.40 and refresh load balancing is working correctly and returning IP addresses alternately.&lt;/p&gt;
&lt;p&gt;We then stop the apache service on one of the VMs using:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;service httpd stop
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-apache-stop.jpg&#34; alt=&#34;Apache Stop&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we now go back to browser and refresh a few times it will direct all traffic to one server IP address.  If we restart apache using&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;service httpd start
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and refresh the browser we find load balancing is returned and refreshes alternate the IP addresses.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>NSX Activity Monitoring</title>
      <link>https://darrylcauldwell.com/post/nsx-activity/</link>
      <pubDate>Thu, 19 May 2016 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-activity/</guid>
      <description>
        
          &lt;p&gt;As part of study for VCIX-NV I&amp;rsquo;ve given myself task of exploring all parts of NSX which I don&amp;rsquo;t use at work. One of these things is NSX Activity Monitoring,  to investigate this I came up with a test scenario and then worked through the steps to achieve a solution to meet the scenario design.&lt;/p&gt;
&lt;h2 id=&#34;scenario-design&#34;&gt;Scenario Design&lt;/h2&gt;
&lt;p&gt;Gather traffic data between Activity Monitored virtual machines.&lt;/p&gt;
&lt;h2 id=&#34;virtual-machine-configuration&#34;&gt;Virtual Machine Configuration&lt;/h2&gt;
&lt;p&gt;I created a Windows 2012 R2 virtual machine template previously,  deploy two of these to a network with DHCP enabled for example in my lab the OOB network.&lt;/p&gt;
&lt;p&gt;The Windows template I created has the VMware Tools typical installation performed. To use the NSX Activity Monitoring the Network File Introspection Driver (vnetflt.sys) is required.  To add this we need to re-run the VMware tools installer,  select modify and then add NSX Network File Introspection Driver.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-activity-WindowsDriver.jpg&#34; alt=&#34;Install Windows Driver&#34;&gt;&lt;/p&gt;
&lt;p&gt;Add IIS role to one of the hosts.&lt;/p&gt;
&lt;p&gt;Add server to Active Directory domain and reboot.&lt;/p&gt;
&lt;p&gt;In vSphere web client select each virtual machine in turn and enable NSX Activity Monitoring.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-activity-EnableMonitoring.jpg&#34; alt=&#34;Enable Monitoring&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;deployactivity-monitor-appliances&#34;&gt;Deploy Activity Monitor Appliances&lt;/h2&gt;
&lt;p&gt;From the Networking and Security menu, select Installation. Navigate to the Service Deployments tab and glick green add button.&lt;/p&gt;
&lt;p&gt;In order to gather activity data virtual appliances are required to be deployed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-activity-DeployActivityServices.jpg&#34; alt=&#34;Deploy Virtual Appliances&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;register-a-windows-domain-with-nsx-manager&#34;&gt;Register a Windows Domain with NSX Manager&lt;/h2&gt;
&lt;p&gt;In vSphere web client click Networking and Security and then click NSX Managers. Click an NSX Manager in the Name column and then click the Manage tab. Click the Domain tab and then click the Add domain (Add domain) icon.&lt;/p&gt;
&lt;p&gt;Complete the domain details as per your Active Directory.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-activity-Add-Domain.jpg&#34; alt=&#34;Add Domain&#34;&gt;&lt;/p&gt;
&lt;p&gt;Wait for synchronization status to show SUCCESS.&lt;/p&gt;
&lt;h2 id=&#34;generate-activity&#34;&gt;Generate Activity&lt;/h2&gt;
&lt;p&gt;Connect to test virtual machine which &lt;!-- raw HTML omitted --&gt;does not&lt;!-- raw HTML omitted --&gt; have IIS installed and open browser to virtual machine with IIS installed.&lt;/p&gt;
&lt;p&gt;Test data is being gathered by Activity Monitor.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-activity-ActivityMonitorActivity.jpg&#34; alt=&#34;Activity Monitor Activity&#34;&gt;&lt;/p&gt;
&lt;p&gt;Another easy test is to copy a file between the two virtual machines.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-activity-ActivityMonitorSMB-CIFS.jpg&#34; alt=&#34;Activity Monitor Activity&#34;&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>NSX Distributed Firewall Under The Covers</title>
      <link>https://darrylcauldwell.com/post/nsx-dfw/</link>
      <pubDate>Tue, 21 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-dfw/</guid>
      <description>
        
          &lt;p&gt;An NSX distributed firewall (dFW) runs as an ESXi host as a kernel module added as a VMware Installation Bundle (VIB). The dFW rules operate on Layer 2 through Layer 4; although this can be extended through Layer 7 by integrating with a 3-Party vendor.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;L2 rules are based on MAC address L2 protocols like ARP, RARP and LLDP etc.&lt;/li&gt;
&lt;li&gt;L3 and 4 rules are based on IP source/destination and L4 uses a TCP or UDP ports.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/dFW_TCP_OSI.png&#34; alt=&#34;TCP OSI&#34;&gt;&lt;/p&gt;
&lt;p&gt;The NSX dFW enforces a state full firewall service for VMs using the vNIC as the enforcement point. Every packet that leaves the VM (before VTEP encapsulation) or enters the VM (After VTEP de-encapsulation) can be inspected with a firewall policy.&lt;/p&gt;
&lt;p&gt;The ruleset is created and managed via NSX Manager either API or UI. The ESXi host has two dFW specific modules vShield Statefull Firewall and VMware Internetworking Service Insertion Platform (vSIP). vShield Statefull Firewall performs the following roles.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interact with NSX Manager to retrieve DFW policy rules.&lt;/li&gt;
&lt;li&gt;Gather DFW statistics information and send them to the NSX Manager.&lt;/li&gt;
&lt;li&gt;Send audit logs information to the NSX Manager.&lt;/li&gt;
&lt;li&gt;Receive configuration from NSX manager to create (or delete) DLR Control VM, create (or delete) ESG.&lt;/li&gt;
&lt;li&gt;Part of the host preparation process SSL related tasks from NSX manager&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;VMware Internetworking Service Insertion Platform is the distributed firewall kernel space module core component. The vSIP receives firewall rules via vShield State-full Firewall and downloads them down to each VMware-sfw. When VM connect to Logical switch there are security services each packet transverse which are implemented as IOChains processed at the Kernel level.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/dFW_Slots.png&#34; alt=&#34;TCP OSI&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Slot 0: Distributed Virtual Filter (DVFilter): It monitors the incoming and outgoing traffic on the protected virtual NIC and performs stateless filtering.&lt;/li&gt;
&lt;li&gt;Slot 1: Switch Security module (SW-sec): Learns VMs IP and MAC address. sw-sec is critical component capture DHCP Ack and ARP broadcast message and forward this info as unicast to NSX Controller to perform the ARP suppression feature. This is also the layer where NSX IP spoof guard is implemented.&lt;/li&gt;
&lt;li&gt;Slot 2: NSX Distributed Firewall (VMware-sfw): This is the place where DFW firewall rules are stored and enforced; VMware-sfw contains rules table and connections table received via vShield State-full Firewall&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The effect of the processing of these packets is that packet leaving the VM which doesn’t match firewall rules get removed before arriving at the vSphere Switch.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Optimal TCP Window Size For Long Distance WAN Link</title>
      <link>https://darrylcauldwell.com/post/tcp-optimal/</link>
      <pubDate>Thu, 24 Jul 2014 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/tcp-optimal/</guid>
      <description>
        
          &lt;p&gt;If you have a requirement to copy large amounts of data along way around the world you may find that despite your link being 60Mb/s if its 5,000 miles away you only can transfer files at much less like 10Mb/s. The cause of this is generally the TCP Window Size is optimized by OS and FTP clients by default to work on networks with less distance and less network round trip latency.&lt;/p&gt;
&lt;p&gt;When using TCP to transfer data the two most important factors are the TCP window size and the round trip latency.  If you know the TCP window size and the round trip latency you can calculate the maximum possible throughput of a data transfer between two hosts, regardless of how much bandwidth you have.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;TCP-Window-Size-in-bits / Latency-in-seconds = Bits-per-second-throughput
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The standard Windows TCP window is 64KB&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;64KB = 65536 Bytes = 524288 bits
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;To calculate the round trip delay time can be done in many ways, the easiest is to use ping.exe as this records RTD in ms.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;images/tcp-rtd.gif&#34; alt=&#34;Round Trip Delay&#34;&gt;&lt;/p&gt;
&lt;p&gt;In the above the average latency is 38ms.  So using formula we can calculate the theoretical maximum.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;524288 / 0.038 = 13797053 bp/s = 13.8 Mbp/s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So you might want to know what you can do to make it faster? As with any equation the answer is adjust part so either increase the TCP window size and/or reduce latency. In many ways the latency is not something we can control easily but we can easily adjust the TCP window size.  If we know the latency and we know the size of pipe we have we should be able to calculate the best TCP window size.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(Link speed * Latency / 8) = TCP Window size Bytes
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So if we have a 60Mb/s link across a route 5,042 miles incurring latency of 173ms.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;(60000000*0.173)/8 = 1297500
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The downside to increasing the TCP window size on your server is that these larger packets increase the buffer size, this buffer sits in memory. Also if larger packets are lost, then more data is lost and therefore more needs retransmitting.&lt;/p&gt;

        
      </description>
    </item>
    
  </channel>
</rss>
