<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>netapp on </title>
    <link>https://darrylcauldwell.github.io/tags/netapp/</link>
    <description>Recent content in netapp on </description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 12 Dec 2014 00:00:00 +0000</lastBuildDate><atom:link href="https://darrylcauldwell.github.io/tags/netapp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>NetApp Cloud OnTap For AWS</title>
      <link>https://darrylcauldwell.github.io/post/aws-ontap/</link>
      <pubDate>Fri, 12 Dec 2014 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/aws-ontap/</guid>
      <description>
        
          &lt;p&gt;NetApp has often been a pioneer of bleeding edge concepts in storage. When designing data ONTAP and the WAFL file system they were the first to look at creating a virtual control plane for data. They also pioneered fully non disruptive upgrades when the introduced clustered data ONTAP.  They then introduced ONTAP Edge for providing the really cool features of data ONTAP for non NetApp SANs. Recently the pioneering spirit has been continuing with the launch of Cloud ONTAP for AWS which takes the clustered data ONTAP features to overlay Amazon EC2 storage.&lt;/p&gt;
&lt;p&gt;There are hundreds of business use cases which spring to mind when you begin to consider what this offers, however the total killer feature for this is using snap mirror to extend your data between private cloud and public cloud.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/aws-ontap.jpg&#34; alt=&#34;NetApp Cloud OnTap for AWS&#34;&gt;&lt;/p&gt;
&lt;p&gt;Using the public cloud makes great sense for standing up solutions quickly and without the up front capital expenditure and instead the operational usage charging. As your new product grows and becomes more popular it maybe your operational charges out weight the cost savings and you then want to bring it to private cloud.&lt;/p&gt;
&lt;p&gt;Most applications have great agility for the application, simply stand up a new OS in other cloud provider and install application.  However application data agility is, until now, much harder to manage.&lt;/p&gt;
&lt;p&gt;In this example you would simply build your private cloud and new application tier and use snap mirror to bring your data to your datacenter and keep it in sync with the application still running.  When all sync&amp;rsquo;d you would cutover by breaking the mirror all could be seam less to your customers.&lt;/p&gt;
&lt;p&gt;I expect as well that while currently this is only for EC2 storage, if it was to also interface into S3 and glacier you could also move your data tiering&lt;/p&gt;
&lt;p&gt;While I love NetApp I&amp;rsquo;m not a storage admin,  and while the ONTAP simulator is useful at $1.50 per hour if I ever wanted to exercise my skills I could just spin up a clustered ONTAP AMI and away I would go then release it to save the recurring charge.&lt;/p&gt;
&lt;p&gt;While there is more detailed info on the &lt;a href=&#34;http://www.netapp.com/us/system/pdf-reader.aspx?cc=us&amp;amp;m=ds-3618.pdf&amp;amp;pdfUri=tcm:10-127470&#34;&gt;NetApp website&lt;/a&gt;, a superb way to see this in action is to sign up for free and look at &lt;a href=&#34;https://poc.netapp.com/cloud/testdrive/&#34;&gt;test drive&lt;/a&gt;where you get to spin up a AMI on NetApp&amp;rsquo;s AWS account and look around.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>NetApp FAS IOPS Theoretical Maximum &amp; Workload Aggregate Load Balancing</title>
      <link>https://darrylcauldwell.github.io/post/netapp-iops/</link>
      <pubDate>Tue, 26 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/netapp-iops/</guid>
      <description>
        
          &lt;p&gt;To achieve good performance for a virtualized workload your Storage Area Network (SAN) should deliver high IOps with a predictable low latency. If you are using a good flash system where IOps (Input/Output Operations Per Second) effectively become an unlimited resource then latency will probably be your limiting factor.&lt;/p&gt;
&lt;p&gt;The NetApp FAS family of storage systems offer a series of arrays at various price points and specifications. NetApp also offer the option of adding Performance Acceleration Module (PAM) to improve the performance of random read intensive workloads by way of a tunable cache to reduce latency. the model you have will shape the latency capability for example the &lt;a href=&#34;https://www.spec.org/sfs2008/results/res2009q1/sfs2008-20081215-00111.html&#34;&gt;FAS3140 with PAM and FC disk gives a latency at 40,000 IOPs of 4.4ms&lt;/a&gt; where the &lt;a href=&#34;https://www.spec.org/sfs2008/results/res2009q3/sfs2008-20090727-00126.html&#34;&gt;FAS3160 with PAM and FC disk gives a latency at 40,000 IOPs of 1.7ms.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You can add varying amounts of disk shelves and disks to the various FAS models. The amount of disk and specification of disk and aggregate design will effect the theoretical maximum amount of IOps. Duncan Epping wrote &lt;a href=&#34;http://www.yellow-bricks.com/2009/12/23/iops/&#34;&gt;this piece&lt;/a&gt; on the calculation of IOps and if we use this in an example. We have 90 disk per storage controller and we form 3 aggregates each with 30 SAN 15K disks. Within the aggregate we follow &lt;a href=&#34;https://communities.netapp.com/message/2676%20&#34;&gt;NetApp best practice&lt;/a&gt; to form two RAID groups of no more than 16x disks and so form 2x RAID-DP RAID groups within each aggregate which consumes 2×2 disks for parity.  We know from using vscsiStats that today our VMs generate approximatley 60% read and 40% write traffic,  so our formula would read for each aggregate ((26*175)&lt;em&gt;0.6)+((26&lt;/em&gt;85)&lt;em&gt;0.4) = 3614 IOps per aggregate.  If we then add a PAM card the cache would potentially absorb some of IOps going as disk reads, this is very worload dependant but if we look on &lt;a href=&#34;https://communities.netapp.com/servlet/JiveServlet/download/3053-2273/TechONTAP_PAM.ppt&#34;&gt;page 9 of this NetApp powerpoint&lt;/a&gt; we can see the 256GB PAM absorbing 2000 of the 5847 disk reads,  around 30% which would effect our formula to (((26&lt;/em&gt;175)*0.6)&lt;em&gt;1.3)+((26&lt;/em&gt;85)*0.4) and give 4433 IOps per aggregate.&lt;/p&gt;
&lt;p&gt;It is important to understand the relationship between IOps and the disks as we can see in our example each storage controller has 3x islands of IOps.  As such workload can become hot in one aggregate and cool in others,  to ensure your driving your system the most efficient way you should monitor the average IOps in each aggregate and can look to balance these.  You can use either svMotion to move VM workload between datastores in different aggregates or data motion to move volumes between aggregates.&lt;/p&gt;

        
      </description>
    </item>
    
  </channel>
</rss>
