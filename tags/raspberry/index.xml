<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>raspberry on </title>
    <link>https://darrylcauldwell.github.io/tags/raspberry/</link>
    <description>Recent content in raspberry on </description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 30 Mar 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://darrylcauldwell.github.io/tags/raspberry/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Pi Supercomputer</title>
      <link>https://darrylcauldwell.github.io/post/homelab-pi-mpi/</link>
      <pubDate>Tue, 30 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/homelab-pi-mpi/</guid>
      <description>
        
          &lt;p&gt;Following on from &lt;a href=&#34;https://darrylcauldwell.github.io/post/homelab-pi&#34;&gt;base Ubuntu build&lt;/a&gt; here I begin to look at Parrallel Programming.&lt;/p&gt;
&lt;h2 id=&#34;parrallel-programming&#34;&gt;Parrallel Programming&lt;/h2&gt;
&lt;p&gt;Each Raspberry Pi is a small unit of compute, one of my goals is to understand how operating many in a cluster. There are various approaches to parallel computational models, message-passing has proven to be an effective one. MPI the Message Passing Interface, is a standardized and portable message-passing system designed to function on a wide variety of parallel computers.&lt;/p&gt;
&lt;p&gt;My prefered language is Python and usefully there is &lt;a href=&#34;https://mpi4py.readthedocs.io/en/stable/install.html&#34;&gt;MPI for Python&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt-get install -y libopenmpi-dev python-dev pip
sudo pip install mpi4py
mpirun --version
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;All the RPIs in the cluster will access each other via SSH, this communication needs to be passwordless. The first thing you need to do is generate an SSH key pair on first host.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh-keygen -t rsa -b 4096
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once key generated to enable passwordless access, upload a copy of the public key to the other three servers.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ssh-copy-id ubuntu@[server_ip_address]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Repeat keygen and copy public key process on all four hosts.&lt;/p&gt;
&lt;p&gt;In order for mpi to distribute workload the node where execution occurs needs to understand which nodes are available.  The machinename parameter can be used to point to a text file containing list of nodes.&lt;/p&gt;
&lt;p&gt;In order name we can use names we can add entries to hosts file.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo sh -c &#39;echo &amp;quot;192.168.1.100 rpi-0&amp;quot; &amp;gt;&amp;gt; /etc/hosts&#39;
sudo sh -c &#39;echo &amp;quot;192.168.1.101 rpi-1&amp;quot; &amp;gt;&amp;gt; /etc/hosts&#39;
sudo sh -c &#39;echo &amp;quot;192.168.1.102 rpi-2&amp;quot; &amp;gt;&amp;gt; /etc/hosts&#39;
sudo sh -c &#39;echo &amp;quot;192.168.1.103 rpi-3&amp;quot; &amp;gt;&amp;gt; /etc/hosts&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can then create a file in home directory listing nodes.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt;&amp;gt; ~/machinefile
rpi-0
rpi-1
rpi-2
rpi-3
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The easiest way to run commands or code is with the mpirun command. This command, run in a shell, will launch multiple copies of your code, and set up communications between them. As each Pi has four cores and we have four we can specify number of processors to run as sixteen.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mpirun -np 16 -machinefile ~/machinefile vcgencmd measure_temp
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/mpirun_temp.png&#34; alt=&#34;MPIRUN Measure Temparature&#34;&gt;&lt;/p&gt;
&lt;p&gt;While its interesting running individual commands across nodes the MPI for Python module exposes options for programs to spread load.  A simple test of this is where we scatter a bunch of elements, like those in a list, to processing nodes.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt;&amp;gt; ~/scatter.py
from mpi4py import MPI

comm = MPI.COMM_WORLD
size = comm.Get_size()
rank = comm.Get_rank()

if rank == 0:
   data = [(x+1)**x for x in range(size)]
   print (&#39;we will be scattering:&#39;,data)
else:
   data = None
   
data = comm.scatter(data, root=0)
print (&#39;rank&#39;,rank,&#39;has data:&#39;,data)
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once script created execute:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mpirun -np 16 -machinefile ~/machinefile python3 ~/scatter.py
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/mpirun_scatter.png&#34; alt=&#34;MPIRUN Scatter&#34;&gt;&lt;/p&gt;
&lt;p&gt;While scattering elements of a list is interesting splitting up a processing problem and distributing to multiple processing nodes is more fun.  Calculating Pi is a nice example of this,  and its nice for a Pi cluster to calculate Pi with MPI.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#pi.py&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; mpi4py &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; MPI

comm &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; MPI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;COMM_WORLD
rank &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; comm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Get_rank()
size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; comm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Get_size()

slice_size &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1000000&lt;/span&gt;
total_slices &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# This is the controller node.&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; rank &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
    pi &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
    slice &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
    process &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;

    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; (size)

    &lt;span style=&#34;color:#75715e&#34;&gt;# Send the first batch of processes to the nodes.&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; process &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; size &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; slice &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; total_slices:
        comm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;send(slice, dest&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;process, tag&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Sending slice&amp;#34;&lt;/span&gt;,slice,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;to process&amp;#34;&lt;/span&gt;,process)
        slice &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
        process &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;

    &lt;span style=&#34;color:#75715e&#34;&gt;# Wait for the data to come back&lt;/span&gt;
    received_processes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; received_processes &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; total_slices:
        pi &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; comm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recv(source&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;MPI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ANY_SOURCE, tag&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
        process &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; comm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recv(source&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;MPI&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ANY_SOURCE, tag&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Recieved data from process&amp;#34;&lt;/span&gt;, process)
        received_processes &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;

        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; slice &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; total_slices:
            comm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;send(slice, dest&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;process, tag&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
            &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Sending slice&amp;#34;&lt;/span&gt;,slice,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;to process&amp;#34;&lt;/span&gt;,process)
            slice &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;

    &lt;span style=&#34;color:#75715e&#34;&gt;# Send the shutdown signal&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; process &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,size):
        comm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;send(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, dest&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;process, tag&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)

    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt; (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Pi is&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4.0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; pi)

&lt;span style=&#34;color:#75715e&#34;&gt;# These are the worker nodes, where rank &amp;gt; 0. They do the real work&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; True:
        start &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; comm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recv(source&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, tag&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; start &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;: &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;

        i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
        slice_value &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; slice_size:
            &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; i&lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:
                slice_value &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;(start&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;slice_size&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;i)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
            &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
                slice_value &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;(start&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;slice_size&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;i)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
            i &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;

        comm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;send(slice_value, dest&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, tag&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
        comm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;send(rank, dest&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, tag&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Execute script with mpi and watch it distribute the calculation around the nodes and aggregate these to final answer.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mpirun -np 16 -machinefile ~/machinefile python3 pi.py
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/pi_with_mpi_on_pi.png&#34; alt=&#34;Pi with MPI on Pi&#34;&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Comparing SD and USB 3 Storage Performance With Raspberry Pi4B</title>
      <link>https://darrylcauldwell.github.io/post/homelab-pi-storage/</link>
      <pubDate>Mon, 29 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/homelab-pi-storage/</guid>
      <description>
        
          &lt;p&gt;Following on from &lt;a href=&#34;https://darrylcauldwell.github.io/post/homelab-pi&#34;&gt;base Ubuntu build&lt;/a&gt; here I look at the comparing the storage performance of SD and USB.&lt;/p&gt;
&lt;h2 id=&#34;sd-card-performance&#34;&gt;SD Card Performance&lt;/h2&gt;
&lt;p&gt;Linux FIO tool will be used to measure sequential write performance of a 4GB file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=4G --readwrite=write
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/sd_reads.png&#34; alt=&#34;SD Card Read Performance&#34;&gt;&lt;/p&gt;
&lt;p&gt;The tool output shows sequential read rate:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;IOPS=10.1k, BW=39.5MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=10.1k, BW=39.3MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=10.1k, BW=39.4MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=10.1k, BW=39.3MiB/s&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Similarly, the tool will be used to measure sequential read performance of a 4GB file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=4G --readwrite=read
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/sd_writes.png&#34; alt=&#34;SD Card Write Performance&#34;&gt;&lt;/p&gt;
&lt;p&gt;The tool output shows sequential write rate:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;IOPS=5429, BW=21.2MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=5128, BW=20.0MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=5136, BW=20.1MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=5245, BW=20.5MiB/s&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With the SD card the performance bottleneck is the reader which supports peak bandwidth 50MiB/s. To test this I has a lower spec SanDisk Ultra card so I repeated test and got near exact throughput to the SanDisk Extreme.&lt;/p&gt;
&lt;p&gt;The tool output shows sequential read rate:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;IOPS=10.1k, BW=39.4MiB/s&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The tool output shows sequential write rate:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;IOPS=5245, BW=20.5MiB/s&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;usb-flash-performance&#34;&gt;USB Flash Performance&lt;/h2&gt;
&lt;p&gt;The USB flash drive should deliver improved performance, first check it can be seen by the system and note its device.&lt;/p&gt;
&lt;p&gt;Then repeated the same performance tests using fio on the SSD. Linux FIO tool will be used to measure sequential write/read performance of a 4GB file:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /mnt/ssd
fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=test --bs=4k --iodepth=64 --size=4G --readwrite=write
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/usb_writes.png&#34; alt=&#34;USB Write Performance&#34;&gt;&lt;/p&gt;
&lt;p&gt;The tool output shows sequential write rate:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;IOPS=14.5k, BW=56.6MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=14.4k, BW=56.4MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=14.4k, BW=56.2MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=11.9k, BW=46.6MiB/s&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;cd /mnt/ssd
fio --randrepeat=1 --ioengine=libaio --direct=1 --gtod_reduce=1 --name=test --filename=/mnt/ssd/test --bs=4k --iodepth=64 --size=4G --readwrite=read
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/usb_reads.png&#34; alt=&#34;USB Read Performance&#34;&gt;&lt;/p&gt;
&lt;p&gt;The tool output shows sequential read rate:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;IOPS=33.3k, BW=130MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=37.0k, BW=148MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=42.6k, BW=166MiB/s&lt;/li&gt;
&lt;li&gt;IOPS=42.5k, BW=166MiB/s&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;summary&#34;&gt;Summary&lt;/h2&gt;
&lt;p&gt;So for a very small investment in the USB Flash Drives we&amp;rsquo;ve increased sequential read potential by 4X and write potential by 3X over the SD card.  The Pi 4 firmware doesn&amp;rsquo;t presently offer option for USB boot so the SD cards are needed but hopefully soon the firmware will get updated.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Raspberry Pi Cluster Ubuntu Base</title>
      <link>https://darrylcauldwell.github.io/post/homelab-pi-ubuntu/</link>
      <pubDate>Sun, 28 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/homelab-pi-ubuntu/</guid>
      <description>
        
          &lt;p&gt;Following on from &lt;a href=&#34;https://darrylcauldwell.github.io/post/homelab-pi&#34;&gt;Raspberry Pi Cluster&lt;/a&gt; here I look at the base Ubuntu build.&lt;/p&gt;
&lt;h2 id=&#34;install-and-configure&#34;&gt;Install and Configure&lt;/h2&gt;
&lt;p&gt;I used &lt;a href=&#34;https://www.raspberrypi.org/software/&#34;&gt;Raspberry Pi imager&lt;/a&gt; to install the Ubuntu 20.10 64bit on each SD Card.  Insert these into the Pi&amp;rsquo;s and power them on.&lt;/p&gt;
&lt;p&gt;The image is configured with DHCP client, &lt;a href=&#34;https://maclookup.app/macaddress/DCA632&#34;&gt;Pi device MAC addresses are prefixed DC:A6:32&lt;/a&gt;. I connected to my router which acts as DHCP server and found the four leases sorting by MAC. With the DHCP addresses can connect via SSH, the Ubuntu image has default username of &lt;code&gt;ubuntu&lt;/code&gt; and password &lt;code&gt;ubuntu&lt;/code&gt;. You&amp;rsquo;re prompted to change password at first connect.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/ubuntu-pw.png&#34; alt=&#34;Ubuntu Password&#34;&gt;&lt;/p&gt;
&lt;p&gt;I want to reliably know how to connect to these and like to change from dynamic to a staticly asssigned IP address. To do this for Ubuntu 20.10 we update Netplan configuration.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo vi /etc/netplan/50-cloud-init.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Here is example of how I update this to reflect static IP.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;network:
    ethernets:
        eth0:
            addresses: [192.168.1.100/24]
            gateway4: 192.168.1.254
            nameservers:
              addresses: [8.8.8.8,8.8.4.4]
            dhcp4: no
            match:
                driver: bcmgenet smsc95xx lan78xx
            optional: true
            set-name: eth0
    version: 2
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With the configuration file updated can have netplan load the config.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo netplan --debug apply
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With any new install its useful to apply latest patches.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt update
sudo apt upgrade -y
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;install-tools&#34;&gt;Install Tools&lt;/h2&gt;
&lt;p&gt;The VideoCore packages provide command line utilities that can get various pieces of information from the VideoCore GPU on the Raspberry Pi. The linux flexible I/O tester tool is  easy to use and useful for understanding storage sub-system performance.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt install -y libraspberrypi-bin
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;# Storage Performance&lt;/p&gt;
&lt;p&gt;The linux flexible I/O tester tool is  easy to use and useful for understanding storage sub-system performance.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt install -y fio
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;usb-flash-disk&#34;&gt;USB Flash Disk&lt;/h2&gt;
&lt;p&gt;The SD card on the Pi will normally show as /dev/mmcblk0. The USB drive will normally show as /dev/sda. The following could be data destructive so check the enumeration before proceeding.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo fdisk -l
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/usb_dev.png&#34; alt=&#34;USB Device&#34;&gt;&lt;/p&gt;
&lt;p&gt;Then create primary partition on USB device&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo sfdisk /dev/sda &amp;lt;&amp;lt; EOF
;
EOF
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/usb_dev.png&#34; alt=&#34;USB Partition&#34;&gt;&lt;/p&gt;
&lt;p&gt;Then format and label the partition then mount and set permissions for the parition&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo mkfs.ext4 -L SSD /dev/sda1
sudo mkdir /mnt/ssd
sudo mount /dev/sda1 /mnt/ssd
echo &amp;quot;LABEL=SSD  /mnt/ssd  ext4  defaults 0 2&amp;quot; | sudo cat /etc/fstab -
sudo chmod 777 .
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/usb_ext4.png&#34; alt=&#34;USB Mount&#34;&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Raspberry Pi Cluster Hardware</title>
      <link>https://darrylcauldwell.github.io/post/homelab-pi/</link>
      <pubDate>Sat, 27 Mar 2021 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/homelab-pi/</guid>
      <description>
        
          &lt;p&gt;I have worked most of my professional life with systems built with Intel x86, x64 and ia64 chips. Intel processors use Complex Instruction Set Computing while Arm uses Reduced Instruction Set Computing. Processors with CISC architecture look to move complexity to hardware to simlify code, so small code sizes with high cycles per second. Processors with RISC architecture invert the relationship larger code size with low cycles per second.&lt;/p&gt;
&lt;p&gt;Anyone who has worked in a data center has an awareness of importance of the amount of heat generated by Intel powered servers. There isn’t much heat generated from allowing electricity to flow through something (unless there is A LOT of electricity). All of Arm’s designs are energy efficient which is why they have become popular for running in smartphones, tablets and other embedded devices. If Arm processors can get traction used in servers this can only be good news for data center electricity usage.&lt;/p&gt;
&lt;p&gt;I enjoy learning by doing and to begin to better understand Arm looked to build a cluster of Raspberry Pi. My goals of cluster include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Understanding more about Raspberry Pi hardware&lt;/li&gt;
&lt;li&gt;Understanding how an distributed application might work across physical hosts using MPI. MPI is a programming model that is widely used for parallel programming in a cluster.&lt;/li&gt;
&lt;li&gt;Understanding how micro-servies application may work in Edge locations running Kubernetes on low-cost hardware.&lt;/li&gt;
&lt;li&gt;Understanding how ESXi on Arm development is progressing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Project repository: &lt;a href=&#34;https://github.com/darrylcauldwell/piCluster&#34;&gt;https://github.com/darrylcauldwell/piCluster&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;pi-cluster-rack-hardware&#34;&gt;Pi Cluster Rack Hardware&lt;/h1&gt;
&lt;h2 id=&#34;power-over-ethernet&#34;&gt;Power over Ethernet&lt;/h2&gt;
&lt;p&gt;One of my goals for the cluster was the hardware self-contained and easy to connect to network and power. Out of the box the Raspberry Pi 4b is powered over USB-C and operates at 5.1V upto 3A so has maximum draw (5.1*3=) 15.3watt. While I could use one official Raspberry Pi USB-C 15.3watt power supply for each this would require four power sockets. A cleaner cabling solution is to equip each Pi 4b with a PoE HAT and power each using a Power over Ethernet–enabled network. When the Pi is cabled to a PoE network switch port the power gets routed to four PoE pins on the Pi board. The PoE switch ports deliver between 27-57volts. The PoE HAT connects to the four PoE pins and has onboard transformer which converts input voltage to 5volt and then routes this to the GPIO power pins.&lt;/p&gt;
&lt;h2 id=&#34;network-switch&#34;&gt;Network Switch&lt;/h2&gt;
&lt;p&gt;For selecting the PoE network switch there are two important factors I considered.  PoE has two specifications the initial 802.3af (upto 15watt) and revised 802.3at (upto 30watt), the Pi 4b has 15.3watt maximum draw so ports supporting 802.3af.  Another important factor to consider is maximum draw across  all PoE ports here our maximum draw would be (15.3*4=) 61.2watts. The Netgear GS305P has five ports with four supported for 802.3af or 802.3at and is rated for 63W power draw, it is also compact with dimensions 158 x 101 x 29 mm.&lt;/p&gt;
&lt;h2 id=&#34;storage&#34;&gt;Storage&lt;/h2&gt;
&lt;p&gt;For storage selection the SanDisk Extreme offers sequential read up to 160MB/sec and sequential write up to 90MB/sec. The SanDisk Ultra offers sequential read up to 100MB/sec and sequential write up to 90MB/sec. The Pi4B has a dedicated SD card socket which suports 1.8V, DDR50 mode (at a peak bandwidth of 50 Megabytes / sec). The Pi 4h as a USB 3 interface which has peak bandwidth 620 Megabytes / sec. The Arcanite 128GB USB 3.1 Flash Drive has a small formfactor and low cost and offers read speeds up to 400 MB/s and write speeds up to 100 MB/s.&lt;/p&gt;
&lt;h2 id=&#34;mounting-rack&#34;&gt;Mounting Rack&lt;/h2&gt;
&lt;p&gt;Towards goal of keeping cluster self contained I wanted to house the network switch and four Pi 4b in a rack. There are vaious off the shelf options for stacking Pi&amp;rsquo;s but I couldn&amp;rsquo;t find a great solution for my specific requirement. I&amp;rsquo;d not done a project using custom cut metal before and thought this would be a  good opportunity to explore. I thought aluminium would be good to polish to a nice finish so I decided to use 3mm medium strength 5251 aluminium.&lt;/p&gt;
&lt;p&gt;I found a UK based mail-order laser cutting provider &lt;a href=&#34;https://lasered.co.uk/&#34;&gt;Lasered&lt;/a&gt;. To make the order required drawing in either Drawing Interchange Format (DXF), AutoCAD (DWG) of Mastercam Numerical Control File (NC) format to  create this I used open-source LibreCAD software. I&amp;rsquo;d not used CAD software before so there was a learning curve but this was not large and within few hours I&amp;rsquo;d created two drawings. The cluster will primarily be used  as Kubernetes cluster so I designed all the plates shaped as heptagons, the top and base plates also have Raspberry Pi logo. For attaching plates together allowing enough space for airflow I chose 35mm standoffs with M4 thread to accept M4 I gave holes a 4.2mm radius. For mounting the Pi the board has 2.7mm holes to accept M2.5 thread screws so I mirror these on the plate with 2.7mm radius and use 5mm standoff to lift slightly. I added a rectangular hole behin Pi on each shelf to allow for internal PoE cable routing. The rack dimensions when assembled, widest points the heptagon is 210mm and (6x3mm=)18mm plates plus (5x40mm)=200 standoffs gives total height of 218mm.
&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/cut_plates.jpeg&#34; alt=&#34;Laser cut plates&#34;&gt;&lt;/p&gt;
&lt;p&gt;I got the plates cut and they looked better than expected except the standoff holes looked rather large. I checked the the calipers and noticed my planned 4.2mm holes were 8.4mm and my 2.7mm holes 5.4mm. Seems I had entered diameter as value for circle radius parameter. Luckily I hadn&amp;rsquo;t ordered the standoffs, nuts or bolts. It was easy to switched from M4 to M6 for the between layer standoffs but as the Pi board only accepts M2.5 I kept these and added a washer to prevent bolt going straight through the mount hole on the shelf.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/radius_diameter.jpeg&#34; alt=&#34;Oops radius != diameter&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/first_assembly.jpeg&#34; alt=&#34;First assembly&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;rack-cabling&#34;&gt;Rack Cabling&lt;/h2&gt;
&lt;p&gt;To keep the internal rack cabling tidy I decided to create each cable with custom length. The PoE standards require category 3 cable or better for 802.3af (upto 15watt) and category 5 cable for better for 802.3at (upto 30watt). The Raspberry Pi NIC can operate at 1Gb/s, category 5 is cable rated for 100Mb/s, category 5e is cable rated for 1Gb/s, category 6 is cable rated for 10Gb/s. To operate the Pi&amp;rsquo;s over PoE at full potential I use category 5e cable and crimped RJ45 connectors.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/custom_cables.jpeg&#34; alt=&#34;Custom cable lengths&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;finished-rack&#34;&gt;Finished Rack&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/rack_full.jpeg&#34; alt=&#34;Pi Rack Full&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/pi/rack_side.jpeg&#34; alt=&#34;Pi Rack Side&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;hardware-bill-of-materials&#34;&gt;Hardware Bill of materials&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;35mm M6 Standoffs (Bag 50) £30&lt;/li&gt;
&lt;li&gt;10mm M6 screws (Bag 100) £6.45&lt;/li&gt;
&lt;li&gt;M6 hexagonal nuts (Bag 250) £6&lt;/li&gt;
&lt;li&gt;5mm M2.5 Standoffs (Bag 20) £5&lt;/li&gt;
&lt;li&gt;6mm M2.5 screws (Bag 100) £3.50&lt;/li&gt;
&lt;li&gt;Custom laser cut aluminium plates £60&lt;/li&gt;
&lt;li&gt;&amp;lt;1m Cat5e cable&lt;/li&gt;
&lt;li&gt;RJ45 cable crimping tool kit £20&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Total rack parts cost ~£130&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;4x Raspberry Pi 4B WITH 8GB RAM (4X £73.50=) £294&lt;/li&gt;
&lt;li&gt;4x Raspberry Pi PoE HAT (4x £18=) £72&lt;/li&gt;
&lt;li&gt;4x 128GB SanDisk Extreme (4x £24=) £96&lt;/li&gt;
&lt;li&gt;4x Arcanite 128GB USB 3.1 Flash Drive (4x £20=) £80&lt;/li&gt;
&lt;li&gt;Netgear GS305P £45&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Total cluster parts cost ~£717&lt;/p&gt;

        
      </description>
    </item>
    
  </channel>
</rss>
