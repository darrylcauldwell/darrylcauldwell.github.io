<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>nsx on </title>
    <link>https://darrylcauldwell.com/tags/nsx/</link>
    <description>Recent content in nsx on </description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 31 Dec 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://darrylcauldwell.com/tags/nsx/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Introduction To Kubernetes Cluster Networking with NSX-T</title>
      <link>https://darrylcauldwell.com/post/k8s-nsxt/</link>
      <pubDate>Tue, 31 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/k8s-nsxt/</guid>
      <description>
        
          &lt;p&gt;When developing a cloud native application using Docker containers she soon needs to understand how Docker containers communicate. In previous &lt;a href=&#34;https://darrylcauldwell.com/post/docker-networking&#34;&gt;post&lt;/a&gt; I looked at how Docker containers communicate on a single host. When the developer wants to scaleout capacity of the hosting across multiple hosts or increase abailability she might look at deploying this on a Kubernetes cluster. The move from single Docker host to multiple hosts managed as Kubernetes Cluster introduces changes to the container networking model. The four distinct networking problems a Kubernetes Cluster needs to address:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Highly-coupled container-to-container communications&lt;/li&gt;
&lt;li&gt;Pod-to-Pod communications&lt;/li&gt;
&lt;li&gt;Pod-to-Service communications&lt;/li&gt;
&lt;li&gt;External-to-Service communications: this is covered by services&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;what-is-a-kubernetes-pod&#34;&gt;What Is A Kubernetes Pod&lt;/h2&gt;
&lt;p&gt;A Docker container is great for deploying a single atomic unit of software. This model can become a bit cumbersome when you want to run multiple pieces of software together. You often see this when developers create Docker images that use &lt;a href=&#34;https://docs.docker.com/config/containers/multi-service_container/&#34;&gt;supervisord&lt;/a&gt; as an entrypoint to start and manage multiple processes. Many have found that it is instead more useful to deploy those applications in groups of containers that are partially isolated and partially share an environment. It is possible to configure Docker to control the level of sharing between groups of containers by creating a parent container and manage the lifetime of those containers, however this is administratively complex. Kubernetes provides an abstraction called Pods for just this use case.&lt;/p&gt;
&lt;p&gt;A Kubernetes Pod implements a &amp;lsquo;pause&amp;rsquo; container as the managing parent container, the Pod also contains one or more of  application containers. The &amp;lsquo;pause&amp;rsquo; container serves as the basis of Linux namespace sharing in the Pod the other containers are starterd within that namespace. Sharing a namespace includes sharing network stack and other resources such as volumes. Sharing a network namespace means containers within a Pod share an IP address and all containers within a Pod can all reach each other’s ports on localhost.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-pod.png&#34; alt=&#34;Kubernets Pod&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;what-is-a-kubernetes-service&#34;&gt;What Is A Kubernetes Service&lt;/h2&gt;
&lt;p&gt;Typically a Kubernetes Deployment is used to When a Kubernetes Pod is deployed Pods. A Deployment describes a desired state it can create and destroy Pods dynamically. While each Pod gets its own IP address, the set of Pods running can change. A Kubernetes Service (sometimes called a micro-service) is an abstraction which defines a logical set of Pods. The Kubernetes API provides Service discovery to Pods, it also offers a method of exposing Services via network port or load balancer to external systems.&lt;/p&gt;
&lt;h2 id=&#34;what-is-container-networking-interface-cni&#34;&gt;What Is Container Networking Interface (CNI)&lt;/h2&gt;
&lt;p&gt;Container-centric infrastructure needs a network and this network must be dynamic. Container networking is designed to be plugable, the Container Networking Interface is a defined &lt;a href=&#34;https://github.com/containernetworking/cni/blob/master/SPEC.md&#34;&gt;specification&lt;/a&gt;. Various open source projects and vendors provide CNI compliant plugins which provide dynamic networking solution for containers.&lt;/p&gt;
&lt;h2 id=&#34;what-is-nsx-t-container-plugin-ncp&#34;&gt;What Is NSX-T Container Plugin (NCP)&lt;/h2&gt;
&lt;p&gt;The NSX-T Container Plug-in (NCP) provides a CNI plugin and an integration with container orchestrators such as Kubernetes and OpenShift.&lt;/p&gt;
&lt;p&gt;NSX-T bring advanced features which can enrich Kubernetes cluster networking &lt;a href=&#34;https://blogs.vmware.com/networkvirtualization/2017/03/kubecon-2017.html/&#34;&gt;including&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Fine-grained traffic control and monitoring&lt;/li&gt;
&lt;li&gt;Fine-grained security policy (firewall rules)&lt;/li&gt;
&lt;li&gt;Automated creation of network topology&lt;/li&gt;
&lt;li&gt;Integration with enterprise networking&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The main component of NCP runs in a container and communicates with NSX Manager and with the Kubernetes control plane. NCP monitors changes to containers and other resources and manages networking resources such as logical ports, switches, routers, and security groups for the containers by calling the NSX API.&lt;/p&gt;
&lt;h2 id=&#34;nsx-t-container-plug-in-release-25&#34;&gt;NSX-T Container Plug-in Release 2.5&lt;/h2&gt;
&lt;p&gt;I looked at the &lt;a href=&#34;http://darrylcauldwell.com/nsx-openshift/&#34;&gt;NCP initially for integration with OpenShift&lt;/a&gt; in mid-2018. I am revisiting this now to refresh my understanding and explore some of the new features introduced with the &lt;a href=&#34;https://blogs.vmware.com/networkvirtualization/2019/09/nsx-t-2-5-what-is-new-for-kubernetes.html/&#34;&gt;2.5 release&lt;/a&gt; including:&lt;/p&gt;
&lt;h3 id=&#34;policy-api-object-support&#34;&gt;Policy API Object Support&lt;/h3&gt;
&lt;p&gt;Prior to the 2.5 release all NSX objects which the NCP interacted with had to be created via the Advanced Networking &amp;amp; Security tab in the UI or the old imperative APIs. The imperative API was harder than it could have been to control programatically so with the NSX-T 2.4 release VMware introduced a new intent-based Policy API and corresponding Simplified UI. The NCP now supports either the imperative or the intent-based API,  to use the intent-based API a new parameter in the NCP configmap (ncp.ini) policy_nsxapi needs to be set to True.&lt;/p&gt;
&lt;h3 id=&#34;simplified-installation&#34;&gt;Simplified Installation&lt;/h3&gt;
&lt;p&gt;Another change I am interested in exploring is the simplified installation. In the past, an admin had to login to every k8s node and perform multiple steps to bootstrap it. She had to install the NSX CNI Plug-in and OpenVSwitch, to create OVS bridge and to add one vNic to the bridge. The 2.5 release introduces a second DaemonSet nsx-ncp-bootstrap Pod this now handles the deployment and lifecycle management of these components and we don’t need to login to every node. This should make it easier to scale out a cluster with additional nodes.&lt;/p&gt;
&lt;h2 id=&#34;lab-hardware-configuration&#34;&gt;Lab Hardware Configuration&lt;/h2&gt;
&lt;p&gt;To explore Kubernetes networking and the NCP I am using my homelab. My homelab has a very simple physical network namely a single subnet (192.168.1.0/24) with DHCP enabled and which has default route to the internet. Connected to the physical network are three Intel NUCs each has two 1G NIC an onboard and an additional 1G USB3 NIC.&lt;/p&gt;
&lt;h2 id=&#34;lab-vsphere-configuration&#34;&gt;Lab vSphere Configuration&lt;/h2&gt;
&lt;p&gt;The hosts run vSphere 6.7 Update 3 and have the onboard NIC configure as a vSphere Standard Switch hosting Management and vSAN VMkernels and the USB3 NIC is unused. The hosts are added to a vCenter appliance (192.168.1.13) and formed into a VSAN enabled cluster. The cluster also hosts a Windows 2019 Server VM running Active Directory (192.168.1.10) this also acts as DNS server and NTP source for lab.&lt;/p&gt;
&lt;h2 id=&#34;lab-nsx-t-configuration&#34;&gt;Lab NSX-T Configuration&lt;/h2&gt;
&lt;p&gt;An extra-small NSX Manager appliance (192.168.1.14) is deployed.  All esxi hosts are configured as Transport Nodes using vusb0 interface to Transport Zone named &amp;lsquo;overlayTransport&amp;rsquo;. A medium sized NSX Edge called &amp;lsquo;nsxEdge&amp;rsquo; is deployed which is a member of &amp;lsquo;overlayTransport&amp;rsquo; and &amp;lsquo;vlanTransport&amp;rsquo; Transport Zones. A Edge Cluster named &amp;lsquo;edgeCluster&amp;rsquo; and add &amp;lsquo;nsxEdge&amp;rsquo; is a member.&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-ip-address-space&#34;&gt;Kubernetes IP Address Space&lt;/h2&gt;
&lt;p&gt;A Kubernetes Cluster requires all Pods on a node to communicate with all Pods on all nodes in the cluster without NAT these are refered to as CluterIP. To support this a range of IP addresses must be defined to be issued to Pods and Services within a cluster. Even though the range is used for both Pods and Services, it is called the Pod address range. The last /20 of the Pod address range is used for Services. A /20 range has 212 = 4096 addresses. So 4096 addresses are used for Services, and the rest of the range is used for Pods.&lt;/p&gt;
&lt;p&gt;The address range I will be using to issue ClusterIP for this lab cluster is 10.0.0.0/16.&lt;/p&gt;
&lt;p&gt;As well as internal communicatins using ClusterIP some parts of your application may need to be exposed as a Service to be accessible on an externally routable IP address. There are two methods for exposing Service onto an externally routable IP address, NodePort and LoadBalancer. Source NAT is used for translating private ClusterIP address to a public routable address.&lt;/p&gt;
&lt;p&gt;The external address range I will be using to issue ExternalIP for this lab cluster is 172.16.0.0/16.&lt;/p&gt;
&lt;h2 id=&#34;ncp---nsx-object-identification&#34;&gt;NCP - NSX Object Identification&lt;/h2&gt;
&lt;p&gt;There can be many objects deployed within NSX-T, the NCP needs to understand which of these objects to interact with. The NSX objects which the NCP needs to interact with include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Overlay transport zone&lt;/li&gt;
&lt;li&gt;Tier-0 logical router&lt;/li&gt;
&lt;li&gt;Logical switch to connect the node VMs&lt;/li&gt;
&lt;li&gt;IP Block for internal ClusterIP addressing&lt;/li&gt;
&lt;li&gt;IP Pool for ExternalIP addressing&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The NCP configuration is stored in ncp.ini file. It is possible to put the UUID of NSX objects in the ncp.ini but this an administrative pain. The mapping of which NSX objects for NCP to interact with is better achieved by applying tags to the appropriate NSX objects.&lt;/p&gt;
&lt;p&gt;An NSX-T instance can support multiple Kubernetes clusters to ensure correct object mapping a cluster name is used. The cluster name is specified in NCP configuration and the appopriate NSX objects must have tag of same name applied.&lt;/p&gt;
&lt;p&gt;For this lab environment I am configuring with cluster name &amp;lsquo;pandora&amp;rsquo;.&lt;/p&gt;
&lt;h2 id=&#34;deploy-and-configure-tier-0&#34;&gt;Deploy and Configure Tier-0&lt;/h2&gt;
&lt;p&gt;The NCP deploys application centric network topology at the top of that topology is a Tier-0 router which provides uplink to the physical network.&lt;/p&gt;
&lt;p&gt;Use the Policy UI to deploy a Tier-0 Logical Router with High-Availability mode Active-Passive. In order the NCP knows the correct Tier-0 Logical Router a tag like this needs to be applied:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Tag&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Scope&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;pandora&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ncp/cluster&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;When applied it should look like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-tier0.png&#34; alt=&#34;NSX-T Tier-0 Router&#34;&gt;&lt;/p&gt;
&lt;p&gt;To communicate with the physical network the Tier-0 requires an uplink IP address.  In order to add uplink IP address we require creating a network segment backed by VLAN.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-physical-segment.png&#34; alt=&#34;VLAN Backed Segment&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can now configure an interface on the Tier-0 with IP address 192.168.1.17.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-tier0-uplink.png&#34; alt=&#34;NSX-T Tier-0 Uplink&#34;&gt;&lt;/p&gt;
&lt;p&gt;To enable ExternalIP routing from physical network add a static route to physical router directing 172.16.0.0/16 to 192.168.1.17.&lt;/p&gt;
&lt;h3 id=&#34;nsx-ip-block-for-internal-clusterip&#34;&gt;NSX IP Block for internal ClusterIP&lt;/h3&gt;
&lt;p&gt;NSX-T has an inbuilt capability for IP Management, in which we can allocate blocks of IP Addresses and create IP Pools.&lt;/p&gt;
&lt;p&gt;The NCP requires an IP Block for issuing internal ClusterIP. Create the 10.0.0.0/16 IP address block named &amp;lsquo;k8s-1-internal&amp;rsquo; with tags applied like this:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Tag&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Scope&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;pandora&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ncp/no_snat&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pandora&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ncp/cluster&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;When applied it should look like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-internal-block.png&#34; alt=&#34;Internal Block&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;nsx-ip-pool-for-externalip&#34;&gt;NSX IP Pool for ExternalIP&lt;/h2&gt;
&lt;p&gt;The NCP requires an IP Pool for issuing internal ExternalIP. First create an IP Block named &amp;lsquo;k8s-1-external&amp;rsquo; with CIDR 172.16.0.0/16. This IP Block is not accessed directly by NCP so does not need any tags.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-external-block.png&#34; alt=&#34;External Block&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once the IP Block is in place create an IP Pool named &amp;lsquo;k8s-1-loadbalancer&amp;rsquo; which has a subnet issued from IP Block &amp;lsquo;k8s-1-external&amp;rsquo; which is sized at 128 the IP Pool.  The External IP Pool can be shared but should at least have tag applied like this:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Tag&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Scope&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;true&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ncp/external&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;When applied it should look like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-external-pool.png&#34; alt=&#34;External Block Tag&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;node-logical-switch&#34;&gt;Node Logical Switch&lt;/h2&gt;
&lt;p&gt;Network connectivity to the containers running in Kubernetes is provided by a NSX-T logical switch segment which is often referred to as the node logical switch. For this create a new segment called &amp;lsquo;node-logical-switch&amp;rsquo; within the &amp;lsquo;overlayTransportZone&amp;rsquo; connected to no gateway.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-node-logical-switch.png&#34; alt=&#34;Node Logical Switch&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;kubernetes-master-vm&#34;&gt;Kubernetes Master VM&lt;/h2&gt;
&lt;p&gt;Create a Ubuntu 18.04 VM with 2x CPU, 4GB RAM and 50GB vHDD named &amp;lsquo;k8s-master&amp;rsquo;. The VM should have two vNIC one which is used to communicate with NSX API and which will host Kubernetes API. The second connected to the node logical switch which will have the Open vSwitch (OVS) bridge configured to give connectivity to the Pods. In my lab first connected to &amp;lsquo;VM Network&amp;rsquo; enumerates as ens160 and the scond connected to &amp;lsquo;node-logical-switch&amp;rsquo; enumerates as ens192.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat /etc/netplan/01-netcfg.yaml 

network:
  version: 2
  renderer: networkd
  ethernets:
    ens160:
      addresses: 
      - 192.168.1.27/24
      gateway4: 192.168.1.254
      nameservers:
          search:
          - darrylcauldwell.com
          addresses: 
          - 192.168.1.10
    ens192: {}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In order the NCP know which vNIC to configure the VM vNIC connected &amp;lsquo;node-logical-switch&amp;rsquo; creates a segment port object in NSX-T this port must have tag applied like this:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Tag&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Scope&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;k8s-master&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ncp/node_name&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;pandora&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ncp/cluster&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;When applied it should look like:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-segment-port-tags.png&#34; alt=&#34;Segment Port Tags&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;install-and-configure-kubernetes&#34;&gt;Install and Configure Kubernetes&lt;/h2&gt;
&lt;p&gt;Docker and Kubernetes require installation do this by running the following.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo apt update -y
sudo apt upgrade -y
sudo apt install docker.io python apt-transport-https -y
sudo gpasswd -a $USER docker
sudo systemctl start docker
sudo systemctl enable docker
curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
sudo apt-add-repository &amp;quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&amp;quot;
sudo apt-get update
sudo swapoff -a 
sudo sed -i &#39;/ swap / s/^\(.*\)$/#\1/g&#39; /etc/fstab
sudo apt-get install -y kubelet=1.16.4-00 kubeadm=1.16.4-00 kubectl=1.16.4-00
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Initialize the Kubernetes cluster by running the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo kubeadm init
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In order to easily run kubectl as a user we need to copy the cluster configuration to the user profile, do this by running the following.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can now connect to cluster and check the state of the nodes, do this by running the following on the Master node.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-init-get-nodes.png&#34; alt=&#34;Init Get Nodes&#34;&gt;&lt;/p&gt;
&lt;p&gt;The status of each Node will show &amp;lsquo;NotReady&amp;rsquo;,  we can get more details of why it is in this state by running the following on the Master node.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl describe nodes k8s-master | grep Conditions -A9
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-init-describe-node.png&#34; alt=&#34;Init Describe Nodes&#34;&gt;&lt;/p&gt;
&lt;p&gt;With this we can see this reason for the error is &amp;lsquo;NetworkPluginNotReady&amp;rsquo; that the cni plugin is not initiated.&lt;/p&gt;
&lt;h2 id=&#34;install-and-configure-nsx-container-plug-in-ncp&#34;&gt;Install and Configure NSX Container Plug-in (NCP)&lt;/h2&gt;
&lt;p&gt;The NSX Container Plug-in (NCP) provides integration between NSX-T and Kubernetes, it is a containerised application which manages communicates between NSX Manager and the Kubernetes control plane. The NSX Container Plug-in (NCP) application runs in Kubernetes it is supplied as .zip download. The configuration of the NCP applications is maintained in a Kubernetes manifest file.&lt;/p&gt;
&lt;p&gt;The NCP application ships a container image file we could deploy this to a container registry but here we will just upload the image file to VM and import the image to the local docker repository. The default container image name specified in the manifest is nsx-ncp so we can apply that as tag on the image.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;sudo docker load -i /home/ubuntu/nsx-ncp-ubuntu-2.5.1.15287458.tar
sudo docker image tag registry.local/2.5.1.15287458/nsx-ncp-ubuntu nsx-ncp
sudo docker images | grep ncp
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Prior to the 2.5 release there were multiple  manifest files each targetting a specific area of configuration.  These are now merged into a single manifest file with multiple sections with multiple resource specifications the sections can be identified by the separator &lt;code&gt;---&lt;/code&gt;. The resources are created in the order they appear in the file.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Resource Kind&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Resource Name&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Comments&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;CustomResourceDefinition&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;nsxerrors.nsx.vmware.com&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CustomResourceDefinition&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;nsxlocks.nsx.vmware.com&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Namespace&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;nsx-system&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ServiceAccount&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ncp-svc-account&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ClusterRole&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ncp-cluster-role&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ClusterRole&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ncp-patch-role&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ClusterRoleBinding&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ncp-cluster-role-binding&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ClusterRoleBinding&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;ncp-patch-role-binding&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ServiceAccount&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;nsx-node-agent-svc-account&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ClusterRole&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;nsx-node-agent-cluster-role&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ClusterRoleBinding&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;nsx-node-agent-cluster-role-binding&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ConfigMap&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;nsx-ncp-config&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Must update Kubernetes API and NSX API parameters&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Deployment&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;nsx-ncp&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;ConfigMap&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;nsx-node-agent-config&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;Must update Kubernetes API and NSX API parameters&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DaemonSet&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;nsx-ncp-bootstrap&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DaemonSet&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;nsx-node-agent&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;For this lab I will use mostly default configuration from the supplied manifest file template. The settings I change from what is in template are the environment specifics such as details for connecting to NSX API including whether to use imperative API or intent-based API, the &amp;lsquo;cluster&amp;rsquo; name and the Node vNIC on which the OVS bridge gets created on.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[nsx_v3]
policy_nsxapi = True

nsx_api_managers = 192.168.1.14
nsx_api_user = admin
nsx_api_password = VMware1!

insecure = True

[coe]
cluster = pandora

[k8s]
apiserver_host_ip = 192.168.1.27
apiserver_host_port = 6443

[nsx_kube_proxy]
ovs_uplink_port = ens192
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once the manifest file is updated and docker image in local registry we can apply the NCP manifest. Applying the manifest takes a couple of minutes while as it creates various Pods we can watch their creation to view progress.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl apply --filename /home/ubuntu/ncp-ubuntu.yaml
kubectl get pods --output wide --namespace nsx-system --watch
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-deploy-ncp.png&#34; alt=&#34;Deploy NCP&#34;&gt;&lt;/p&gt;
&lt;p&gt;If all has gone well we can take a look at the objects created within the namespace.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get all --namespace nsx-system
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-deployed-ncp.png&#34; alt=&#34;Deployed NCP&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now its running we can check health of the NCP, the NCP has a &lt;a href=&#34;https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.5/ncp-kubernetes/GUID-EA8E6CEE-36F4-423C-AD1E-DD6421A5FB1C.html&#34;&gt;CLI&lt;/a&gt; where we can run various commands including health checks.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl exec -it nsx-ncp-6978b9cb69-dj4k2 --namespace nsx-system -- /bin/bash 
nsxcli
get ncp-k8s-api-server status
get ncp-nsx status
exit
exit
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-ncp-health.png&#34; alt=&#34;NCP Health&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;nsx-policy-ui-object-creation&#34;&gt;NSX Policy UI Object Creation&lt;/h2&gt;
&lt;p&gt;With NSX integration when we deploy a Kubernetes namespace the NCP creates a corresponding segment, IP Pool (from internet 10.0.0.0/8 range), subnet and Tier-1 router.  If you open NSX Manager and view one of the object categories which should have objects created. Then create two Kubernetes namespaces one called development and one called production.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create --filename https://k8s.io/examples/admin/namespace-dev.json
kubectl create --filename https://k8s.io/examples/admin/namespace-prod.json
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If you refresh view in NSX Manager you will see the new objects appear in the Policy UI.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-tier1s.png&#34; alt=&#34;NSX-T Tier-1&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can then remove these two namespaces the NCP removes the corresponding segment, IP Pool (from internet 10.0.0.0/8 range), subnet and Tier-1 router.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl delete -f https://k8s.io/examples/admin/namespace-dev.json
kubectl delete -f https://k8s.io/examples/admin/namespace-prod.json
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;ncp-bootstrap-scaleout-cluster&#34;&gt;NCP Bootstrap Scaleout Cluster&lt;/h2&gt;
&lt;p&gt;We started this lab with a single node cluster to look at the nsx-ncp-bootstrap Daemonset. When Nodes are added to the cluster this should install and configure the Node with NCP.&lt;/p&gt;
&lt;p&gt;Create a second and third VM with same hardware configuration as k8s-master but name these k8s-worker-1 / k8s-worker-2 and give IPs 192.168.1.28  / 192.168.1.29.  Ensure the NSX segment port attached to  VMs is tagged correctly. Ensure the NCP docker image is uploaded, imported to local docker registry and has tag applied.&lt;/p&gt;
&lt;p&gt;To add the additional Node to the cluster first step is to create a token on master.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubeadm token create --print-join-command
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-token.png&#34; alt=&#34;Token&#34;&gt;&lt;/p&gt;
&lt;p&gt;We use the output of command from the master to add the additional worker nodes to the cluster. The nsx-ncp-bootstap and nsx-node-agent are DaemonSets this ensures that all Nodes run a copy of a Pod. When we add the worker nodes to the cluster we can see the nsx-ncp-bootstrap Pods initialize and configure the Node and the nsx-node-agent Pods initialize.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get pods -o wide --namespace nsx-system --watch
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-ncp-scale-out.png&#34; alt=&#34;NCP Scale Out&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;ip-assignments&#34;&gt;IP Assignments&lt;/h2&gt;
&lt;p&gt;When a Pod is deployed it can be exposed as a Service, the service&lt;/p&gt;
&lt;p&gt;If we deploy an simple stateless application example like &lt;a href=&#34;https://kubernetes.io/docs/tutorials/stateless-application/guestbook/&#34;&gt;guestbook&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
kubectl config set-context --current --namespace=development
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-deployment.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-service.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-deployment.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-slave-service.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-deployment.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-service.yaml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When we configures the lab we created NSX managed IP Block 10.0.0.0/16. When created the development namespace got allocated a /24 subnet from this block. If we view the Pods get IP in correct range.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get pods --output wide
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-internal-ip-allocation.png&#34; alt=&#34;Internal IP Allocation&#34;&gt;&lt;/p&gt;
&lt;p&gt;As well as Pod deployments the guestbook application installation also creates Service resources. A Service is an abstraction which defines a logical set of Pods. Our application runs three frontend pods and two Redis slaves the Service provides virtual IP. The kube-proxy is responsible for implementing the virtual IP for Services. We can see the ClusterIP are issued from the last /20 of the Pod address range. If we look in more detail at the Service resource we can see the three internal IP addresses behind it.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl get services --namespace development
kubectl cluster-info dump | grep -m 1 service-cluster-ip-range
kubectl describe service frontend
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-clusterip.png&#34; alt=&#34;Cluster IP&#34;&gt;&lt;/p&gt;
&lt;p&gt;To facilitate communications between Pods the NCP is configuring the Open vSwitch on each Node. The Open vSwitch user space daemon runs on a container named nsx-ovs within the nsx-node-agent Pods. The Open vSwitch bridge name can be specified in the ncp.ini but defaults to br-int. If we connect to this container we can view the Open vSwitch flows .&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl exec -it nsx-node-agent-llf2c -c nsx-ovs bash --namespace nsx-system
ovs-ofctl dump-flows br-int
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/k8s-nsxt-ovs-flows.png&#34; alt=&#34;OVS Flows&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can then remove the namespace and all objects within.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl delete -f https://k8s.io/examples/admin/namespace-dev.json
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;nsx-loadbalancer&#34;&gt;NSX Loadbalancer&lt;/h2&gt;
&lt;p&gt;NSX provides an load balancer capability to Kubernetes we can use this by creating service resource with type LoadBalancer.  If we create a simple replicaset of five pods and expose this we can see it gets issued with IP Address from the IP Pool tagged with ncp/external = True. We can also see this in NSX Loadbalancer configuration.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
kubectl config set-context --current --namespace=development
kubectl apply -f https://k8s.io/examples/service/load-balancer-example.yaml
kubectl get replicasets
kubectl expose deployment hello-world --type=LoadBalancer --name=hello-world-nsx-lb
kubectl get services hello-world-nsx-lb --watch

NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
hello-world-nsx-lb   LoadBalancer   10.110.64.115   &amp;lt;pending&amp;gt;     8080:32091/TCP   7s
hello-world-nsx-lb   LoadBalancer   10.110.64.115   172.16.0.13   8080:32091/TCP   7s
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can then remove the namespace and all objects within.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl delete -f https://k8s.io/examples/admin/namespace-dev.json
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;kubernetes-pods-micro-segmentation&#34;&gt;Kubernetes Pods Micro-Segmentation&lt;/h2&gt;
&lt;p&gt;One of the great usecases for NSX with vSphere has been the is the Distributed Firewall which protects VM workload at the Hypervisor layer. When we extend NSX into the Kubernetes container we also extend the Distributed Firewall capability. We can create firewall rules which contain members of groups, the groups can be dynamically populated by use of NSX tags. Kubernetes objects can have labels attached a label attached to a Pod is reflected in NSX as a tag on the segement port of the Pod.&lt;/p&gt;
&lt;p&gt;A simple test might be to deploy a two tier app where frontend can talk to backend but frontend cannot talk to other frontend. If we create a NSX group called Web with membership criteria Segement Port, Tag, Equals web Scope secgroup. We then create a NSX DFW rule with Web as source and destination and action of drop.&lt;/p&gt;
&lt;p&gt;With this in place we can ping test from one frontend pod to another frontend and backend and see this works.  We can then apply the label to the three frontend web Pods so they become members of the NSX group and are affected by the firewall rule.  With these in place we can retest ping from one frontend pod to another frontend and see that this is now blocked.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl create -f https://k8s.io/examples/admin/namespace-dev.json
kubectl config set-context --current --namespace=development
kubectl apply -f https://k8s.io/examples/application/guestbook/redis-master-deployment.yaml
kubectl apply -f https://k8s.io/examples/application/guestbook/frontend-deployment.yaml
kubectl get pods --output wide --watch

NAME                            READY   STATUS    RESTARTS   AGE   IP         NODE           NOMINATED NODE   READINESS GATES
frontend-6cb7f8bd65-4mctt       1/1     Running   0          13m   10.0.6.4   k8s-worker-1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
frontend-6cb7f8bd65-8wkhr       1/1     Running   0          13m   10.0.6.2   k8s-worker-1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
frontend-6cb7f8bd65-rtgc9       1/1     Running   0          13m   10.0.6.3   k8s-worker-1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
redis-master-7db7f6579f-zlx26   1/1     Running   0          3s    10.0.6.5   k8s-worker-1   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;

kubectl exec -it frontend-6cb7f8bd65-4mctt ping 10.0.6.2
PING 10.0.6.2 (10.0.6.2): 56 data bytes
64 bytes from 10.0.6.2: icmp_seq=0 ttl=64 time=0.083 ms

kubectl exec -it frontend-6cb7f8bd65-4mctt ping 10.0.6.5
PING 10.0.6.5 (10.0.6.5): 56 data bytes
64 bytes from 10.0.6.5: icmp_seq=0 ttl=64 time=3.191 ms

kubectl label pod frontend-6cb7f8bd65-4mctt secgroup=web
kubectl label pod frontend-6cb7f8bd65-8wkhr secgroup=web
kubectl label pod frontend-6cb7f8bd65-rtgc9 secgroup=web

kubectl exec -it frontend-6cb7f8bd65-4mctt ping 10.0.6.2
PING 10.0.6.2 (10.0.6.2): 56 data bytes
^C--- 10.0.6.2 ping statistics ---
2 packets transmitted, 0 packets received, 100% packet loss
command terminated with exit code 1

kubectl exec -it frontend-6cb7f8bd65-4mctt ping 10.0.6.5
PING 10.0.6.5 (10.0.6.5): 56 data bytes
64 bytes from 10.0.6.5: icmp_seq=0 ttl=64 time=5.672 ms
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can then remove the namespace and all objects within.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kubectl delete -f https://k8s.io/examples/admin/namespace-dev.json
&lt;/code&gt;&lt;/pre&gt;
        
      </description>
    </item>
    
    <item>
      <title>NSX-T for Planespotter</title>
      <link>https://darrylcauldwell.com/post/nsx-planespotter/</link>
      <pubDate>Thu, 14 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-planespotter/</guid>
      <description>
        
          &lt;p&gt;Moving towards application centric infrastructure suitable for microservices applications requires a good example application to work with.  I saw an NSX demo presented by &lt;a href=&#34;https://github.com/yfauser&#34;&gt;Yves Fauser&lt;/a&gt; where he used just such an application called planespotter.&lt;/p&gt;
&lt;p&gt;My first task towards understanding this more was to get this setup in my homelab and look at it in context of NSX-T.&lt;/p&gt;
&lt;h2 id=&#34;homelab-core&#34;&gt;Homelab Core&lt;/h2&gt;
&lt;p&gt;My homelab networking is very simple, I have a domestic BT broadband router, this connects a Cisco SB200 8-port L2 only switch. Each port connected to ESXi is configured with MTU 9216 and has the VLAN passed untagged.&lt;/p&gt;
&lt;p&gt;Connected to this are a pair of Intel NUCs each has onboard 1GB NIC and two USB 1GB NICs.  The lab is used for other things so the on-board vmnic0 NIC is assigned to a VSS which has default &amp;lsquo;VM Network&amp;rsquo; portgroup. This leaves both vusb0 and vusb1 free to be used for N-VDS.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/planespotter-esxi.png&#34; alt=&#34;ESXi Networking&#34;&gt;&lt;/p&gt;
&lt;p&gt;The NUCs are single socket, dual core i5s running at 1.8GHz and each has only 32GB of RAM, these currently run vSphere 6.7 Update 1 and shared storage is all-flash VSAN with no data protection.&lt;/p&gt;
&lt;h2 id=&#34;nsx-t-base&#34;&gt;NSX-T Base&lt;/h2&gt;
&lt;p&gt;Download OVA&amp;rsquo;s and perform initial deployment of NSX Manager,  NSX Controller and NSX Edge.  As lab only has a single L2 all get deployed with all vNICs connected to the portgroup &amp;lsquo;VM Network&amp;rsquo;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/planespotter-edge.png&#34; alt=&#34;NSX-T Base&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;nsx-t-routing&#34;&gt;NSX-T Routing&lt;/h2&gt;
&lt;p&gt;One of the biggest differences between NSX for vSphere and NSX-T is the routing architecture. The services are split between service router (SR) and distributed router (DR), the service router functions are run on the NSX edge and the distributed router (DR) is a kernel module running on the ESXi hosts. My lab setup uses defaults for all transit switches, it is importanrt to understand the relationship when we look at packets flow through these various hops.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/planespotter-edge-arch.png&#34; alt=&#34;NSX-T Routing&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;planespotter&#34;&gt;Planespotter&lt;/h2&gt;
&lt;p&gt;The planespotter application is made up of various microservices and database. For this NSX-T setup each is installed on a VM. The application can be installed by following &lt;a href=&#34;https://github.com/darrylcauldwell/planespotter/blob/master/docs/vm_deployment/README.md&#34;&gt;this guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/planespotter-logical-switches.png&#34; alt=&#34;NSX-T Logical Switches&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;planespotter-fe-to-api-traceflow&#34;&gt;Planespotter FE to API Traceflow&lt;/h2&gt;
&lt;p&gt;One neat feature of NSX-T and geneve is to inject data into the header and use this to trace flows. The traceflow feature helps inspect the path of a packet as it travels from one logical port to a single or multiple logical ports.&lt;/p&gt;
&lt;p&gt;So if we select the port connected to planespotter frontend and port connected to planespotter api, we get a nice visual represenation of the path.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/planespotter-traceflow.png&#34; alt=&#34;NSX-T Logical Switches&#34;&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Controlling NSX-T with Terraform</title>
      <link>https://darrylcauldwell.com/post/nsx-terraform/</link>
      <pubDate>Mon, 07 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-terraform/</guid>
      <description>
        
          &lt;p&gt;Hashicorp Terraform is a tool which enables you to safely and predictably create, change, and improve infrastructure by defining its configuration as code.&lt;/p&gt;
&lt;p&gt;VMware NSX-T is a product which enables software defined network infrastructure.&lt;/p&gt;
&lt;p&gt;The Terraform NSX-T provider allows us to deliver and maintain NSX-T configuration as code.&lt;/p&gt;
&lt;p&gt;This is a walkthrough of how in a very few commands you can begin to control NSX-T configuration using NSX-T.&lt;/p&gt;
&lt;p&gt;If starting from scratch &lt;a href=&#34;https://learn.hashicorp.com/terraform/getting-started/install.html&#34;&gt;install a simple terraform server&lt;/a&gt;,  on CentOS / RHEL you would use these commands.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;yum update -y
yum install wget net-tools unzip -y
cd /tmp
wget https://releases.hashicorp.com/terraform/0.11.11/terraform_0.11.11_linux_amd64.zip
mkdir /terraform
unzip terraform_0.11.11_linux_amd64.zip -d /terraform
cd /terraform
./terraform --version
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once we have this installed we can configure the  NSX-T Manager connection details as variables in a reusable variables file.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat &amp;gt; /terraform/variables.tf &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; &amp;#39;__EOF__&amp;#39;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;variable &amp;#34;nsx_manager&amp;#34; {}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;variable &amp;#34;nsx_username&amp;#34; {}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;variable &amp;#34;nsx_password&amp;#34; {}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;__EOF__&lt;/span&gt;

cat &amp;gt; /terraform/terraform.tfvars &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; &amp;#39;__EOF__&amp;#39;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;nsx_manager = &amp;#34;192.168.1.15&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;nsx_username = &amp;#34;admin&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;nsx_password = &amp;#34;VMware1!&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;__EOF__&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can then create a very basic Terraform configuration file which uses these variables and then performs a simple action like creating a NSX IP Set.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cat &amp;gt; /terraform/nsx.tf &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; &amp;#39;__EOF__&amp;#39;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;provider &amp;#34;nsxt&amp;#34; {
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  host                  = &amp;#34;${var.nsx_manager}&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  username              = &amp;#34;${var.nsx_username}&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  password              = &amp;#34;${var.nsx_password}&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  allow_unverified_ssl  = true
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  max_retries           = 10
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  retry_min_delay       = 500
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  retry_max_delay       = 5000
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  retry_on_status_codes = [429]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;resource &amp;#34;nsxt_ip_set&amp;#34; &amp;#34;ip_set1&amp;#34; {
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  description  = &amp;#34;IP Set provisioned by Terraform&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  display_name = &amp;#34;IP Set&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  tag {
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    scope = &amp;#34;color&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    tag   = &amp;#34;blue&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  }
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  ip_addresses = [&amp;#34;1.1.1.1&amp;#34;, &amp;#34;2.2.2.2&amp;#34;]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;__EOF__&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;With this in place we can initialize Terraform and get it to pull down the NSX-T provider.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;./terraform init
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If all has gone well Terraform should initialize successfully.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;We can now look at the changes our Terraform configuration file will make to NSX.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;./terraform plan
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We should see the single resource defined in the Terraform configuration file.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Now we have verified it does what we hope we can then look to apply the change.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;./terraform apply
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Not as this is changing configuration we get asked to confirm action.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;Once ran successfully we can then check in NSX GUI and confirm the IP Set is created.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;p&gt;If we would like to launch and destroy application specific NSX network infrastructure when we deploy our application. We would do this with a CI/CD pipeline tool like Jenkins will walk through this. If starting from scratch &lt;a href=&#34;https://wiki.jenkins.io/display/JENKINS/Installing+Jenkins+on+Red+Hat+distributions&#34;&gt;install a simple jenkins server&lt;/a&gt;, on CentOS / RHEL you would use these commands.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;yum update -y
yum install wget net-tools unzip -y
wget -O /etc/yum.repos.d/jenkins.repo http://pkg.jenkins-ci.org/redhat-stable/jenkins.repo
rpm --import https://jenkins-ci.org/redhat/jenkins-ci.org.key
yum install jenkins java -y
service jenkins start
chkconfig jenkins on
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once installed and running we would normally load the &lt;a href=&#34;https://wiki.jenkins.io/display/JENKINS/Terraform+Plugin&#34;&gt;Terraform plugin&lt;/a&gt;,  however this does not presently work as Terraform prompts for confirmation.  There is a &lt;a href=&#34;https://github.com/jenkinsci/terraform-plugin/pull/4/commits/47d6d3da54dd2cc437c1efb5df89cdccdb0f3eb0&#34;&gt;pending pull request to fix this&lt;/a&gt; until this gets merged we need a workaround.&lt;/p&gt;
&lt;p&gt;To workaround this issue we can still control Terraform with Jenkins by calling a shell script from within Jenkins job.  To do this we will create a folder and give Jenkins user account permissions by running following.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;mkdir /terraform
sudo usermod -a -G root jenkins
chmod -R g+w /terraform
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can then create a Jenkins job with Build contents which creates a Terraform file and applies this.  A simple example would be.&lt;/p&gt;
&lt;!-- raw HTML omitted --&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cd /terraform

cat &amp;gt; /terraform/nsx.tf &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;lt;&amp;lt; &amp;#39;__EOF__&amp;#39;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;provider &amp;#34;nsxt&amp;#34; {
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  host                  = &amp;#34;192.168.1.15&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  username              = &amp;#34;admin&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  password              = &amp;#34;VMware1!&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  allow_unverified_ssl  = true
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  max_retries           = 10
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  retry_min_delay       = 500
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  retry_max_delay       = 5000
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  retry_on_status_codes = [429]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;resource &amp;#34;nsxt_ip_set&amp;#34; &amp;#34;ip_set1&amp;#34; {
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  description  = &amp;#34;IP Set provisioned by Terraform&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  display_name = &amp;#34;IP Set&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  tag {
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    scope = &amp;#34;color&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;    tag   = &amp;#34;blue&amp;#34;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  }
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;  ip_addresses = [&amp;#34;1.1.1.1&amp;#34;, &amp;#34;2.2.2.2&amp;#34;]
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;}
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;__EOF__&lt;/span&gt;

./terraform init

./terraform apply -auto-approve

rm -f nsx.tf
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This is an overly simple example and more likely we would pull a config file from distributed source control such as github.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>NSX-T for OpenShift</title>
      <link>https://darrylcauldwell.com/post/nsx-openshift/</link>
      <pubDate>Sat, 28 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-openshift/</guid>
      <description>
        
          &lt;p&gt;While looking at the various documentation sets I found it difficult to understand the NSX-T and OpenShift integration. A lot was masked by configuration performed by Ansible scripts. Here I try and record my understanding of the technology and then work through getting this running in a capacity constrained lab environment.&lt;/p&gt;
&lt;h2 id=&#34;nsx-t&#34;&gt;NSX T&lt;/h2&gt;
&lt;p&gt;NSX-T (NSX Transformers) can provide network virtualization for multi-hypervisor environments, including both vSphere and KVM. It is also designed to address emerging application frameworks and architectures that have heterogeneous endpoints and technology stacks such as OpenStack, Red Hat OpenShift, Pivotal Cloud Foundry, Kubernetes, and Docker. NSX-V (NSX for vSphere) Manager integrates into vCenter and leverages a vSphere dvSwitch to form an overlay. NSX-T Manager can be used with vSphere it does not integrate with vCenter or dvSwitch, instead NSX is managed via its API, and its overlay is formed by each member having Open vSwitch (OVS) installed.&lt;/p&gt;
&lt;h2 id=&#34;red-hat-openshift&#34;&gt;Red Hat OpenShift&lt;/h2&gt;
&lt;p&gt;OpenShift helps you to develop, deploy, and manage container-based applications. It provides you with a self-service platform to create, modify, and deploy applications on demand, thus enabling faster development and release life cycles. OpenShift is built around a core of application containers powered by Docker, with orchestration and management provided by Kubernetes.&lt;/p&gt;
&lt;h2 id=&#34;container-networking-framework-background&#34;&gt;Container Networking Framework Background&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/docker/libnetwork/blob/master/docs/design.md&#34;&gt;Libnetwork&lt;/a&gt; is the canonical implementation Container Network Model (CNM) which formalizes the steps required to provide networking for containers while providing an abstraction that can be used to support multiple network drivers. Libnetwork provides an interface between the Docker daemon and network drivers. Container Network Model (CNM) is designed to support the Docker runtime engine only.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/containernetworking/cni&#34;&gt;Container Network Interface&lt;/a&gt; (CNI), consists of a specification and libraries for writing plugins to configure network interfaces in Linux containers, along with a number of supported plugins. Container Network Interface (CNI) supports integration with any container runtime.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-cni.jpeg&#34; alt=&#34;Container Network Interface (CNI) Integration&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;vmware-nsx-and-kubernetes-integration&#34;&gt;VMware NSX and Kubernetes Integration&lt;/h2&gt;
&lt;p&gt;VMware provide an &lt;a href=&#34;https://my.vmware.com/group/vmware/details?downloadGroup=NSX-T-PKS-221&amp;amp;productId=673&#34;&gt;NSX Container Plugin package&lt;/a&gt; which contains the required modules to integrate NSX-T with Kubernetes.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NSX Container Plugin (NCP) - is a container image which watches the Kubernetes API for changes to Kubernetes Objects (namespaces, network policies, services etc.). It calls the NSX API to creates network constructs based on object addition and changes.&lt;/li&gt;
&lt;li&gt;NSX DaemonSet
&lt;ul&gt;
&lt;li&gt;NSX Node Agent - is a container image which manages the container network interface&lt;/li&gt;
&lt;li&gt;NSX Kube-Proxy - is a container image which replaces the native distributed east-west load balancer in Kubernetes with the NSX load-balancer based on Open vSwitch (OVS).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;NSX Container Network Interface (CNI) - is an executable which allow the integration of NSX into Kubernetes.&lt;/li&gt;
&lt;li&gt;Open vSwitch&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-ncp.jpeg&#34; alt=&#34;NSX and Kubernetes Integration&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;nsx-for-openshift&#34;&gt;NSX For OpenShift&lt;/h2&gt;
&lt;p&gt;NSX implements a discreet network topology per Kubernetes namespace. NSX maps logical network elements like logical switches and distributed logical router to Kubernetes namespaces. Each of those network topologies can be directly routed, or privately addressed and behind NAT.&lt;/p&gt;
&lt;h2 id=&#34;nsx-for-openshift-homelab&#34;&gt;NSX For OpenShift Homelab&lt;/h2&gt;
&lt;p&gt;For the rest of this blog post I am aiming to create a NSX OpenShift integration. I aiming for two namespaces, each with a logical router and three subnets. The namespaces will use private address ranges and the tier-0 router will provide SNAT connectivity to the routed network.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-topology.jpeg&#34; alt=&#34;NSX Topology&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;starting-point-homelab-configuration&#34;&gt;Starting point homelab configuration&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;1GbE Switch (Layer 2 only)
&lt;ul&gt;
&lt;li&gt;VLAN 0 - CIDR 192.168.1.0/24&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;vSphere vCenter Appliance 6.7&lt;/li&gt;
&lt;li&gt;3x vSphere ESXi 6.7 Update 1 hosts (Intel NUC - 3x 1.8GHz CPU &amp;amp; 32GB RAM)
&lt;ul&gt;
&lt;li&gt;Onboard NIC is connected to a vSphere Standard Switch&lt;/li&gt;
&lt;li&gt;USB3 NIC is unused and will be used for NSX&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;VSAN&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The following resources are required&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Small NSX-T Manager is a VM sized 8GB vRAM, 2x vCPU and 140GB vHDD&lt;/li&gt;
&lt;li&gt;Small NSX Controller is a VM sized 8GB vRAM, 2x vCPU and 120GB vHDD&lt;/li&gt;
&lt;li&gt;Small NSX Edge is a VM sized 4GB vRAM, 2x vCPU and 120GB vHDD&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;nsx-management-plane&#34;&gt;NSX Management Plane&lt;/h2&gt;
&lt;p&gt;Deploy a small NSX unifed appliance specifying the nsx-manager role. Once deployed link this to vCenter, to do this add vCenter in &amp;lsquo;Fabric / Compute Manager&amp;rsquo;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-compute-manager.jpeg&#34; alt=&#34;NSX-T Management Plane&#34;&gt;&lt;/p&gt;
&lt;p&gt;With the manager in place we now need to create the management plane, to do this we need to install the management plane agent (MPA) on each host so they are added as usable Fabric Nodes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-nodes.jpeg&#34; alt=&#34;NSX-T Fabric Nodes&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;tunnel-endpoint-ip-pool&#34;&gt;Tunnel Endpoint IP Pool&lt;/h2&gt;
&lt;p&gt;We create an IP pool one for the Transort Nodes to communicate for my scenario the three ESXi hosts and an edge will all participate so I create an IP Pool with four addresses. Navigate to Inventory &amp;gt; Groups &amp;gt; IP Pools and click add.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-ip-pool.png&#34; alt=&#34;NSX-T IP Pool&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;nsx-control-plane&#34;&gt;NSX Control Plane&lt;/h2&gt;
&lt;p&gt;In order to create an overlay network we need an NSX Controller to manage the hosts. NSX Controllers serve as the central control point got all hosts, logical switches, and logical routers.&lt;/p&gt;
&lt;p&gt;While NSX Manager can deploy and configure NSX Controllers the size cannot be selected. As lab is resource constrained I only want a small NSX Controller, the &amp;lsquo;NSX Controller for VMware ESXi&amp;rsquo; is a separate OVA download where size can be selected.&lt;/p&gt;
&lt;p&gt;Once the controller appliance is deployed we need to facilitate communications between it and nsx manager.  To do this open an SSH session with admin user to NSX Manager and run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;get certificate api thumbprint
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Open an SSH session to NSX Controller with admin user and run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;join management-plane &amp;lt;NSX-Manager&amp;gt; username admin thumbprint &amp;lt;NSX-Managers-thumbprint&amp;gt;

set control-cluster security-model shared-secret

initialize control-cluster
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-mgr-ctrl-thumb.jpeg&#34; alt=&#34;NSX-T Controller&#34;&gt;&lt;/p&gt;
&lt;p&gt;This should then be viewable in NSX Manager&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-control-cluster.jpeg&#34; alt=&#34;NSX-T Controller Cluster&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;overlay-transport-zone&#34;&gt;Overlay Transport Zone&lt;/h2&gt;
&lt;p&gt;All the virtual network objects will need to communicate across an overlay network. To faciliate this the three esxi hosts and edges need to be part of an Overlay Transport Zone.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-transport.jpeg&#34; alt=&#34;NSX-T Transport Zone&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once we have a Transport Zone we can add our NSX fabric nodes as transport nodes. Navigate menu to Select Fabric &amp;gt; Transport Nodes and click Add.  A wizard will open on the general tab select first Node (host), give appropriate name for that host and select the openshift transport zone.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-transport-node.jpeg&#34; alt=&#34;NSX-T Transport Node&#34;&gt;&lt;/p&gt;
&lt;p&gt;Change to N-VDS tab, create N-VDS for openshift, select default NIOC, select default hostswitch Uplink profile, select transport IP Pool and enter Physical NIC identifier for Uplink-1.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-host-vds.jpeg&#34; alt=&#34;NSX-T Transport Zone N-VDS&#34;&gt;&lt;/p&gt;
&lt;p&gt;In order that the NSX Container Plugin can find the correct NSX objects all of the NSX objects created require a tag applying. For this lab build I am using tag dc-openshift. Navigate within NSX Manager to Fabric &amp;gt; Transport Zones, select overlay network then Actions &amp;gt; Manage Tags and apply tag.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Scope = ncp/cluster and Tag = dc-openshift
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-ncp-tags.jpeg&#34; alt=&#34;NSX-T Openshift Tags&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;vlan-transport-zone&#34;&gt;VLAN Transport Zone&lt;/h2&gt;
&lt;p&gt;As well as connecting to the overlay network the Edges running Tier-0 routing functions also needs to be able to connect to the physical network. This connectivity is achieved by using a Transport Zone of type VLAN.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-vlan-transport.png&#34; alt=&#34;NSX-T VLAN Transport Zone&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;nsx-edge&#34;&gt;NSX Edge&lt;/h2&gt;
&lt;p&gt;We need some way for the logical container overlay network to communicate with the physical network. AN NSX Edge can host services which provide this connectivity.&lt;/p&gt;
&lt;p&gt;The NSX Edge has 4 network adapters, the first is used by the management network, the other 3 interfaces (fp-eth0, fp-eth1 and fp-eth2) can then be used for connecting to overlay networks or for routing. Within my lab I have a single flat physical network so all NSX Edge interfaces connect to the same Port Group.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;GUI Reference&lt;/th&gt;
&lt;th&gt;VM vNIC&lt;/th&gt;
&lt;th&gt;NIC&lt;/th&gt;
&lt;th&gt;Lab Function&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Managewment&lt;/td&gt;
&lt;td&gt;Network adapter 1&lt;/td&gt;
&lt;td&gt;eth0&lt;/td&gt;
&lt;td&gt;Management&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Datapath #1&lt;/td&gt;
&lt;td&gt;Network adapter 2&lt;/td&gt;
&lt;td&gt;fp-eth0&lt;/td&gt;
&lt;td&gt;Overlay&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Datapath #2&lt;/td&gt;
&lt;td&gt;Network adapter 3&lt;/td&gt;
&lt;td&gt;fp-eth1&lt;/td&gt;
&lt;td&gt;Uplink&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Datapath #3&lt;/td&gt;
&lt;td&gt;Network adapter 4&lt;/td&gt;
&lt;td&gt;fp-eth2&lt;/td&gt;
&lt;td&gt;Unused&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-edge.jpeg&#34; alt=&#34;NSX-T Add Edge&#34;&gt;&lt;/p&gt;
&lt;p&gt;The NSX Edge needs to participate in the Overlay Transport Zone so we need to first configure this as Transport Node.  This is very similar process to how we setup ESXi hosts as Transport Nodes except on N-VDS tab we add to both overlay and vlan transport zones,  we use the edge-vm Uplink profile and for Virtual NIC select appropriate NIC as per table above.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-edge-nvds.png&#34; alt=&#34;NSX-T Edge N-VDS&#34;&gt;&lt;/p&gt;
&lt;p&gt;In order we can deploy Tier-0 router the Edge needs to be a member of an Edge Cluster.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-edge-cluster.jpeg&#34; alt=&#34;NSX-T Add Edge Cluster&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;tier-0-router&#34;&gt;Tier-0 Router&lt;/h2&gt;
&lt;p&gt;Once the Edge Cluster is created we can create the tier-0 router.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-tier0.jpeg&#34; alt=&#34;NSX-T Add Edge Tier-0 Router&#34;&gt;&lt;/p&gt;
&lt;p&gt;In my lab I have 192.168.1.0 /24 and will be using the 172.16.0.0 /16 address space for NSX. I would like to use network address translation (NAT) and allocate a separate SNAT IP on the 192.168.1.0 network for each OpenShift namespace on the 172.16.0.0 network.  To achieve this I need to configure a redistribution criteria of type Tier-0 NAT.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-tier0-route-redist.jpeg&#34; alt=&#34;NSX-T Add Edge Tier-0 Route Redist&#34;&gt;&lt;/p&gt;
&lt;p&gt;The next step requires an NSX Logical Switch so we create that.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-logical-switch.jpeg&#34; alt=&#34;NSX-T Add Logical Switch&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can now configure the Router Port,  selecting the Transport Node and Logical Switch.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-tier0-route-port.jpeg&#34; alt=&#34;NSX-T Add Tier-0 Router Port&#34;&gt;&lt;/p&gt;
&lt;p&gt;This will be used by OpenShift to once created navigate to Actions &amp;gt; Manage Tags and apply tag.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Scope = ncp/cluster and Tag = dc-openshift
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-ncp-tags.jpeg&#34; alt=&#34;NSX-T Add NCP Tags&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;ip-block-kubernetes-pods&#34;&gt;IP Block Kubernetes Pods&lt;/h2&gt;
&lt;p&gt;In order to create the topology we are aiming for we need to create an IP Blocks for each of our two namespaces.  Within each IP Block we need to create the three subnets. In the end you should end up with something which looks like this, and all IP Block needs to have the ncp/cluster tag.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-ddi-blocks.jpeg&#34; alt=&#34;NSX-T Add NCP Tags&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;ip-pool-snat&#34;&gt;IP Pool SNAT&lt;/h2&gt;
&lt;p&gt;We create an IP pool for the tier-0 router to issue SNAT and provide external (floating) IPs to OpenShift.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift-nsx-snat-pool.jpeg&#34; alt=&#34;NSX-T SNAT Pool&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once created add the following two tags,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Scope = ncp/cluster and Tag = dc-openshift
Scope = ncp/external and Tag = true
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;red-hat-openshift-origin&#34;&gt;Red Hat OpenShift Origin&lt;/h2&gt;
&lt;p&gt;OpenShift Origin is a computer software product from Red Hat for container-based software deployment and management. It is a supported distribution of Kubernetes using Docker containers and DevOps tools for accelerated application development.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/openshift.jpeg&#34; alt=&#34;Openshift Stack&#34;&gt;&lt;/p&gt;
&lt;p&gt;OpenShift Origin is the upstream community project used in &lt;a href=&#34;https://www.openshift.com/products/online/&#34;&gt;OpenShift Online&lt;/a&gt;, &lt;a href=&#34;https://www.openshift.com/products/dedicated/&#34;&gt;OpenShift Dedicated&lt;/a&gt;, and &lt;a href=&#34;https://www.openshift.com/products/container-platform/&#34;&gt;OpenShift Container Platform (formerly known as OpenShift Enterprise)&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;VMware provides &lt;a href=&#34;https://github.com/vmware/nsx-integration-for-openshift&#34;&gt;Red Hat Ansible playbooks for installing NSX-T for OpenShift Container Platform&lt;/a&gt;. However, OpenShift Container Platform is a licensed product and this deploys a scaled-out deployment. Neither of these lend itself to a home lab deployment, my goal for the rest of this blog post is to detail the steps I follow for a cutdown installation.&lt;/p&gt;
&lt;h2 id=&#34;create-openshift-origin-base-vm&#34;&gt;Create OpenShift Origin Base VM&lt;/h2&gt;
&lt;p&gt;The OpenShift Container Platform is Red Hat Enterprise Linux based, I don&amp;rsquo;t have a Red Hat Enterprise Linux subscription license. As such I created a CentOS 7 (64-bit) virtual machine, as the library versions are the same, so binaries that work on one will work on the other.&lt;/p&gt;
&lt;p&gt;Each OpenShift node needs to be managed and also provide connectivity to NSX, it is possible to perform these two functions on same vNIC however, I give my VM two vNICs one for management on VLAN backed dvPortgroup and one for NSX on VXLAN backed dvPortgroup. I used the CentOS minimal installation ISO set static IP address on management vNIC, and create DNS A &amp;amp; PTR records for this.&lt;/p&gt;
&lt;p&gt;Once built I run following commands to install Docker, some other basic tools and apply latest patches.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cat &amp;gt; /etc/yum.repos.d/docker.repo &amp;lt;&amp;lt; &#39;__EOF__&#39;
[docker]
name=Docker Repository
baseurl=https://yum.dockerproject.org/repo/main/centos/7/
enabled=1
gpgcheck=1
gpgkey=https://yum.dockerproject.org/gpg
__EOF__
yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm
yum install -y git open-vm-tools wget docker-engine net-tools python-pip
pip install docker-py
systemctl enable docker.service
yum update -y
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;default-kubernetes-service-addresses&#34;&gt;Default Kubernetes Service Addresses&lt;/h2&gt;
&lt;p&gt;OpenShift leverages the Kubernetes concept of a pod, which is one or more containers deployed together on one host, and the smallest compute unit that can be defined, deployed, and managed. A Kubernetes service address serves as an internal load balancer. It identifies a set of replicated pods in order to proxy the connections it receives to them. Services are assigned an IP address and port pair that, when accessed, proxy to an appropriate backing pod. These service addresses are assigned and managed by OpenShift. By default they are assigned out of the 172.30.0.0/16 network.&lt;/p&gt;
&lt;p&gt;To setup our environment we can configure the Docker daemon with an insecure registry parameter of 172.30.0.0/16.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;systemctl start docker
touch /etc/docker/daemon.json
cat &amp;gt; /etc/docker/daemon.json &amp;lt;&amp;lt; &#39;__EOF__&#39;
{
&amp;quot;insecure-registries&amp;quot;: [
    &amp;quot;172.30.0.0/16&amp;quot;
    ]
}
__EOF__
systemctl daemon-reload
systemctl restart docker
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;add-openshift-client&#34;&gt;Add OpenShift Client&lt;/h1&gt;
&lt;p&gt;The OpenShift client is used to manage the OpenShift installation and configuration it is supplied as a package. Download this, unpack and add to runtime path.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /tmp
wget https://github.com/openshift/origin/releases/download/v3.10.0-rc.0/openshift-origin-client-tools-v3.10.0-rc.0-c20e215-linux-64bit.tar.gz
tar -xvf /tmp/openshift-origin-client-tools-v3.10.0-rc.0-c20e215-linux-64bit.tar.gz -C /bin
mv /bin/openshift* /home/openshift
echo &#39;PATH=$PATH:/home/openshift&#39; &amp;gt; /etc/profile.d/oc-path.sh
chmod +x /etc/profile.d/oc-path.sh
. /etc/profile
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;start-openshift-origin-as-all-in-one-cluster&#34;&gt;Start OpenShift Origin as all-in-one Cluster&lt;/h2&gt;
&lt;p&gt;For next steps we need a basic OpenShift stack. Rather than build something custom we can simply start a local OpenShift all-in-one cluster with a configured registry, router, image streams, and default templates, by running the following command (where openshift.darrylcauldwell.com is the FQDN which points to IP address of management interface of your VM),&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc cluster up --public-hostname=openshift.darrylcauldwell.com
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We should also be able to logon and see all of the OpenShift services listed&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc login -u system:admin
oc get services --all-namespaces
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;NAMESPACE&lt;/th&gt;
&lt;th&gt;NAME&lt;/th&gt;
&lt;th&gt;TYPE&lt;/th&gt;
&lt;th&gt;CLUSTER-IP&lt;/th&gt;
&lt;th&gt;EXTERNAL-IP&lt;/th&gt;
&lt;th&gt;PORT(S)&lt;/th&gt;
&lt;th&gt;AGE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;default&lt;/td&gt;
&lt;td&gt;docker-registry&lt;/td&gt;
&lt;td&gt;ClusterIP&lt;/td&gt;
&lt;td&gt;172.30.1.1&lt;/td&gt;
&lt;td&gt;none&lt;/td&gt;
&lt;td&gt;5000/TCP&lt;/td&gt;
&lt;td&gt;9m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;default&lt;/td&gt;
&lt;td&gt;kubernetes&lt;/td&gt;
&lt;td&gt;ClusterIP&lt;/td&gt;
&lt;td&gt;172.30.0.1&lt;/td&gt;
&lt;td&gt;none&lt;/td&gt;
&lt;td&gt;443/TCP,53/UDP,53/TCP&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;default&lt;/td&gt;
&lt;td&gt;router&lt;/td&gt;
&lt;td&gt;ClusterIP&lt;/td&gt;
&lt;td&gt;172.30.88.3&lt;/td&gt;
&lt;td&gt;none&lt;/td&gt;
&lt;td&gt;80/TCP,443/TCP,1936/TCP&lt;/td&gt;
&lt;td&gt;9m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-dns&lt;/td&gt;
&lt;td&gt;kube-dns&lt;/td&gt;
&lt;td&gt;ClusterIP&lt;/td&gt;
&lt;td&gt;172.30.0.2&lt;/td&gt;
&lt;td&gt;none&lt;/td&gt;
&lt;td&gt;53/UDP,53/TCP&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openshift-apiserver&lt;/td&gt;
&lt;td&gt;api&lt;/td&gt;
&lt;td&gt;ClusterIP&lt;/td&gt;
&lt;td&gt;172.30.85.121&lt;/td&gt;
&lt;td&gt;none&lt;/td&gt;
&lt;td&gt;443/TCP&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openshift-web-console&lt;/td&gt;
&lt;td&gt;webconsole&lt;/td&gt;
&lt;td&gt;ClusterIP&lt;/td&gt;
&lt;td&gt;172.30.83.178&lt;/td&gt;
&lt;td&gt;none&lt;/td&gt;
&lt;td&gt;443/TCP&lt;/td&gt;
&lt;td&gt;9m&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We should also be able to see all of the OpenShift pods listed&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc get pod --all-namespaces
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;NAMESPACE&lt;/th&gt;
&lt;th&gt;NAME&lt;/th&gt;
&lt;th&gt;READY&lt;/th&gt;
&lt;th&gt;STATUS&lt;/th&gt;
&lt;th&gt;RESTARTS&lt;/th&gt;
&lt;th&gt;AGE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;default&lt;/td&gt;
&lt;td&gt;docker-registry-1-4l59n&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;default&lt;/td&gt;
&lt;td&gt;persistent-volume-setup-grm9s&lt;/td&gt;
&lt;td&gt;0/1&lt;/td&gt;
&lt;td&gt;Completed&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;default&lt;/td&gt;
&lt;td&gt;router-1-5xtqg&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-dns&lt;/td&gt;
&lt;td&gt;kube-dns-bj5cq&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;11m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-proxy&lt;/td&gt;
&lt;td&gt;kube-proxy-9l8ql&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;11m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-system&lt;/td&gt;
&lt;td&gt;kube-controller-manager-localhost&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-system&lt;/td&gt;
&lt;td&gt;kube-scheduler-localhost&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-system&lt;/td&gt;
&lt;td&gt;master-api-localhost&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-system&lt;/td&gt;
&lt;td&gt;master-etcd-localhost&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;11m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openshift-apiserver&lt;/td&gt;
&lt;td&gt;openshift-apiserver-ptk5j&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;11m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openshift-controller-manager&lt;/td&gt;
&lt;td&gt;openshift-controller-manager-vg7gm&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openshift-core-operators&lt;/td&gt;
&lt;td&gt;openshift-web-console-operator-78ddf7cbb7-r8dhd&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openshift-web-console&lt;/td&gt;
&lt;td&gt;webconsole-847bc4ccc4-hgsv4&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Once running we can open browser to OpenShift Origin&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;https://openshift.darrylcauldwell.com:8443/console/catalog
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Default credentials username &amp;lsquo;system&amp;rsquo; password &amp;lsquo;admin&amp;rsquo;&lt;/p&gt;
&lt;h2 id=&#34;nsx-t-open-vswitch&#34;&gt;NSX-T Open vSwitch&lt;/h2&gt;
&lt;p&gt;The NSX-T Container Plug-in (NCP) relies on Open vSwitch (OVS) providing a bridge to the NSX Logical Switch. VMware provide an Open vSwitch (OVS)  in the &lt;a href=&#34;https://my.vmware.com/web/vmware/details?downloadGroup=NSX-T-PKS-220&amp;amp;productId=673&#34;&gt;NSX Container Plugin 2.2.0&lt;/a&gt;, package.  Download expand and copy to OpenShift VM /tmp folder. Once uploaded install the following packages.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install -y /tmp/nsx-container-2.2.0.8740202/OpenvSwitch/rhel74_x86_64/kmod-openvswitch-2.9.1.8614397.rhel74-1.el7.x86_64.rpm
yum install -y /tmp/nsx-container-2.2.0.8740202/OpenvSwitch/rhel74_x86_64/openvswitch-2.9.1.8614397.rhel74-1.x86_64.rpm
yum install -y /tmp/nsx-container-2.2.0.8740202/OpenvSwitch/rhel74_x86_64/openvswitch-kmod-2.9.1.8614397.rhel74-1.el7.x86_64.rpm
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once installed start the Open vSwitch&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;service openvswitch start
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once the Open vSwitch is running we can create a bridge network interface, and then connect this to the VM network interface located on the NSX-T Logical Switch. You can do this by running the following command (where eno33559296 is the devicename of NIC on NSX Logical Switch),&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ovs-vsctl add-br br-int
ovs-vsctl add-port br-int eno33559296 -- set Interface eno33559296 ofport_request=1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;These connections are created with link state DOWN in order to use them we need to set link status is up for both,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ip link set br-int up
ip link set eno33559296 up
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Update the network configuration file to ensure that the network interface is up after a reboot.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vi /etc/sysconfig/network-scripts/ifcfg-eno33559296
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Ensure has a line reading,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ONBOOT=yes
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;nsx-t-container-network-interface-cni&#34;&gt;NSX-T Container Network Interface (CNI)&lt;/h2&gt;
&lt;p&gt;The NSX-T Container Plug-in (NCP) provides integration between NSX-T and container orchestrators such as Kubernetes. The installation files are in same package as the NSX Open vSwitch (OVS). Install using command.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;yum install -y /tmp/nsx-container-2.2.0.8740202/Kubernetes/rhel_x86_64/nsx-cni-2.2.0.8740202-1.x86_64.rpm
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;nsx-t-container-plug-in-ncp-replicationcontroller-rc&#34;&gt;NSX-T Container Plug-in (NCP) ReplicationController (RC)&lt;/h2&gt;
&lt;p&gt;There are a few accounts used for rights assignments, the project, users and roles are defined in NCP RBAC file. To create the users within the project run,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc login -u system:admin
oc create -f /tmp/nsx-container-2.2.0.8740202/nsx-ncp-rbac.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The RBAC creates two service account users, the tokens for these are required by NCP in folder /etc/nsx-ujo. This gets mounted as config-volume and these tokens used for authentication.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc project nsx-system
mkdir -p /etc/nsx-ujo
SVC_TOKEN_NAME=&amp;quot;$(oc get serviceaccount ncp-svc-account -o yaml | grep -A1 secrets | tail -n1 | awk {&#39;print $3&#39;})&amp;quot;
oc get secret $SVC_TOKEN_NAME -o yaml | grep &#39;token:&#39; | awk {&#39;print $2&#39;} | base64 -d &amp;gt; /etc/nsx-ujo/ncp_token
NODE_TOKEN_NAME=&amp;quot;$(oc get serviceaccount nsx-node-agent-svc-account -o yaml | grep -A1 secrets | tail -n1 | awk {&#39;print $3&#39;})&amp;quot;
oc get secret $NOD_TOKEN_NAME -o yaml | grep &#39;token:&#39; | awk {&#39;print $2&#39;} | base64 -d &amp;gt; /etc/nsx-ujo/node_agent_token
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The pods which NSX-T Container Plug-in (NCP) ReplicationController (RC) run in need to use the host networking so we need to allow then this right by loading the NCP Security Context Constraints for NCP and NSX Node Agent.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc apply -f /tmp/nsx-container-2.2.0.8740202/Kubernetes/rhel_x86_64/ncp-os-scc.yml
oc adm policy add-scc-to-user ncp-scc -z ncp-svc-account
oc adm policy add-scc-to-user ncp-scc -z nsx-node-agent-svc-account
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Edit the ReplicationController (RC) YML file,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vi /tmp/nsx-container-2.2.0.8740202/Kubernetes/ncp-rc.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Ensure the following lines are configured thus,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;serviceAccountName: ncp-svc-account
apiserver_host_port = 8443
apiserver_host_ip = 192.168.1.20
nsx_api_managers = 192.168.1.15
insecure = True
nsx_api_user = admin
nsx_api_password = VMware1!
cluster = dc-openshift
adaptor = openshift
enable_snat = True
tier0_router = 0d772616-4c44-47ae-ac9e-06f3c0222211
overlay_tz = 5eeefd4c-bd7d-4871-9eba-d7ed02394dec
container_ip_blocks = 562c85de-8675-4bb2-b211-3f95a6342e0e, f225d518-2fe3-4f8d-a476-a4697bff3ea6
external_ip_pools = d5095d53-c7f8-4fcd-9fad-3032afd080a4
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The NSX-T Container Plug-in (NCP) is a docker image which we import into the local registry.  The image is referenced by later script by different tag name so we add an additional tag.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker load -i /tmp/nsx-container-2.2.0.8740202/Kubernetes/nsx-ncp-rhel-2.2.0.8740202.tar
docker image tag registry.local/2.2.0.8740202/nsx-ncp-rhel nsx-ncp
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Then we can create NSX ReplicationController&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc project nsx-system
oc create -f /tmp/nsx-container-2.2.0.8740202/Kubernetes/ncp-rc.yml
oc describe rc/nsx-ncp
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We should now see the container running within pod namespace nsx-system.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc get pod --all-namespaces
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If all has gone well we can now connect to the NCP container and use the &lt;a href=&#34;https://docs.vmware.com/en/VMware-NSX-T/2.2/com.vmware.nsxt.ncp_openshift.doc/GUID-12F44CD5-0518-41C3-BB14-5507224A5D60.html&#34;&gt;nsxcli&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc exec -it nsx-ncp-6k5t2 nsxcli
get ncp-nsx status
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;nsx-t-container-plug-in-ncp-node-agent-daemonset-ds&#34;&gt;NSX-T Container Plug-in (NCP) Node Agent DaemonSet (DS)&lt;/h2&gt;
&lt;p&gt;Edit the nsx-node-agent-ds.yml file,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vi /tmp/nsx-container-2.2.0.8740202/Kubernetes/rhel_x86_64/nsx-node-agent-ds.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Ensure the following is set,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;serviceAccountName: nsx-node-agent-svc-account
cluster = dc-openshift
apiserver_host_port = 8443
apiserver_host_ip = 192.168.1.20
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once updated create the Node Agent Daemonset (DS),&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc login -u system:admin
oc apply -f /tmp/nsx-container-2.2.0.8740202/Kubernetes/rhel_x86_64/nsx-node-agent-ds.yml
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Check the Node Agent Daemonset is there,&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc describe daemonset.apps/nsx-node-agent
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We should also be able to see all of the OpenShift pods listed including our two NSX ones.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;oc get pod --all-namespaces
&lt;/code&gt;&lt;/pre&gt;&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;NAMESPACE&lt;/th&gt;
&lt;th&gt;NAME&lt;/th&gt;
&lt;th&gt;READY&lt;/th&gt;
&lt;th&gt;STATUS&lt;/th&gt;
&lt;th&gt;RESTARTS&lt;/th&gt;
&lt;th&gt;AGE&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;default&lt;/td&gt;
&lt;td&gt;docker-registry-1-4l59n&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;default&lt;/td&gt;
&lt;td&gt;persistent-volume-setup-grm9s&lt;/td&gt;
&lt;td&gt;0/1&lt;/td&gt;
&lt;td&gt;Completed&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;default&lt;/td&gt;
&lt;td&gt;router-1-5xtqg&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-dns&lt;/td&gt;
&lt;td&gt;kube-dns-bj5cq&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-proxy&lt;/td&gt;
&lt;td&gt;kube-proxy-9l8ql&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-system&lt;/td&gt;
&lt;td&gt;kube-controller-manager-localhost&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-system&lt;/td&gt;
&lt;td&gt;kube-scheduler-localhost&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-system&lt;/td&gt;
&lt;td&gt;master-api-localhost&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;kube-system&lt;/td&gt;
&lt;td&gt;master-etcd-localhost&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nsx-system&lt;/td&gt;
&lt;td&gt;nsx-ncp-9m2jl&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;10m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nsx-system&lt;/td&gt;
&lt;td&gt;nsx-node-agent-jlt5t&lt;/td&gt;
&lt;td&gt;2/2&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;4m&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openshift-apiserver&lt;/td&gt;
&lt;td&gt;openshift-apiserver-ptk5j&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openshift-controller-manager&lt;/td&gt;
&lt;td&gt;openshift-controller-manager-vg7gm&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openshift-core-operators&lt;/td&gt;
&lt;td&gt;openshift-web-console-operator-78ddf7cbb7-r8dhd&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;openshift-web-console&lt;/td&gt;
&lt;td&gt;webconsole-847bc4ccc4-hgsv4&lt;/td&gt;
&lt;td&gt;1/1&lt;/td&gt;
&lt;td&gt;Running&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1h&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;testing&#34;&gt;Testing&lt;/h2&gt;
&lt;pre&gt;&lt;code&gt;oc create namespace my-first
oc logs nsx-ncp-9m2jl | grep ERROR
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;nsx_ujo.k8s.ns_watcher Failed to create NSX topology for project my-first: Unexpected error from backend manager ([&amp;lsquo;192.168.1.15&amp;rsquo;]) for Allocate subnet from IP block&lt;/p&gt;
&lt;p&gt;more commands for working OpenShift here&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://static.rainfocus.com/vmware/vmworldus17/sess/148924638739800152Do/finalpresentationPDF/NET1522BU_FORMATTED_FINAL_1507910147966001nlDx.pdf&#34;&gt;https://static.rainfocus.com/vmware/vmworldus17/sess/148924638739800152Do/finalpresentationPDF/NET1522BU_FORMATTED_FINAL_1507910147966001nlDx.pdf&lt;/a&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Calling the NSX REST API with vRO and Navigating The XML Response</title>
      <link>https://darrylcauldwell.com/post/vro-nsx-rest/</link>
      <pubDate>Wed, 20 Jun 2018 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/vro-nsx-rest/</guid>
      <description>
        
          &lt;p&gt;While the NSX for vSphere plugin for vRealize Orchestrator is useful, occasionally there are limitations. For example when creating an NSX Edge Services Gateway we use this method.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NSXEdgeTrinityBasicController.createEdgeV4
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;That method returns void,  so next you need to make a second method call using filter by name to get the object and obtain object ID which is typically the input to other methods.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;NSXEdgeTrinityBasicController.getAllEdgeSummariesV4
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When creating NSX Distributed Logical Router it is the same method to create. However the second method to get the object fails as it cannot handle the additional property LogicalRouterScope which DLRs have, so throws an error.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;java.lang.NoSuchMethodException:com.vmware.vshield.edge.dto.trinity.LogicalRouterScope.&amp;lt;init&amp;gt;()
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After trying various other NSX plugin methods all have same issue so I changed approach. NSX offers all its capabilities via &lt;a href=&#34;https://docs.vmware.com/en/VMware-NSX-for-vSphere/6.4/nsx_64_api.pdf&#34;&gt;REST API&lt;/a&gt;. vRealize Orchestrator comes with a HTTP-REST plugin, so in theory we can add NSX Managers as HTTP-REST endpoint and call everything direct.&lt;/p&gt;
&lt;p&gt;If you have multiple REST HTTP hosts added you would want to bring back a list of these.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;restHosts = RESTHostManager.getHosts();
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This returns an array of UUIDs of each REST host, we can use these UUIDs to get the REST host objects. We could normally loop to find the specific &lt;a href=&#34;http://www.vroapi.com/Class/REST/2.2.2/RESTHost&#34;&gt;RESThost&lt;/a&gt; object we are looking for, but for ease here we would use the first item from array.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;host = RESTHostManager.getHost(restHosts[0]);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So we have the target &lt;a href=&#34;http://www.vroapi.com/Class/REST/2.2.2/RESTHost&#34;&gt;RESThost&lt;/a&gt; object look at forming the request using the &lt;a href=&#34;http://www.vroapi.com/Method/REST/2.2.2/RESTHost/createRequest&#34;&gt;createRequest&lt;/a&gt; method.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;request = host.createRequest(&#39;GET&#39;,&#39;/api/4.0/edges&#39;,&#39;&#39;);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once we have our request formed we can look to make the call using the &lt;a href=&#34;http://www.vroapi.com/Method/REST/2.2.2/RESTHost/executeRequestWithCredentials&#34;&gt;executeRequestWithCredentials&lt;/a&gt; method. Here I store username and password as Secure String variables.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;responseStr = request.executeWithCredentials(username,password);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This returns a &lt;a href=&#34;http://www.vroapi.com/Class/REST/2.2.2/RESTResponse&#34;&gt;RESTResponse&lt;/a&gt; object, this has attribute contentAsString to navigate this XML we first convert this string to a &lt;a href=&#34;https://www.w3schools.com/XML/dom_document.asp&#34;&gt;XML DOM Document Object&lt;/a&gt;. vRealize Orchestrator comes with a &lt;a href=&#34;http://www.vroapi.com/Plugin/XML/7.0.1&#34;&gt;XML plugin&lt;/a&gt; we can use the &lt;a href=&#34;http://www.vroapi.com/Method/XML/7.0.1/XMLManager/fromString&#34;&gt;XMLManager.fromString&lt;/a&gt; method to perform this converstion.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;responseXml = XMLManager.fromString(responseStr.contentAsString);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If we look inside of the XML which is returned from /api/4.0/edges we can see each the hierachy that each NSX Edge is a XML Node called edgeSummary and within that Node are child nodes for all of its attributes.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;pagedEdgeList&amp;gt;
    &amp;lt;edgePage&amp;gt;
        &amp;lt;edgeSummary&amp;gt;
            &amp;lt;objectId&amp;gt;
            &amp;lt;objectTypeName&amp;gt;
            &amp;lt;vsmUuid&amp;gt;
            &amp;lt;nodeId&amp;gt;
            &amp;lt;Revision&amp;gt;
            &amp;lt;type&amp;gt;
            &amp;lt;name&amp;gt;
            ...
        &amp;lt;edgeSummary&amp;gt;
            &amp;lt;objectId&amp;gt;
            &amp;lt;objectTypeName&amp;gt;
            &amp;lt;vsmUuid&amp;gt;
            &amp;lt;nodeId&amp;gt;
            &amp;lt;Revision&amp;gt;
            &amp;lt;type&amp;gt;
            &amp;lt;name&amp;gt;
            ...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So we need to bring back all the edgeSummary Nodes and create a &lt;a href=&#34;http://www.vroapi.com/Class/XML/7.0.1/XMLNodeList&#34;&gt;XMLNodeList&lt;/a&gt; object containing all of out Edges.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;edgeNodeList = responseXml.getElementsByTagName(&amp;quot;edgeSummary&amp;quot;);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once we have this we can loop around the children of this and find the correct Edge entry and then get the objectId. We know the child order is always the same so we can choose data from positon in list and output the text content of Node.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;for (var i = 0 ; i &amp;lt; edgeNodeList.length ; i++) {
    var node = edgeNodeList.item(i) ;
    edgeChildNodeList = node.getChildNodes();
    if (edgeChildNodeList.item(6).textContent == edgeName) {
        var objectId = edgeChildNodeList.item(0).textContent ;
        System.log(&amp;quot;Edge named &amp;quot; + edgeName + &amp;quot; has objectId &amp;quot; + objectId) ;
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With this technique we should be able to get around any vRealize Orchestrator NSX Plugin limitation. This would also be useful for calling other RESTful APIs in absence of a specific vRealize Orchestrator plugin.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>vRealize Orchestrator NSX Plug-in Troubleshooting</title>
      <link>https://darrylcauldwell.com/post/vro-nsx-logging/</link>
      <pubDate>Tue, 06 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/vro-nsx-logging/</guid>
      <description>
        
          &lt;p&gt;The NSX-V Plug-in for vRealize Orchestrator offers some great functionally, however creating custom workflows caused me some headaches.&lt;/p&gt;
&lt;p&gt;When building some custom workflows using the vRO API explorer there are inconsistencies, I&amp;rsquo;ve learned to not rely on the vRO API explorer for NSX plug-in documentation and instead use &lt;a href=&#34;http://www.vroapi.com/Plugin/NSX/1.2.0&#34;&gt;www.vroapi.com&lt;/a&gt; which is more complete.&lt;/p&gt;
&lt;p&gt;Problems manifest themselves as failures to execute methods, but very little detail is returned with the error message. Within vRealize Orchestrator is a JavaScript runtime environment therefore we can install a plug-in such as &lt;a href=&#34;https://labs.vmware.com/flings/vco-cli&#34;&gt;vCO-CLI&lt;/a&gt; to get an exploratory programming interface and try the commands directly to see the return values. While this was useful it didn&amp;rsquo;t help with all issues.&lt;/p&gt;
&lt;p&gt;The NSX plug-in talks to NSX Manager via the REST API, so another avenue to follow is that you can enable HttpClient logging which captures the REST calls made by the Plug-in in real-time. After some searching &lt;a href=&#34;https://www.vcoteam.info/articles/learn-vco/199-how-to-handle-vcenter-orchestrator-logs.html&#34;&gt;I found vRealize Orchestrator&lt;/a&gt; uses &lt;a href=&#34;https://logging.apache.org/log4j/2.x/&#34;&gt;Apache log4j&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We can edit the log4j configuration file&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;/etc/vco/app-server/log4j.xml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Within this configuration file we find section for the HttpClient org.apache.http&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;category&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;additivity=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;name=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;org.apache.http&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;priority&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;value=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;INFO&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/category&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can enable HttpClient debugging by amending this to have value of DEBUG.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-xml&#34; data-lang=&#34;xml&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;category&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;additivity=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;name=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;org.apache.http&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;priority&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;value=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;DEBUG&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&amp;gt;&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;/category&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once the vRealize Orchestrator server services are restarted the logging level will change and events will be written to the following log file.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;/var/log/vco/app-server/integration-server.log
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Using a combination of these methods has enabled resolution of all issues experienced up to now. To note putting a vRealize Orchestrator server into debug mode will slow down the vRealize Orchetrator server considerably. It is recommended to use DEBUG mode temporarily.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Controlling vSphere &amp; NSX-V With Python</title>
      <link>https://darrylcauldwell.com/post/nsx-python/</link>
      <pubDate>Mon, 15 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-python/</guid>
      <description>
        
          &lt;p&gt;My former colleagues had made me aware of pyVmomi an open source library which VMware provide and mostly maintain for managing vSphere, so its here I shall start. Since then NSX for vSphere has also an open source library NSX RAML Client provided by VMware so I&amp;rsquo;ll then move to that.&lt;/p&gt;
&lt;p&gt;I am performing this learning exercise in my home lab  using is vSphere 6.5, vSAN 6.5, NSX6.3, with Python 2.7.10, although this should work the same with other versions.&lt;/p&gt;
&lt;p&gt;Install pyVmomi and open vCenter connection and then initiate an interactive python environment&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;git clone https://github.com/vmware/pyvmomi.git
sudo pip install -r ~/pyvmomi/requirements.txt
python
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Import the pyVim, pyVmomi &amp;amp; SSL libraries we are using,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; ssl
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pyVim &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; connect
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pyVmomi &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; vim, vmodl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Open connection to vCenter then gather contents as an object&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;vcenter &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; connect&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;SmartConnect(
    host&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;192.168.1.13&amp;#39;&lt;/span&gt;,
    user&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;administrator@vsphere.local&amp;#39;&lt;/span&gt;,
    pwd&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;VMware1!&amp;#39;&lt;/span&gt;,
    port&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;443&lt;/span&gt;,
    sslContext&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;ssl&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_create_unverified_context()
)
content &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; vcenter&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;RetrieveContent()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;basic-get-of-information&#34;&gt;Basic get of information&lt;/h1&gt;
&lt;p&gt;Once we have vCenter Object Model as content object we can output any part of this data&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;content.about.fullName
&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;VMware vCenter Server 6.5.0 build-5178943&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can also explore the Object Model which is well descrived here in the &lt;a href=&#34;http://pubs.vmware.com/vsphere-65/topic/com.vmware.wssdk.apiref.doc/right-pane.html&#34;&gt;vSphere SDK API Docs&lt;/a&gt; and when we know what we want to look for we can search and display anything we like, for example the list of virtual machines.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;objView &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; content&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;viewManager&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;CreateContainerView(content&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rootFolder,[vim&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;VirtualMachine],True)
vmList &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; objView&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;view
objView&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Destroy()
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt;  vm &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; vmList:
    vm&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;summary&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;config&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;basic-put-of-configuration-information&#34;&gt;Basic put of configuration information&lt;/h1&gt;
&lt;p&gt;As well as getting information from the Object Model we can just as easily apply configuration to items within (assuming the account we connect with has sufficient rights),  for example if we gather the list of hosts and set a advanced option on all of them.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;objView &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; content&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;viewManager&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;CreateContainerView(content&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rootFolder,[vim&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;HostSystem],True)
hostList &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; objView&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;view
objView&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Destroy()
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; host &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; hostList:
    optionManager &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; host&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;configManager&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;advancedOption
    option &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; vim&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;option&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;OptionValue(key&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;VSAN.ClomRepairDelay&amp;#39;&lt;/span&gt;, value&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;long(&lt;span style=&#34;color:#ae81ff&#34;&gt;120&lt;/span&gt;))
    optionManager&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;UpdateOptions(changedValue&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[option])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;nsx-for-vsphere&#34;&gt;NSX for vSphere&lt;/h1&gt;
&lt;p&gt;So we have the pyVmomi library for vSphere, in addition to this VMware have provided open source library for &lt;a href=&#34;https://github.com/vmware/nsxramlclient&#34;&gt;NSX for vSphere&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll first make sure the packages are installed along with the additional packages for managing NSX.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;sudo pip install nsxramlclient pyvim pyvmomi lxml requests
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The NSX for vSphere REST API changes with each version, so in order to use the nsxramlclient library we will need a RAML file specific to version of NSX-V we are connecting to. The RAML file also produces nice &lt;a href=&#34;https://htmlpreview.github.io/?https://github.com/vmware/nsxraml/blob/6.3/html-version/nsxvapi.html&#34;&gt;dynamic documentation of the NSX APIs&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;git clone https://github.com/vmware/nsxraml
cd nsxraml
git checkout 6.3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So now we can try and connect and get some information about anything described in the API document, like NSX Controllers.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; nsxramlclient.client &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; NsxClient
nsx_manager &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;192.168.1.18&amp;#34;&lt;/span&gt;
nsx_username &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;admin&amp;#34;&lt;/span&gt;
nsx_password &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;VMware1!VMware1!&amp;#34;&lt;/span&gt;
nsxraml_file &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;nsxvapi.raml&amp;#39;&lt;/span&gt;
nsx_manager_session &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; NsxClient(nsxraml_file, nsx_manager, nsx_username, nsx_password)
nsx_controllers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nsx_manager_session&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;nsxControllers&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;read&amp;#39;&lt;/span&gt;)
nsx_controllers
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;When it comes to putting and posting information getting the formatting right can be a challenge. To this end with the library it is possible to create a template python dictionary using extract_resource_body_example.  Once we have this we can display the output structure but more usefully we can also substitute values into the template.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;new_ls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nsx_manager_session&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;extract_resource_body_example(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;logicalSwitches&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;create&amp;#39;&lt;/span&gt;)
nsx_manager_session&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;view_body_dict(new_ls)
new_ls[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;virtualWireCreateSpec&amp;#39;&lt;/span&gt;][&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;TestLogicalSwitch1&amp;#39;&lt;/span&gt;
new_ls[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;virtualWireCreateSpec&amp;#39;&lt;/span&gt;][&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;description&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;TestLogicalSwitch1&amp;#39;&lt;/span&gt;
new_ls[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;virtualWireCreateSpec&amp;#39;&lt;/span&gt;][&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;tenantId&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Tenant1&amp;#39;&lt;/span&gt;
new_ls[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;virtualWireCreateSpec&amp;#39;&lt;/span&gt;][&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;controlPlaneMode&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;UNICAST_MODE&amp;#39;&lt;/span&gt;
nsx_manager_session&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;view_body_dict(new_ls)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once we have out body template correctly describing what we want we can post this and if all goes to plan create a new Logical Switch. In this example I am passing in the scopeId (transport zone) manually to keep it a simple example.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;new_ls_response &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nsx_manager_session&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;create(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;logicalSwitches&amp;#39;&lt;/span&gt;, uri_parameters&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;scopeId&amp;#39;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;vdnscope-1&amp;#39;&lt;/span&gt;}, request_body_dict&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;new_ls)
nsx_manager_session&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;view_response(new_ls_response)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;summary&#34;&gt;Summary&lt;/h1&gt;
&lt;p&gt;It looks like with our new found friend the VMware Python libraries we can easily create and deploy a VMware configuration &amp;lsquo;infrastructure as code&amp;rsquo;.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Safely Lockdown NSX Distributed Firewall (DFW) Ruleset</title>
      <link>https://darrylcauldwell.com/post/nsx-dfw-lockdown/</link>
      <pubDate>Tue, 05 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-dfw-lockdown/</guid>
      <description>
        
          &lt;p&gt;A common dilemma when developing a solution with firewall is whether to change the Default rule to Deny at the start and develop the ruleset as part of development or leave the Default rule to Allow and secure it later. In modern agile teams its best to develop the ruleset as part of development ensuring the ruleset is tested with the product as introducing it later could well invalidate every bit of testing performed.&lt;/p&gt;
&lt;p&gt;If however you find yourself in the situation where a NSX firewall solution is deployed with the Default rule to Allow and your asked to implement a ruleset to cover the traffic and change default to Deny. This is one possible solution to capture the required configuration.&lt;/p&gt;
&lt;h2 id=&#34;enable-default-rule-logging&#34;&gt;Enable Default Rule Logging&lt;/h2&gt;
&lt;p&gt;In order we can capture the active traffic we can first enable Logging on the default rule.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/dFW-EnableLogging.jpg&#34; alt=&#34;NSX Enable Logging&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can then operate the environment normally for a period of time which captures all business processes. This maybe a day, a week, a month or more.&lt;/p&gt;
&lt;h2 id=&#34;centralize-logging&#34;&gt;Centralize Logging&lt;/h2&gt;
&lt;p&gt;NSX data plane logging is written to the VMkernel.log files,  therefore if a logical firewall rule log is generated for a vNIC of a VM it is written to the ESX host log file which it was residing at that time.&lt;/p&gt;
&lt;p&gt;The distributed firewall configuration can apply to the whole vCenter and all objects within. You must therefore configure the remote syslog server for each host in each cluster that has firewall enabled. The remote syslog server is specified in the &lt;em&gt;Syslog.global.logHost&lt;/em&gt; attribute. My preference is to use vRealize Log Insight for centralized syslog.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/dFW-EnableRemoteLogging.jpg&#34; alt=&#34;NSX Remote Logging&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;identifying-traffic-hitting-default-allow&#34;&gt;Identifying Traffic Hitting Default Allow&lt;/h2&gt;
&lt;p&gt;When we browse the firewall log entries we find that in order to narrow our search to the correct rule we need to establish the &lt;em&gt;vmw_nsx_firewall_ruleid&lt;/em&gt; for the default layer 3 rule which we are logging. The &lt;em&gt;vmw_nsx_firewall_ruleid&lt;/em&gt; is not displayed via the NSX Firewall GUI but can be easily got from the NSX REST API by running the GET method on this URL&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;https://&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;nsx-mgr-ip&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;/api/4.0/firewall/globalroot-0/config?ruleType&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;LAYER3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/dFW-DefaultRuleID.jpg&#34; alt=&#34;NSX Default Rule ID&#34;&gt;&lt;/p&gt;
&lt;p&gt;We see in this example the &lt;em&gt;rule id=&amp;ldquo;1001&amp;rdquo;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Once we have this we can create a vRealize Log Insight query to list all logs generated by this rule.  To make this query easier to view I remove all columns except timestamp and vmw_nsx_firewall.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/dFW-LI-Query.jpg&#34; alt=&#34;Query Firewall Events Log Insight &#34;&gt;&lt;/p&gt;
&lt;p&gt;From this data we can then identify what traffic would be blocked if we changed the default rule to Deny. We can work through this data, identify valid traffic flows. We can then put in explicit allow rules for the valid traffic. When we are happy no traffic is being logged by the Default Rule we can change it to Deny.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Deploying NSX-V With Ansible</title>
      <link>https://darrylcauldwell.com/post/nsx-install-ansible/</link>
      <pubDate>Wed, 08 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-install-ansible/</guid>
      <description>
        
          &lt;p&gt;Here I will describe the steps taken to deploy NSX Manager using the &lt;a href=&#34;https://github.com/vmware/nsxansible&#34;&gt;Ansible NSX Module&lt;/a&gt;. The NSX Ansible module is written by VMware and is provided opensource on GitHub. To work properly this depends on the &lt;a href=&#34;https://github.com/vmware/nsxraml&#34;&gt;NSX RAML Specification&lt;/a&gt;, &lt;a href=&#34;https://github.com/vmware/nsxramlclient&#34;&gt;NSX RAML Python Client&lt;/a&gt;, &lt;a href=&#34;https://github.com/vmware/pyvmomi&#34;&gt;vSphere API Python Bindings&lt;/a&gt; and the &lt;a href=&#34;https://www.vmware.com/support/developer/ovf/&#34;&gt;OVF Tool&lt;/a&gt; all being installed on the Ansible server.&lt;/p&gt;
&lt;p&gt;The following assumes you have a working Ansible installation already, and a vSphere environmentto install NSX to. If you don&amp;rsquo;t yet have these you can see how I performed my &lt;a href=&#34;%7B%7Bsite.url%7D%7D/how-to-setup-an-ansible-test-lab-for-windows-managed-nodes-custom-windows-modules/&#34;&gt;Ansible Installation&lt;/a&gt; in this earlier blog post.&lt;/p&gt;
&lt;h2 id=&#34;nsx-raml-specification&#34;&gt;NSX RAML Specification&lt;/h2&gt;
&lt;p&gt;As the NSX REST API changes with each release the REST API Markup Language (RAML) specification for NSX is provided as a different branch of the GitHub repository.  In my environment I will be using NSX 6.2.2 so first I ensure the git client is installed to my Ansible server and then I use this to take a clone of the correct branch.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;yum install git-all
git clone -b 6.2.2 https://github.com/vmware/nsxraml.git /nsxraml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;nsx-raml-python-client&#34;&gt;NSX RAML Python Client&lt;/h2&gt;
&lt;p&gt;The NSX RAML python client is series of functions which can be used standalone or in our case called by the Ansible NSX module.  To install these we need to ensure the &amp;lsquo;Python Package Manger&amp;rsquo; and some other tools are installed and available on our Ansible server.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;yum install python-pip gcc libxslt-devel python-devel pyOpenSSL
pip install --upgrade pip
pip install lxml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Once thes pre-requiste components are in place we can then install the NSX RAML python client. Similar to the RAML specification, the client functions are also dependant on version. The version which works with NSX RAML Specification 6.2.2 is Python client 1.0.4.  To install this version from Python package manager we use.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;pip install nsxramlclient&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;1.0.4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you have NSX Manager already deployed you can create a session to this using the python client.  First start python by running&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;python
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;then you can run the commands similar to this to create a session.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; nsxramlclient.client &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; NsxClient
nsxraml_file &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;/nsxraml/nsxvapi.raml&amp;#39;&lt;/span&gt;
nsxmanager &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;nsx.darrylcauldwell.local&amp;#39;&lt;/span&gt;
nsx_username &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;admin&amp;#39;&lt;/span&gt;
nsx_password &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;VMware!&amp;#39;&lt;/span&gt;
client_session &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; NsxClient(nsxraml_file, nsxmanager, nsx_username, nsx_password, debug&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;False)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;you can then see the session by running the following command in python session&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;client_session
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-install-ansible-RAMLclient.jpg&#34; alt=&#34;NSX RAML Client&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;vmware-vsphere-api-python-bindings&#34;&gt;VMware vSphere API Python Bindings&lt;/h2&gt;
&lt;p&gt;As well as the NSX Python client the Ansible NSX Module also depends on the VMware vSphere API Python Bindings (pyVmomi. pyVmomi is the Python SDK for the VMware vSphere API that allows you to manage ESX, ESXi, and vCenter. This is similarly installed with the python package manager using command like.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;pip install pyvmomi
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;ovf-tool&#34;&gt;OVF Tool&lt;/h2&gt;
&lt;p&gt;The final thing to be installed for the NSX Module to operate correctly is the VMware OVF Tool. The OVF tool for Linux version 4.1.0 is &lt;a href=&#34;https://my.vmware.com/group/vmware/details?downloadGroup=OVFTOOL410&amp;amp;productId=491#&#34;&gt;available here&lt;/a&gt;, please note a VMware login is required to get this.&lt;/p&gt;
&lt;p&gt;Once downloaded to the Ansbile server, we need to ensure it has execute attribute and then execute it to start the install, the commands to do this for the current version 4.1.0 are.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;chmod +x VMware-ovftool-4.1.0-2459827-lin.x86_64.bundle
./VMware-ovftool-4.1.0-2459827-lin.x86_64.bundle
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-install-ansible-ovftool.jpg&#34; alt=&#34;OVFTool&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once the install is running you have to agree to EULA, to get to the end of the text, hold down the Space bar. When prompted &lt;em&gt;Do you agree?&lt;/em&gt; type yes.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-install-ansible-ovftool2.jpg&#34; alt=&#34;OVFTool&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once OVF Tool is installed we can use SCP to copy the NSX OVA, VMware-NSX-Manager-6.2.2-3604087.ova, to Ansible server I placed this in folder named /OVAs.&lt;/p&gt;
&lt;h2 id=&#34;ansible-nsx-module&#34;&gt;Ansible NSX Module&lt;/h2&gt;
&lt;p&gt;We already have Git installed for dowlnloading the NSX RAML Specification so we can use this to clone the NSX Ansible repository.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;git clone https://github.com/vmware/nsxansible.git /nsxansible
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;deploy-nsx-manager&#34;&gt;Deploy NSX Manager&lt;/h2&gt;
&lt;p&gt;The NSX Module comes supplied with some example playbooks for performing common tasks, we’ll first take a copy of the example to deploy NSX Manager&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;cp /nsxansible/test_deploynsxova.yml /nsxansible/darryl_deploynsxova.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can then edit the contents to match the variables we plan to deploy in environment. While most of playbook contents are environmental specific varibles its worth noting that we run this module against the Ansible server itself as this is where the OVA and ovftool are located so hosts: value will always be localhost.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-install-ansible-NSXmgr.jpg&#34; alt=&#34;Deploy NSX Manager&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once we have our environmental specific entries set we can execute the playbook with Ansible.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;ansible-playbook darryl_deploynsxova.yml
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-install-ansible-playbook.jpg&#34; alt=&#34;Deploy NSX Manager Playbook&#34;&gt;&lt;/p&gt;
&lt;p&gt;We see this deploys ‘NSX Manager’ and configures the setting specified in the playbook.&lt;/p&gt;
&lt;h2 id=&#34;deploy-nsx-manager-and-register-with-vcenter-and-sso&#34;&gt;Deploy NSX Manager and register with vCenter and SSO&lt;/h2&gt;
&lt;p&gt;As with any Ansible playbook we can put common variables in a central location and call these from playbooks. An example is provided called answerfile-deployLab.yml. Variable names are overlapped between parts of the NSX Modules, in order that they are unique in the central answer file the names vary but its very easy to match these.&lt;/p&gt;
&lt;p&gt;An example of my playbook to deploy NSX Manager and then register this with vCenter and SSO.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-install-ansible-mgr-cfg.jpg&#34; alt=&#34;Deploy and Configure NSX Manager Playbook&#34;&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Ansibile NSX Configuration - Infrastructure As Code</title>
      <link>https://darrylcauldwell.com/post/nsx-ansible-iac/</link>
      <pubDate>Tue, 07 Jun 2016 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-ansible-iac/</guid>
      <description>
        
          &lt;p&gt;With the rise of DevOps culture and the ethos of automate all the things. Using test driven development techniques to apply application configuration declaratively has become widespread. The ever closer working relations of developers and infrastructure operations staff to become an application centric delivery and management unit has lead to many benefits.  One of the benefits is the shared empathy for each others pain points and understanding of how each approaches tackling these. It is clear that managing configuration as code has many benefits for consistent repeated delivery of applications.&lt;/p&gt;
&lt;p&gt;Tools which are commonly used for deploying applications with declarative configuration include Puppet, Chef and Ansible. The processes for distributed source control are also mature and with Git, Bitbucket and Apache Subversion there is a DVCS for most use-cases. As we work as one delivery team sharing tools and repositories is natural as with this we can look to deploy our infrastructure and application configuration with a single tool and avoid issues with interoperability and hand off.&lt;/p&gt;
&lt;p&gt;VMware NSX offers many things to many people with this rich feature set comes complexity in configuration.  At VMworld Europe 2015 I was introduced to Yves Fauser and attended his session &lt;a href=&#34;https://vmworld2015.lanyonevents.com/connect/sessionDetail.ww?SESSION_ID=4972&amp;amp;tclass=popup&#34;&gt;‘NET4972 – Incorporating VMware NSX in your DevOps Toolchain – Network Programmability with Python and Ansible‘.&lt;/a&gt;  He had created an [NSX RAML specification]((&lt;a href=&#34;http://github.com/vmware/nsxraml),&#34;&gt;http://github.com/vmware/nsxraml),&lt;/a&gt; a &lt;a href=&#34;http://github.com/vmware/nsxramlclient.&#34;&gt;Python NSX RAML client&lt;/a&gt; he then brings these altogether into a usable form by way of an &lt;a href=&#34;https://github.com/vmware/nsxansible&#34;&gt;NSX Ansible module.&lt;/a&gt;  The &lt;a href=&#34;https://github.com/vmware/nsxansible&#34;&gt;Ansible module&lt;/a&gt; offers examples which give the ability to NSX Manager, configure NSX to vCenter integration, configure VXLAN, deploy NSX Controllers and then deploy some logical switches.&lt;/p&gt;
&lt;p&gt;I explorered this capability in [this follow up post.]({{ site.url }}/deploy-nsx-from-ansible)&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Configuring NSX Edge to DLR OSPF</title>
      <link>https://darrylcauldwell.com/post/nsx-edge-ospf/</link>
      <pubDate>Mon, 23 May 2016 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-edge-ospf/</guid>
      <description>
        
          &lt;p&gt;As part of study for VCIX-NV I’ve given myself task of exploring in my new home lab all parts of NSX which I&amp;rsquo;m still not fully comfortable.  One of these things is OSPF,  to investigate this I came up with a test scenario and then worked through the steps to achieve a solution to meet the scenario design.&lt;/p&gt;
&lt;h2 id=&#34;scenario-design&#34;&gt;Scenario Design&lt;/h2&gt;
&lt;p&gt;We will replicate a typical development environment secured with an Edge Gateway.  Within the environment the developers will have ability to add and remove logical switches to the DLR and the automatic updating of the Edge Gateway routing table.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-test.jpg&#34; alt=&#34;NSX Edge OSPF Test&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;deploy-edge-transit-logical-switch-and-distributed-logical-router&#34;&gt;Deploy Edge, Transit Logical Switch and Distributed Logical Router&lt;/h2&gt;
&lt;p&gt;The lab has VXLAN capability during my NSX core home lab configuration, so has hosts prepared, VNI pool and deployed Controller.&lt;/p&gt;
&lt;p&gt;We&amp;rsquo;ll first deploy the Transit logical switch.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-ls.jpg&#34; alt=&#34;NSX Edge Logical Switch&#34;&gt;&lt;/p&gt;
&lt;p&gt;Create Edge Gateway with an Uplink interface on the OOB network  (192.168.1.43) and on the Transit network (192.168.2.1) .&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-edge-deploy.jpg&#34; alt=&#34;NSX Edge Deploy&#34;&gt;&lt;/p&gt;
&lt;p&gt;Create DLR initially with a single uplink on Transit network (192.168.2.254) , do not configure a default gateway.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-dlr.jpg&#34; alt=&#34;NSX Edge DLR&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;configure-ospf-area&#34;&gt;Configure OSPF Area&lt;/h2&gt;
&lt;p&gt;An OSPF area is a logical collection of OSPF networks, routers, and links that have the same area identification. The simulated development environment we are using RFC1918 address space in order that the IP address schema can overlap as such our OSPF routing will be internal so we will create an OSPF area which is type Stub.&lt;/p&gt;
&lt;p&gt;On Edge Gateway Enable Dynamic Routing Configuration on the Transit interface.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-dynamic.jpg&#34; alt=&#34;NSX Edge OSPF Dynamic&#34;&gt;&lt;/p&gt;
&lt;p&gt;Then we need to enable OSPF&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-enable.jpg&#34; alt=&#34;NSX Edge OSPF Enable&#34;&gt;&lt;/p&gt;
&lt;p&gt;We require a stub OSPF area, and usefully Area ID 51 is configured already so we&amp;rsquo;ll use this.  We just need to associate this with the Transit interface in order it can communicate with DLR.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-mapping.jpg&#34; alt=&#34;NSX Edge OSPF Mapping&#34;&gt;&lt;/p&gt;
&lt;p&gt;Repeat the steps used for configuring OSPF on the Edge on the DLR, when enabling OSPF enter the forwarding address to match the Transit network IP interface and the protocol address to be another available IP address on the Transit network.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-ProFwd.jpg&#34; alt=&#34;NSX Edge OSPF Protocol Forward&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;route-distribution&#34;&gt;Route Distribution&lt;/h2&gt;
&lt;p&gt;The default Route Redistribution configuration for DLR is to distribute information about networks it is connected to.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-RouteDistribute.jpg&#34; alt=&#34;OSPF Route Redistribution&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we now check the Edge Gateway routing table we can see all routing is of type C (directly connected) or S (static entry for default gateway).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-RouteTable.jpg&#34; alt=&#34;OSPF Route Table&#34;&gt;&lt;/p&gt;
&lt;p&gt;If I now simulate a developer creating a new Logical Switch for network 192.168.3.0/24 which is attached to DLR as internal interface 192.168.3.1.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-test1DLR.jpg&#34; alt=&#34;OSPF Test 1 DLR&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can then check the Edge Gateway routing table again.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-test1DLR.jpg&#34; alt=&#34;OSPF Test 1 Edge&#34;&gt;&lt;/p&gt;
&lt;p&gt;We see a new route added type O (ospf derived) N2 (OSPF NSSA external type 2).  Which directs this traffic to the DLR interface on Transit network.&lt;/p&gt;
&lt;p&gt;If we then simulate the developer adding the test2 and test3 networks. We see the other routes populated.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-edge-ospf-test2-3.jpg&#34; alt=&#34;OSPF Test 2-3&#34;&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>NSX Backup and Restore</title>
      <link>https://darrylcauldwell.com/post/nsx-backup-restore/</link>
      <pubDate>Fri, 20 May 2016 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-backup-restore/</guid>
      <description>
        
          &lt;p&gt;As part of study for VCIX-NV I&amp;rsquo;ve given myself task of exploring in my new home lab all parts of NSX which I don&amp;rsquo;t use at work. One of these things is NSX Backup and Restore,  its not that we don&amp;rsquo;t use these function but that its relatively high impacting if it goes wrong.  To investigate this I came up with a list of networking configuration to test backup and restore of and the steps to I followed to perform each backup and restore type.&lt;/p&gt;
&lt;h2 id=&#34;scenario-1---distributed-firewall&#34;&gt;Scenario 1 - Distributed Firewall&lt;/h2&gt;
&lt;p&gt;To backup and restore distributed firewall rules is relatively straight forwards to do.  I&amp;rsquo;m starting with no rules created.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-BlankRuleset.jpg&#34; alt=&#34;NSX Firewall Blank Ruleset&#34;&gt;&lt;/p&gt;
&lt;p&gt;Then create a new section.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-BandR.jpg&#34; alt=&#34;NSX Firewall New Section&#34;&gt;&lt;/p&gt;
&lt;p&gt;Add some rules for example any to WEB on HTTP Service and WEB to DB on SQL Service.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-testRules.jpg&#34; alt=&#34;NSX Firewall New Rules&#34;&gt;&lt;/p&gt;
&lt;p&gt;Click Export and save these as a file.&lt;/p&gt;
&lt;p&gt;Delete the WEB to DB rule.&lt;/p&gt;
&lt;p&gt;To perform a restore we do this from the Saved Configuration tab,  we see that there are only auto saved backups.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-Restore1.jpg&#34; alt=&#34;NSX Firewall Auto-Saved&#34;&gt;&lt;/p&gt;
&lt;p&gt;In order to restore from the file backup we need to import this into the Saved Configurations.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-Restore2.jpg&#34; alt=&#34;NSX Firewall Import Saved&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now we can change back to the Configuration tab and use &amp;lsquo;Load Saved Configuration&amp;rsquo; option to restore the exported file based backup.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-Restore3.jpg&#34; alt=&#34;NSX Firewall Restore&#34;&gt;&lt;/p&gt;
&lt;p&gt;The deleted rule then gets added back.&lt;/p&gt;
&lt;h2 id=&#34;scenario-2---nsx-manager&#34;&gt;Scenario 2 - NSX Manager&lt;/h2&gt;
&lt;p&gt;To backup and restore NSX Manager it depends on having access to a FTP or SFTP to store the configuration in.&lt;/p&gt;
&lt;p&gt;Previously I&amp;rsquo;d created a CentOS7 virtual machine template,  I deployed one of these and then followed this procedure to configure FTP.&lt;/p&gt;
&lt;p&gt;Configure the FTP server settings in NSX.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-nsxBackup.jpg&#34; alt=&#34;NSX Manager Configuration Backup&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once backup location is configured click Backup to create a backup.&lt;/p&gt;
&lt;p&gt;Check the FTP server and it should have a folder called nsx created and within a file prefixed with nsx followed by the date and time stamp.&lt;/p&gt;
&lt;p&gt;Make a change to NSX configuration,  for example assign the NSX Manager the primary role.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-PrimaryRole.jpg&#34; alt=&#34;NSX Manager Assign Primary Role&#34;&gt;&lt;/p&gt;
&lt;p&gt;Check role is applied.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-Primary.jpg&#34; alt=&#34;NSX Manager Is Primary&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we now restore the configuration from the NSX Backup we should see the NSX Manager revert to running the Standalone role.&lt;/p&gt;
&lt;p&gt;Select the backup created prior to making the configuration change and click Restore.  The NSX Manager will restore and then restart the Management Services so will take a minute or two. We can then check the role is reverted to Standalone.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-Standalone.jpg&#34; alt=&#34;NSX Manager Is Standalone&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;scenario-3---vsphere-distributed-switch&#34;&gt;Scenario 3 - vSphere Distributed Switch&lt;/h2&gt;
&lt;p&gt;I have in lab a vSphere Distributed Switch configured with some Distributed Portgroups and Logical Switches created on.  Otherwise you might need to create some configuration.&lt;/p&gt;
&lt;p&gt;Use the context menu of the vDistributed Switch to Export the configuration to a file.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-Export-vDS.jpg&#34; alt=&#34;Export VDS&#34;&gt;&lt;/p&gt;
&lt;p&gt;This creates a compressed zip file,  most files within this are not readable except for an xml file which appears to be a manifest of the zip file.&lt;/p&gt;
&lt;p&gt;To test that the configuration is restored add an additional distributed portgroup named TestRestore1.&lt;/p&gt;
&lt;p&gt;Use the context menu of the vDistributed Switch to Restore the configuration from the file.  Interesting we find that the Portgroup created post backup is not removed.&lt;/p&gt;
&lt;p&gt;If we export the configuration again to include the new TestRestore1 Portgroup. Then delete the TestRestore1 Portgroup. Then Restore the configuration we find the TestRestore1 Portgroup is added back.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-backup-TestRestore.jpg&#34; alt=&#34;Restored VDS&#34;&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>NSX-V Load Balancer As A Service LBaaS</title>
      <link>https://darrylcauldwell.com/post/nsx-lbaas/</link>
      <pubDate>Fri, 20 May 2016 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-lbaas/</guid>
      <description>
        
          &lt;p&gt;As part of study for VCIX-NV I&amp;rsquo;ve given myself task of exploring in my homelab all parts of NSX which I don&amp;rsquo;t use
at work. One of these things is NSX Load Balancer as a Service (LBaaS), to investigate this I came up with a test
scenario and then worked through the steps to achieve a solution to meet the design.&lt;/p&gt;
&lt;h2 id=&#34;load-balancer-scenariodesign&#34;&gt;Load Balancer Scenario Design&lt;/h2&gt;
&lt;p&gt;I started by reading the &lt;a href=&#34;http://pubs.vmware.com/NSX-62/index.jsp#com.vmware.nsx.admin.doc/GUID-412635AE-1F2C-4CEC-979F-CC5B5D866F53.html&#34;&gt;NSX Administration Guide&lt;/a&gt; section covering load balancer configuration.&lt;/p&gt;
&lt;p&gt;Here is what I plan to achieve my client and browser will sit on the 192.168.1.0/24 out of band network.  I&amp;rsquo;ll create a new WEB VLAN 1000 to host 192.168.2.0/24 network,  the Edge Gateway will serve this network with DHCP and DHCP will issue gateway IP to be its own address,  two web server VMs will be assigned to this network.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-onearm.jpg&#34; alt=&#34;One Armed Load Balancer&#34;&gt;&lt;/p&gt;
&lt;p&gt;A destination network address translation (DNAT) rule will be configured to pass this OOB network IP address to the load balancerd address on the WEB network. The load balancer will round robin traffic between the two web servers. Port 80 will be monitored on web servers and when apache is stopped on one server all traffic should direct to the active server. The Edge firewall will be configured to block all traffic except for HTTP to the web servers.&lt;/p&gt;
&lt;h2 id=&#34;configure-underlay&#34;&gt;Configure Underlay&lt;/h2&gt;
&lt;p&gt;Create VLAN 1000 on physical switch and ensure this is allowed in the trunk presented to the virtual distributed switch ports.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-VLAN1000-Assignment.jpg&#34; alt=&#34;VLAN 1000 Assignment&#34;&gt;&lt;/p&gt;
&lt;p&gt;Create a distributed portgroup for VLAN1000 and OOB.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-vDS-OOB-and-WEB.jpg&#34; alt=&#34;Distributed Portgroup&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;deploy-edge-gateway&#34;&gt;Deploy Edge Gateway&lt;/h2&gt;
&lt;p&gt;Create a Compact NSX Edge Gateway with an &lt;em&gt;Uplink&lt;/em&gt; interface on the OOB with IP address 192.168.1.40 and &lt;em&gt;Internal&lt;/em&gt; interface on the WEB - VLAN1000 with IP address 192.168.2.254 .  Specify 192.168.1.254 as default gateway on the OOB interface. Ensure firewall is set to accept as default policy.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-EdgeSummary.jpg&#34; alt=&#34;Edge Deployment Summary&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;create-dnat-rule&#34;&gt;Create DNAT Rule&lt;/h2&gt;
&lt;p&gt;As 192.168.2.0/24 is not a network my home router is aware of I must create a Destination NAT rule on 192.168.1.0/24 network to map to the 192.168.2.0/24 address.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-DNAT.jpg&#34; alt=&#34;Destination NAT&#34;&gt;&lt;/p&gt;
&lt;p&gt;Ensure this is created by connecting SSH session to the Edge and viewing the NAT configuration by running&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;show nat
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-NAT-CLI.jpg&#34; alt=&#34;Destination NAT&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;configure-web-server-dhcp&#34;&gt;Configure Web Server DHCP&lt;/h2&gt;
&lt;p&gt;Enable DHCP and create a DHCP scope 192.168.2.10-192.168.2.50.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-DHCP.jpg&#34; alt=&#34;DHCP&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;create-web-servers&#34;&gt;Create Web Servers&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;ll look to create two test web servers each will host a single web page with its actual IP address on.  When I connect to load balanced IP the page contents returned will flip between the two servers.&lt;/p&gt;
&lt;p&gt;I created a &lt;a href=&#34;https://darrylcauldwell.com/post/vsphere-vm-templates&#34;&gt;CentOS 7 VM template&lt;/a&gt; previously in lab and so for this test scenario I provisioned two new VMs from this named web1 and web2.  I left both as DHCP and setup the web server using following commands.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;yum install httpd -y
service httpd start
ifconfig | grep &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;inet&amp;#34;&lt;/span&gt; &amp;amp;gt; /var/www/html/test.html
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;configure-load-balancer&#34;&gt;Configure Load Balancer&lt;/h2&gt;
&lt;p&gt;Enable the load balance function on the Edge.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-LoadBalancer-Enable.jpg&#34; alt=&#34;DHCP&#34;&gt;&lt;/p&gt;
&lt;p&gt;Create a load balancer application profile called Web and mark is for HTTP traffic.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-LoadBalancer-AppProfile.jpg&#34; alt=&#34;Load Balancer AppProfile&#34;&gt;&lt;/p&gt;
&lt;p&gt;Add web1 and web2 virtual machines to a load balancer pool with monitor port 80.  If adding as VM object as per screenshot then you might have IPv6 issue see section below for solution,  alternatively enter IP address.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-LoadBalancer-PoolMembers.jpg&#34; alt=&#34;Load Balancer Pool Membership&#34;&gt;&lt;/p&gt;
&lt;p&gt;Create a load balancer virtual server to link all the parts into a VIP.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-WebLB.jpg&#34; alt=&#34;Load Balancer Virtual Server&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once this is created you will be able to browse to http://192.168.1.40/test.html on the OOB network this will return a web page displaying IP address.  Refreshing the page will display a similar page but with alternate VM IP address.&lt;/p&gt;
&lt;h2 id=&#34;nsx-loadbalancer-ipv6-issue&#34;&gt;NSX LoadBalancer IPv6 Issue&lt;/h2&gt;
&lt;p&gt;I found when I configured this initially by selecting VM objects in the Virtual Server the loadbalancer tried to configure all IP addresses including the IPv6 addresses which are present by default in Centos7.  This caused a error in LoadBalancer and wouldn&amp;rsquo;t allow me to proceed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-LoadBalance-IPv6.jpg&#34; alt=&#34;Load Balancer Virtual Server&#34;&gt;&lt;/p&gt;
&lt;p&gt;I disabled IPv6 on both CentOS7 VMs by adding the following to /etc/sysctl.conf:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;net.ipv6.conf.all.disable_ipv6 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
net.ipv6.conf.default.disable_ipv6 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then run the following to commit the changes.  When VMs had only IPv4 addresses the process works without error.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;sysctl -p
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;secure-solution&#34;&gt;Secure Solution&lt;/h2&gt;
&lt;p&gt;Up to now we have had the Edge firewall default rule set to allow,  one of the first tasks we would want to do once web load balancing is working is to secure the solution.&lt;/p&gt;
&lt;p&gt;To do this we create a new rule explicitly allowing HTTP traffic to the IP address it will enter the Edge on,  and once in place change default rule to Deny.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-LB-Firewall.jpg&#34; alt=&#34;Load Balancer Firewall&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once in place test you still have access to the web page.&lt;/p&gt;
&lt;h2 id=&#34;load-balance-monitoring&#34;&gt;Load Balance Monitoring&lt;/h2&gt;
&lt;p&gt;When creating the Loadbalance configuration we specified to monitor port 80 was available in order to include the server in the balanced set. Before proceeding here ensure that when you connect to 192.168.1.40 and refresh load balancing is working correctly and returning IP addresses alternately.&lt;/p&gt;
&lt;p&gt;We then stop the apache service on one of the VMs using:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;service httpd stop
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-lbaas-apache-stop.jpg&#34; alt=&#34;Apache Stop&#34;&gt;&lt;/p&gt;
&lt;p&gt;If we now go back to browser and refresh a few times it will direct all traffic to one server IP address.  If we restart apache using&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;service httpd start
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and refresh the browser we find load balancing is returned and refreshes alternate the IP addresses.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>NSX Activity Monitoring</title>
      <link>https://darrylcauldwell.com/post/nsx-activity/</link>
      <pubDate>Thu, 19 May 2016 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-activity/</guid>
      <description>
        
          &lt;p&gt;As part of study for VCIX-NV I&amp;rsquo;ve given myself task of exploring all parts of NSX which I don&amp;rsquo;t use at work. One of these things is NSX Activity Monitoring,  to investigate this I came up with a test scenario and then worked through the steps to achieve a solution to meet the scenario design.&lt;/p&gt;
&lt;h2 id=&#34;scenario-design&#34;&gt;Scenario Design&lt;/h2&gt;
&lt;p&gt;Gather traffic data between Activity Monitored virtual machines.&lt;/p&gt;
&lt;h2 id=&#34;virtual-machine-configuration&#34;&gt;Virtual Machine Configuration&lt;/h2&gt;
&lt;p&gt;I created a Windows 2012 R2 virtual machine template previously,  deploy two of these to a network with DHCP enabled for example in my lab the OOB network.&lt;/p&gt;
&lt;p&gt;The Windows template I created has the VMware Tools typical installation performed. To use the NSX Activity Monitoring the Network File Introspection Driver (vnetflt.sys) is required.  To add this we need to re-run the VMware tools installer,  select modify and then add NSX Network File Introspection Driver.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-activity-WindowsDriver.jpg&#34; alt=&#34;Install Windows Driver&#34;&gt;&lt;/p&gt;
&lt;p&gt;Add IIS role to one of the hosts.&lt;/p&gt;
&lt;p&gt;Add server to Active Directory domain and reboot.&lt;/p&gt;
&lt;p&gt;In vSphere web client select each virtual machine in turn and enable NSX Activity Monitoring.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-activity-EnableMonitoring.jpg&#34; alt=&#34;Enable Monitoring&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;deployactivity-monitor-appliances&#34;&gt;Deploy Activity Monitor Appliances&lt;/h2&gt;
&lt;p&gt;From the Networking and Security menu, select Installation. Navigate to the Service Deployments tab and glick green add button.&lt;/p&gt;
&lt;p&gt;In order to gather activity data virtual appliances are required to be deployed.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-activity-DeployActivityServices.jpg&#34; alt=&#34;Deploy Virtual Appliances&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;register-a-windows-domain-with-nsx-manager&#34;&gt;Register a Windows Domain with NSX Manager&lt;/h2&gt;
&lt;p&gt;In vSphere web client click Networking and Security and then click NSX Managers. Click an NSX Manager in the Name column and then click the Manage tab. Click the Domain tab and then click the Add domain (Add domain) icon.&lt;/p&gt;
&lt;p&gt;Complete the domain details as per your Active Directory.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-activity-Add-Domain.jpg&#34; alt=&#34;Add Domain&#34;&gt;&lt;/p&gt;
&lt;p&gt;Wait for synchronization status to show SUCCESS.&lt;/p&gt;
&lt;h2 id=&#34;generate-activity&#34;&gt;Generate Activity&lt;/h2&gt;
&lt;p&gt;Connect to test virtual machine which &lt;!-- raw HTML omitted --&gt;does not&lt;!-- raw HTML omitted --&gt; have IIS installed and open browser to virtual machine with IIS installed.&lt;/p&gt;
&lt;p&gt;Test data is being gathered by Activity Monitor.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-activity-ActivityMonitorActivity.jpg&#34; alt=&#34;Activity Monitor Activity&#34;&gt;&lt;/p&gt;
&lt;p&gt;Another easy test is to copy a file between the two virtual machines.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/nsx-activity-ActivityMonitorSMB-CIFS.jpg&#34; alt=&#34;Activity Monitor Activity&#34;&gt;&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>NSX Distributed Firewall Under The Covers</title>
      <link>https://darrylcauldwell.com/post/nsx-dfw/</link>
      <pubDate>Tue, 21 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/nsx-dfw/</guid>
      <description>
        
          &lt;p&gt;An NSX distributed firewall (dFW) runs as an ESXi host as a kernel module added as a VMware Installation Bundle (VIB). The dFW rules operate on Layer 2 through Layer 4; although this can be extended through Layer 7 by integrating with a 3-Party vendor.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;L2 rules are based on MAC address L2 protocols like ARP, RARP and LLDP etc.&lt;/li&gt;
&lt;li&gt;L3 and 4 rules are based on IP source/destination and L4 uses a TCP or UDP ports.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/dFW_TCP_OSI.png&#34; alt=&#34;TCP OSI&#34;&gt;&lt;/p&gt;
&lt;p&gt;The NSX dFW enforces a state full firewall service for VMs using the vNIC as the enforcement point. Every packet that leaves the VM (before VTEP encapsulation) or enters the VM (After VTEP de-encapsulation) can be inspected with a firewall policy.&lt;/p&gt;
&lt;p&gt;The ruleset is created and managed via NSX Manager either API or UI. The ESXi host has two dFW specific modules vShield Statefull Firewall and VMware Internetworking Service Insertion Platform (vSIP). vShield Statefull Firewall performs the following roles.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interact with NSX Manager to retrieve DFW policy rules.&lt;/li&gt;
&lt;li&gt;Gather DFW statistics information and send them to the NSX Manager.&lt;/li&gt;
&lt;li&gt;Send audit logs information to the NSX Manager.&lt;/li&gt;
&lt;li&gt;Receive configuration from NSX manager to create (or delete) DLR Control VM, create (or delete) ESG.&lt;/li&gt;
&lt;li&gt;Part of the host preparation process SSL related tasks from NSX manager&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;VMware Internetworking Service Insertion Platform is the distributed firewall kernel space module core component. The vSIP receives firewall rules via vShield State-full Firewall and downloads them down to each VMware-sfw. When VM connect to Logical switch there are security services each packet transverse which are implemented as IOChains processed at the Kernel level.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/dFW_Slots.png&#34; alt=&#34;TCP OSI&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Slot 0: Distributed Virtual Filter (DVFilter): It monitors the incoming and outgoing traffic on the protected virtual NIC and performs stateless filtering.&lt;/li&gt;
&lt;li&gt;Slot 1: Switch Security module (SW-sec): Learns VMs IP and MAC address. sw-sec is critical component capture DHCP Ack and ARP broadcast message and forward this info as unicast to NSX Controller to perform the ARP suppression feature. This is also the layer where NSX IP spoof guard is implemented.&lt;/li&gt;
&lt;li&gt;Slot 2: NSX Distributed Firewall (VMware-sfw): This is the place where DFW firewall rules are stored and enforced; VMware-sfw contains rules table and connections table received via vShield State-full Firewall&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The effect of the processing of these packets is that packet leaving the VM which doesn’t match firewall rules get removed before arriving at the vSphere Switch.&lt;/p&gt;

        
      </description>
    </item>
    
  </channel>
</rss>
