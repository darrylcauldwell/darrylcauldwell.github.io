<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>docker on </title>
    <link>https://darrylcauldwell.github.io/tags/docker/</link>
    <description>Recent content in docker on </description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 18 Nov 2019 00:00:00 +0000</lastBuildDate><atom:link href="https://darrylcauldwell.github.io/tags/docker/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Introduction To Docker Container Networking</title>
      <link>https://darrylcauldwell.github.io/post/docker-networking/</link>
      <pubDate>Mon, 18 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/docker-networking/</guid>
      <description>
        
          &lt;p&gt;When developing a cloud native application using Docker containers understanding network connectivity between containers and non-Docker workloads such as other servers and cloud APIs is essential. Docker’s networking subsystem is pluggable using drivers, several drivers exist by default, and others can be added easily.&lt;/p&gt;
&lt;h2 id=&#34;linking-containers-legacy&#34;&gt;Linking Containers (legacy)&lt;/h2&gt;
&lt;p&gt;Linking containers by name is a simple method of enabling communications between containers. To link a an app server to a DB we simply reference the db container name in the command we execute to run app server container.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -d --name my_mongodb mongo
docker run -d --link my_mongodb --name my_nodeapp -it node
docker ps -l

    CONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS               NAMES
    5dc2aa4d8542        node                &amp;quot;docker-entrypoint.s…&amp;quot;   7 seconds ago       Up 6 seconds                            my_nodeapp
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In order to facilitate communications Docker automatically creates environment variables in the target container. It also exposes all environment variables originating from Docker from the source container. If we view the IP address of the two containers and then view the environment variables of the app server we can see the details for the db server.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker inspect --format &#39;{{ .ID }} - {{ .Name }} - {{ .NetworkSettings.IPAddress }}&#39; my_mongodb
docker inspect --format &#39;{{ .ID }} - {{ .Name }} - {{ .NetworkSettings.IPAddress }}&#39; my_nodeapp

docker exec my_nodeapp env

    PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
    HOSTNAME=a88e6e3f1107
    MY_MONGODB_PORT=tcp://172.17.0.2:27017
    MY_MONGODB_PORT_27017_TCP=tcp://172.17.0.2:27017
    MY_MONGODB_PORT_27017_TCP_ADDR=172.17.0.2
    MY_MONGODB_PORT_27017_TCP_PORT=27017
    MY_MONGODB_PORT_27017_TCP_PROTO=tcp
    MY_MONGODB_NAME=/my_nodeapp/my_mongodb
    MY_MONGODB_ENV_GOSU_VERSION=1.11
    MY_MONGODB_ENV_JSYAML_VERSION=3.13.0
    MY_MONGODB_ENV_GPG_KEYS=E162F504A20CDF15827F718D4B7C549A058F8B6B
    MY_MONGODB_ENV_MONGO_PACKAGE=mongodb-org
    MY_MONGODB_ENV_MONGO_REPO=repo.mongodb.org
    MY_MONGODB_ENV_MONGO_MAJOR=4.2
    MY_MONGODB_ENV_MONGO_VERSION=4.2.1
    NODE_VERSION=13.1.0
    YARN_VERSION=1.19.1
    HOME=/root
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Container linking is a legacy method of connecting containers it is much more likely today that communications on single host will use a bridge network.&lt;/p&gt;
&lt;h2 id=&#34;container-networking-with-a-bridge-driver&#34;&gt;Container Networking With A Bridge Driver&lt;/h2&gt;
&lt;p&gt;A bridge network can be used on Docker host to enable communications between containers. The first step is to create a custom bridge network on the Docker host which the containers will connect to. Each Docker network gets assigned a CIDR range and has a IPAM capability, we can view these by inspecting the network.&lt;/p&gt;
&lt;p&gt;The default bridge network is present on all Docker hosts, if you do not specify a different network, new containers are automatically connected to the default bridge network. More than likely you will want to separate containers to different networks and so will want to create multiple networks and attach different containers to each.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker network create --driver bridge my_isolated_network
docker network ls
docker network inspect my_isolated_network

    [
        {
            &amp;quot;Name&amp;quot;: &amp;quot;my_isolated_network&amp;quot;,
            &amp;quot;Id&amp;quot;: &amp;quot;76e440d68a136ffe1ab9a4dd4977a7dc0499167814de14bf4bedfaffb646197b&amp;quot;,
            &amp;quot;Created&amp;quot;: &amp;quot;2019-11-18T09:59:09.8310474Z&amp;quot;,
            &amp;quot;Scope&amp;quot;: &amp;quot;local&amp;quot;,
            &amp;quot;Driver&amp;quot;: &amp;quot;bridge&amp;quot;,
            &amp;quot;EnableIPv6&amp;quot;: false,
            &amp;quot;IPAM&amp;quot;: {
                &amp;quot;Driver&amp;quot;: &amp;quot;default&amp;quot;,
                &amp;quot;Options&amp;quot;: {},
                &amp;quot;Config&amp;quot;: [
                    {
                        &amp;quot;Subnet&amp;quot;: &amp;quot;172.18.0.0/16&amp;quot;,
                        &amp;quot;Gateway&amp;quot;: &amp;quot;172.18.0.1&amp;quot;
                    }
                ]
            },
            &amp;quot;Internal&amp;quot;: false,
            &amp;quot;Attachable&amp;quot;: false,
            &amp;quot;Ingress&amp;quot;: false,
            &amp;quot;ConfigFrom&amp;quot;: {
                &amp;quot;Network&amp;quot;: &amp;quot;&amp;quot;
            },
            &amp;quot;ConfigOnly&amp;quot;: false,
            &amp;quot;Containers&amp;quot;: {},
            &amp;quot;Options&amp;quot;: {},
            &amp;quot;Labels&amp;quot;: {}
        }
    ]
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;When networks are created you can start containers in those bridge networks.  The containers in same isolated network can then communicate by referencing name. When containers are running and connected we can inspect the network again and get the container IP address assignments etc.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -d --net=my_isolated_network --name my_mongodb mongo
docker run -d --net=my_isolated_network --name my_nodeapp -it node
docker network inspect my_isolated_network

    [
        {
            &amp;quot;Name&amp;quot;: &amp;quot;my_isolated_network&amp;quot;,
            &amp;quot;Id&amp;quot;: &amp;quot;76e440d68a136ffe1ab9a4dd4977a7dc0499167814de14bf4bedfaffb646197b&amp;quot;,
            &amp;quot;Created&amp;quot;: &amp;quot;2019-11-18T09:59:09.8310474Z&amp;quot;,
            &amp;quot;Scope&amp;quot;: &amp;quot;local&amp;quot;,
            &amp;quot;Driver&amp;quot;: &amp;quot;bridge&amp;quot;,
            &amp;quot;EnableIPv6&amp;quot;: false,
            &amp;quot;IPAM&amp;quot;: {
                &amp;quot;Driver&amp;quot;: &amp;quot;default&amp;quot;,
                &amp;quot;Options&amp;quot;: {},
                &amp;quot;Config&amp;quot;: [
                    {
                        &amp;quot;Subnet&amp;quot;: &amp;quot;172.18.0.0/16&amp;quot;,
                        &amp;quot;Gateway&amp;quot;: &amp;quot;172.18.0.1&amp;quot;
                    }
                ]
            },
            &amp;quot;Internal&amp;quot;: false,
            &amp;quot;Attachable&amp;quot;: false,
            &amp;quot;Ingress&amp;quot;: false,
            &amp;quot;ConfigFrom&amp;quot;: {
                &amp;quot;Network&amp;quot;: &amp;quot;&amp;quot;
            },
            &amp;quot;ConfigOnly&amp;quot;: false,
            &amp;quot;Containers&amp;quot;: {
                &amp;quot;996b590f206dded3db3a653e554167e046fe3852f55a78d6d983eaa0f7f7fc3d&amp;quot;: {
                    &amp;quot;Name&amp;quot;: &amp;quot;my_mongodb&amp;quot;,
                    &amp;quot;EndpointID&amp;quot;: &amp;quot;e78b23529f29b2a6f5291f06fb1875cac9958888ca0eb95cae814dc8b631613a&amp;quot;,
                    &amp;quot;MacAddress&amp;quot;: &amp;quot;02:42:ac:12:00:02&amp;quot;,
                    &amp;quot;IPv4Address&amp;quot;: &amp;quot;172.18.0.2/16&amp;quot;,
                    &amp;quot;IPv6Address&amp;quot;: &amp;quot;&amp;quot;
                },
                &amp;quot;eebe0a3bf7116eab4731ef68da445f24c15b185efc8d7cea20937736702b93f8&amp;quot;: {
                    &amp;quot;Name&amp;quot;: &amp;quot;my_nodeapp&amp;quot;,
                    &amp;quot;EndpointID&amp;quot;: &amp;quot;6903d08b5942c71b5afd09caf535d4abb818ded97742fe324d54bb6396b595a5&amp;quot;,
                    &amp;quot;MacAddress&amp;quot;: &amp;quot;02:42:ac:12:00:03&amp;quot;,
                    &amp;quot;IPv4Address&amp;quot;: &amp;quot;172.18.0.3/16&amp;quot;,
                    &amp;quot;IPv6Address&amp;quot;: &amp;quot;&amp;quot;
                }
            },
            &amp;quot;Options&amp;quot;: {},
            &amp;quot;Labels&amp;quot;: {}
        }
    ]

&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The docker daemon implements an embedded DNS server (127.0.0.11) which provides built-in service discovery for any container created with a valid name. The containers are able to use this by docker overlay three crucial /etc files inside the container with virtual files. This arrangement allows Docker to do clever things like keep resolv.conf up to date across all containers.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker exec my_nodeapp mount

    ...
    /dev/sda1 on /etc/resolv.conf type ext4 (rw,relatime,data=ordered)
    /dev/sda1 on /etc/hostname type ext4 (rw,relatime,data=ordered)
    /dev/sda1 on /etc/hosts type ext4 (rw,relatime,data=ordered)
    ...
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If we view the DNS settings of one of the containers we see that it is set by default to 127.0.0.11.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker exec -it my_nodeapp cat /etc/resolv.conf

    nameserver 127.0.0.11
    options ndots:0
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;We can therefore ping the database container by using its container name from the app server.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker exec -it my_nodeapp ping -c 1 my_mongodb

    PING my_mongodb (172.18.0.2) 56(84) bytes of data.
    64 bytes from my_mongodb.my_isolated_network (172.18.0.2): icmp_seq=1 ttl=64 time=0.177 ms

    --- my_mongodb ping statistics ---
    1 packets transmitted, 1 received, 0% packet loss, time 0ms
    rtt min/avg/max/mdev = 0.177/0.177/0.177/0.000 ms
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;It can be a relatively common requirement for me to have different container names for same application function in different environments e.g. dev_mongodb, qa_mongodb, prod_mongodb. While I could configuring connection strings in the app server it might be more useful to give the database a connection alias e.g. mongodb. Docker run command has flag &amp;ndash;network-alias so we can use a common alias between environments.&lt;/p&gt;
&lt;p&gt;If we run the two containers and specify an alias for the database we can see we can ping the alias name from app container.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker run -d --net=my_isolated_network --name my_mongodb --network-alias mongodb mongo 
docker run -d --net=my_isolated_network --name my_nodeapp -it node
docker exec -it my_nodeapp ping -c 1 mongodb

    PING mongodb (172.18.0.2) 56(84) bytes of data.
    64 bytes from my_mongodb.my_isolated_network (172.18.0.2): icmp_seq=1 ttl=64 time=0.136 ms

    --- mongodb ping statistics ---
    1 packets transmitted, 1 received, 0% packet loss, time 0ms
    rtt min/avg/max/mdev = 0.136/0.136/0.136/0.000 ms
&lt;/code&gt;&lt;/pre&gt;
        
      </description>
    </item>
    
    <item>
      <title>Docker Networking</title>
      <link>https://darrylcauldwell.github.io/post/docker-net/</link>
      <pubDate>Tue, 08 Nov 2016 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/docker-net/</guid>
      <description>
        
          &lt;p&gt;Hear I describe Docker networking from the very basic through to extending it by pluggin in alternative drivers. In order to follow along with this post you will require Docker capability on your laptop to do this follow &lt;a href=&#34;https://docs.docker.com/engine/installation/&#34;&gt;these instructions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Mostly will be using &lt;a href=&#34;https://www.alpinelinux.org/&#34;&gt;Alpine Linux&lt;/a&gt; as it is very small so downloads quickly and doesn&amp;rsquo;t consume many resources.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker pull alpine
docker images
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Output from images command should list all images you hold locally including Alpine Linux&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;REPOSITORY  TAG     IMAGE ID        CREATED         SIZE
alpine      latest  baa5d63471ea    2 weeks ago     4.803 MB
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once we have the Alpine Linux image local we can then launch it,  if we do this with interactive and simulated TTY options we get presented with the Linux shell&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker run --interactive --tty alpine sh
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;From here we can view the docker containers view of the networking,  we see here it has allocated an rfc1918 address and that it can ping outbound.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;/ &lt;span style=&#34;color:#75715e&#34;&gt;# ip addr&lt;/span&gt;
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu &lt;span style=&#34;color:#ae81ff&#34;&gt;65536&lt;/span&gt; qdisc noqueue state UNKNOWN qlen &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
    valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
    valid_lft forever preferred_lft forever
16: eth0@if17: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&amp;gt; mtu &lt;span style=&#34;color:#ae81ff&#34;&gt;1500&lt;/span&gt; qdisc noqueue state UP 
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.2/16 scope global eth0
    valid_lft forever preferred_lft forever
    inet6 fe80::42:acff:fe11:2/64 scope link 
    valid_lft forever preferred_lft forever
/ &lt;span style=&#34;color:#75715e&#34;&gt;# ping 8.8.8.8&lt;/span&gt;
PING 8.8.8.8 &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;8.8.8.8&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;56&lt;/span&gt; data bytes
&lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt; bytes from 8.8.8.8: seq&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; ttl&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;61&lt;/span&gt; time&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;43.770 ms
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;To detach the tty without exiting the shell, use the escape sequence Ctrl+p + Ctrl+q more details &lt;a href=&#34;https://docs.docker.com/engine/reference/commandline/attach/&#34;&gt;here&lt;/a&gt;.  Once exited you should still be able to see the container in the running state.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker ps -a

CONTAINER ID    IMAGE   COMMAND     CREATED             STATUS          PORTS   NAMES
5099910cc9d0    alpine  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sh&amp;#34;&lt;/span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt; minutes ago       Up &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt; minutes            elegant_torvalds
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;If you need to get back to the running shell you can use either the container id or the name, these two commands are equivelant.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker attach 5099910cc9d0
docker attach elegant_torvalds
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So now if we start a second instance of Apline Linux in container&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker run --interactive --tty alpine sh
docker ps -a

CONTAINER ID    IMAGE   COMMAND     CREATED             STATUS          PORTS   NAMES
e96c3e67410c    alpine  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sh&amp;#34;&lt;/span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt; seconds ago       Up &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt; seconds            suspicious_pare
5099910cc9d0    alpine  &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;sh&amp;#34;&lt;/span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt; minutes ago       Up &lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt; minutes            elegant_torvalds
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can attach and detach to each,  we notice that the second instance has been assigned next IP 172.17.0.3 and that it can ping the first instance 172.17.0.2.&lt;/p&gt;
&lt;p&gt;If we then detach the containers and view what docker networking shows, we see that Docker uses the concept of Drivers and that we are using the default bridge driver.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker network ls

NETWORK ID          NAME                DRIVER
64481f7454d1        bridge              bridge              
a8f3b81d9991        host                host                
6ccbf074e447        none                null         
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can explore the properties of the bridge network a little more, and see the IP addresses it has issued to the containers.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker network inspect bridge

&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;bridge&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;64481f7454d138e43558359d1e41bb96564769bfcccc547b42b2a37b873c3a73&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Scope&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;local&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Driver&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;bridge&amp;#34;&lt;/span&gt;,
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;EnableIPv6&amp;#34;&lt;/span&gt;: false,
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;IPAM&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Driver&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Options&amp;#34;&lt;/span&gt;: null,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Config&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
                &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Subnet&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;172.17.0.0/16&amp;#34;&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;,
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Internal&amp;#34;&lt;/span&gt;: false,
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Containers&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{}&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;,
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Options&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;com.docker.network.bridge.default_bridge&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;com.docker.network.bridge.enable_icc&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;com.docker.network.bridge.enable_ip_masquerade&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;true&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;com.docker.network.bridge.host_binding_ipv4&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;0.0.0.0&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;com.docker.network.bridge.name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;docker0&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;com.docker.network.driver.mtu&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1500&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;,
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Labels&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{}&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So we can see that docker created a shared bridge, but what happens if you want to isolate containers on the same host well we can create a different bridge network we see it allocates the 172.18.0.0/16 CIDR.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker network create --driver bridge my-bridge-network
docker network inspect my-bridge-network

&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Name&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;my-bridge-network&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Id&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;5fec8c172a6c323a616c54938815b42d6baab04e816d7a5bb9113a0e914c52a4&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Scope&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;local&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Driver&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;bridge&amp;#34;&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;EnableIPv6&amp;#34;&lt;/span&gt;: false,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;IPAM&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Driver&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;default&amp;#34;&lt;/span&gt;,
            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Options&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{}&lt;/span&gt;,
            &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Config&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
                    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Subnet&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;172.18.0.0/16&amp;#34;&lt;/span&gt;,
                    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Gateway&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;172.18.0.1/16&amp;#34;&lt;/span&gt;
                &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
            &lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Internal&amp;#34;&lt;/span&gt;: false,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Containers&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{}&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Options&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{}&lt;/span&gt;,
        &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Labels&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#f92672&#34;&gt;{}&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So if we create a new container now on this new network, we see it gets IP address on 172.18.0.0/16 and it cannot ping containers on 172.17.0.0/16.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker run --interactive --tty --net&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;my-bridge-network alpine sh
/ &lt;span style=&#34;color:#75715e&#34;&gt;# ipaddr&lt;/span&gt;
1: lo: &amp;lt;LOOPBACK,UP,LOWER_UP&amp;gt; mtu &lt;span style=&#34;color:#ae81ff&#34;&gt;65536&lt;/span&gt; qdisc noqueue state UNKNOWN qlen &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
    valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
    valid_lft forever preferred_lft forever
25: eth0@if26: &amp;lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&amp;gt; mtu &lt;span style=&#34;color:#ae81ff&#34;&gt;1500&lt;/span&gt; qdisc noqueue state UP 
    link/ether 02:42:ac:12:00:02 brd ff:ff:ff:ff:ff:ff
    inet 172.18.0.2/16 scope global eth0
    valid_lft forever preferred_lft forever
    inet6 fe80::42:acff:fe12:2/64 scope link 
    valid_lft forever preferred_lft forever
/ &lt;span style=&#34;color:#75715e&#34;&gt;# ping 172.17.0.3&lt;/span&gt;
PING 172.17.0.3 &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;172.17.0.3&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt;: &lt;span style=&#34;color:#ae81ff&#34;&gt;56&lt;/span&gt; data bytes
^C
--- 172.17.0.3 ping statistics ---
&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt; packets transmitted, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; packets received, 100% packet loss
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We have focussed on the shipped bridge driver, however since &lt;a href=&#34;https://github.com/docker/libnetwork&#34;&gt;LibNetwork project&lt;/a&gt; got integrated into docker the driver became plugable. Some of the Docker networking ecosystems, include Weave, Nuage, Cisco, Microsoft, Calico, Midokura, and VMware.&lt;/p&gt;
&lt;p&gt;Upto now we have looked at networking between containers on the same host by bridging to the network interface card. You will probably want to extend the layer 2 networking capacibility so containers on different hosts can communicate. To do this we would use &lt;a href=&#34;https://docs.docker.com/swarm/networking/&#34;&gt;Docker Swarm&lt;/a&gt; which includes the multi-host networking feature and allows the creation of custom container networks which span multiple Docker hosts. By default it does this by use of the &amp;lsquo;overlay&amp;rsquo; network driver&lt;/p&gt;
&lt;p&gt;An interesting option to use as a Docker networking driver is &lt;a href=&#34;https://github.com/projectcalico/calico-containers&#34;&gt;Project Calico for Containers&lt;/a&gt; as the overlay networking expands across more and more hosts it can experience performance issues. Project Calico aims to over come this issue but allow container connectivity across hosts by having each container route directly at Layer 3 rather than Layer 2.  It does this by having a calico-node container installed and running on each host which manages the network routing etc.&lt;/p&gt;
&lt;p&gt;As in my previous post about &lt;a href=&#34;https://darrylcauldwell.github.io/install-vic/&#34;&gt;vSphere Integrated Containers&lt;/a&gt; its interesting to see VMware integrating with Docker and I&amp;rsquo;d expect it won&amp;rsquo;t be too long until we discover how NSX will plug into Docker Networking.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Installing vSphere Integrated Containers In Five Minutes</title>
      <link>https://darrylcauldwell.github.io/post/vic/</link>
      <pubDate>Wed, 13 Jul 2016 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/vic/</guid>
      <description>
        
          &lt;p&gt;As it looks like vSphere Integrated Containers will feature a lot at the upcoming VMworld I took the chance to install this and find out more.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/vmware/vic/tree/master/doc/user_doc/vic_installation&#34;&gt;documentation available&lt;/a&gt; on isn&amp;rsquo;t extensive, but, contains all I needed to know and I think benefits from being to the point.&lt;/p&gt;
&lt;p&gt;The files needed to set this up are provided as a gzip file
&lt;a href=&#34;https://bintray.com/vmware/vic-repo/build#files&#34;&gt;hosted on bintray&lt;/a&gt;. Just checking the file dates you can see the development is iterating at a great pace with around six releases per day. I chose to download the latest package which contains installers to be run from different operating systems.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Platform&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Supported Versions&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Windows&lt;/td&gt;
&lt;td&gt;7, 10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mac OS X&lt;/td&gt;
&lt;td&gt;10.11 (TBC)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Linux&lt;/td&gt;
&lt;td&gt;Ubuntu 15.04, others TBD&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;My homelab is vCenter, single cluster of two ESX hosts, storage VSAN and networking NSX. Security is not important in my homelab and TLS adds complication so I chose not to add this. There are great syntaxt examples to follow in the documentation, but here are options I used.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;./vic-machine-darwin create --target 192.168.1.13 --user Administrator@vsphere.local --password VMware1! --compute-resource ElectricChair --image-datastore vsanDatastore --bridge-network DPortGroup --name vch
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/vic-install.jpg&#34; alt=&#34;vSphere Integrated Containers Install&#34;&gt;&lt;/p&gt;
&lt;p&gt;In order to use vSphere Integrated Containers you&amp;rsquo;ll need a Docker client, for Mac there is the choice of &lt;a href=&#34;https://www.docker.com/products/docker-toolbox&#34;&gt;Docker Toolbox&lt;/a&gt; or the newer &lt;a href=&#34;https://docs.docker.com/docker-for-mac/&#34;&gt;Beta Docker for Mac&lt;/a&gt;. I had the beta installed, and found I needed to remove this and replace with Docker Toolbox.&lt;/p&gt;
&lt;p&gt;The first thing to do is ensure that you can connect and that it looks healthy, to do this run&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker -H ipaddress-vch:2375 info
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/vic-info.jpg&#34; alt=&#34;vSphere Integrated Containers Info&#34;&gt;&lt;/p&gt;
&lt;p&gt;Its always useful to look in the log files when your setting something up for the first time, however SSH is disabled on the VCH host so I enabled SSH for current session.  Logon via console using &lt;em&gt;root&lt;/em&gt; with password of &lt;em&gt;password&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;systemctl start sshd
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;There are five log files I found so far&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;/var/log/vic/docker-personality.log
/var/log/vic/imagec.log
/var/log/vic/init.log
/var/log/vic/port-layer.log
/var/log/vic/vicadmin.log
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;So now we&amp;rsquo;re running and can access logs the next thing to do is pull a container&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker -H paddress-vch:2375 pull vmwarecna/nginx
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/vic-nginx.jpg&#34; alt=&#34;vSphere Integrated Containers NGINX&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once its local we can start it up,  we&amp;rsquo;ll start it in the background and put a port mapping of port 80 in place and then view it.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;docker -H ipaddress-vch:2375 run -d -p 80:80 vmwarecna/nginx
docker -H ipaddress-vch:2375 ps
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/vic-nginx-go.jpg&#34; alt=&#34;vSphere Integrated Containers NGINX Go&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can then also see that a new VM has been created in vCenter with VM name matching the UID of the docker container.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/vic-nginx-vm.jpg&#34; alt=&#34;vSphere Integrated Containers NGINX VM&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can then stop this and tidy up our test container&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;docker -H ipaddress-vch:2375 stop container-id
docker -H ipaddress-vch:2375 rm container-id
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;As its so easy to install and configure its probably worth while removing VCH and when you come to use it pull the latest to remove is as easy as installing.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;./vic-machine-darwin delete --target vcenter.darrylcauldwell.local --user Administrator@vsphere.local --password VMware1! --name vch --force
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/vic-bye.jpg&#34; alt=&#34;vSphere Integrated Containers Bye!&#34;&gt;&lt;/p&gt;
&lt;p&gt;There is a little bug just now and the image files don&amp;rsquo;t seem to get removed by uninstall process but these are easy enough to delete along with the VIC folder from the datatore.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;m disappointed in myself for waiting so long to look at this, what was putting me off was the thought it might be complex but it proved to be very much straight forwards.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Project Bonneville - vSphere Integrated Containers</title>
      <link>https://darrylcauldwell.github.io/post/vic-bonneville/</link>
      <pubDate>Tue, 06 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/vic-bonneville/</guid>
      <description>
        
          &lt;p&gt;Working for a company which develops software for business I&amp;rsquo;ve been keen on following the story of containers unfold. The explosion of containers and microservices in the software development space has been a revolution in the way applications can be architected and deployed. This has put a great deal of power in the hands of the developer but with great power comes great responsibility for operational management.&lt;/p&gt;
&lt;p&gt;Working in the operational side the key words to my working life are Availability, Reliability, Maintainability, Supportability and Security. The operational model to effectively run containers in production requires a paradigm shift where the approach and underlying assumptions are revisited. Working in a company promoting DevOps culture and values we in the operational side are beginning to gain greater empathy to the challenges a developer faces and how they believe containers will help. Gaining a technical understanding of Docker has therefore become a priority so I can ensure the infrastructure I design and engineer can help the developer deliver not only velocity but help them to deliver Availability, Reliability, Maintainability, Supportability and Security.&lt;/p&gt;
&lt;p&gt;During &lt;a href=&#34;https://darrylcauldwell.github.io/post/fargo&#34;&gt;VMworld 2014 project Fargo (VMfork)&lt;/a&gt; was described which has become the Instant Clone feature of vSphere 6. At the time I didn&amp;rsquo;t take too much notice as this was signed to virtual desktop space.  During April this year VMware released project Photon which is essentially a minimal Linux container host,  at the time this was confusing to me as RedHat Atomic, Ubuntu Snappy and CoreOS seemed to have all bases covered in this space. That is until two months later when Ben Corrie of VMware announced project Bonneville to the world.&lt;/p&gt;
&lt;p&gt;The fundamental to this solution is a 1:1 model between container host and containers rather than one host with many containers. Project Bonneville is a Docker daemon that has custom drivers for networks and execution that is compatible with the Docker Client APIs, and that means a vSphere VM is treated exactly like a Docker container. With this it redefined a Docker container as a x86 hardware virtualised VM, (not tied to host running the Linux kernel), a Bonneville VM can theoretically run containers of any kernel version of any operating system and can do it fast and efficiently.&lt;/p&gt;
&lt;p&gt;‘Instant Clone’ also known as VMfork or Project Fargo, gives the ability to clone and deploy virtual machines, as much as 10x faster than what is currently possible before. It does this by using rapid in-memory cloning of running virtual machines and copy-on-write to quickly deploy clones of a parent virtual machine. Using this clone method of the container host ensures the only changed blocks and memory pages are those used by container itself this.&lt;/p&gt;
&lt;p&gt;A fantastic architecture flow of how Bonneville might work is described in this diagram as posted by George Hicken @hickeng.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/vic-bonneville.png&#34; alt=&#34;vSphere Integrated Containers&#34;&gt;&lt;/p&gt;
&lt;p&gt;The benefits this technology appears to offer appear fantastic and I am really looking forwards to attending Ben Corrie&amp;rsquo;s session &amp;lsquo;INF5229 — Docker and Fargo: Exploding the Linux Container Host&amp;rsquo; to solidify my understanding and learn more about how we can begin exploiting this to help our developers deliver Availability, Reliability, Maintainability, Supportability and Security alongside the awesome features they write.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>VMware Projects Fargo (VMFork) &amp; Meteor</title>
      <link>https://darrylcauldwell.github.io/post/fargo/</link>
      <pubDate>Tue, 06 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/fargo/</guid>
      <description>
        
          &lt;p&gt;There was announced during VMworld a technology preview for Project Fargo (formerly VMFork) it is likely this will be launched with vSphere6. The aim of Fargo is to provides a fast, scalable differential clone of a running VM.  I see this as very similar to Redirect-on-Write (RoW) methodology used by NetApp snapshots where at the point of snap the existing blocks are frozen and any writes (creations/changes/deletions) are redirected to new blocks. However with Fargo rather than than a snapshot we are creating a Copy-on-Write(CoW) the difference being that with CoW the original data that is being written to is copied into a new file that is set aside for the snapshot before original data is overwritten. So before a write is allowed to a block, copy-on-write moves the original data block to the snapshot storage.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/fargo-cow.gif&#34; alt=&#34;Copy On Write&#34;&gt;&lt;/p&gt;
&lt;p&gt;The key benefits of the use of this method is that its near instantaneous and can be done from a running VM,  so a new VM spawned would typically take less than 1 second and be in the same running state. As well as this as only changed blocks are written the solution will take up dramatically less disk space.&lt;/p&gt;
&lt;p&gt;While there are many potential use cased for Fargo it was presented with virtual desktop in mind where providing an instant clone of running non-persistent desktop would avoid the boot storms to the storage subsystem.&lt;/p&gt;
&lt;p&gt;So now you have quick provisioning of operating system you then need to deliver the right user applications to it quickly and this is I assume to be done by &lt;a href=&#34;http://blogs.vmware.com/euc/2014/08/cloudvolumes.html&#34;&gt;CloudVolumes&lt;/a&gt; where applications are abstracted and layered onto the users operating system.  The combination of these two tools appears to be known as Project Meteor.&lt;/p&gt;

        
      </description>
    </item>
    
    <item>
      <title>Docker On Photon On vCloud Air</title>
      <link>https://darrylcauldwell.github.io/post/docker-photon/</link>
      <pubDate>Thu, 04 Jun 2015 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.github.io/post/docker-photon/</guid>
      <description>
        
          &lt;p&gt;There has been a lot of industry noise around Docker and VMware project Photon so these have been on the agenda to explore as well.  I generally perform my little research projects running nested ESXi on VMware Fusion on my laptop, but VMware are &lt;a href=&#34;http://vcloud.vmware.com/uk/service-offering/virtual-private-cloud-ondemand&#34;&gt;offering £300 of free service credits&lt;/a&gt; for trialing vCloud Air OnDemand.  Here is my how to guide of the steps I used.&lt;/p&gt;
&lt;h2 id=&#34;vcloud-air-networking&#34;&gt;vCloud Air Networking&lt;/h2&gt;
&lt;p&gt;This post assumes your starting with a clean vCloud Air account,  obviously you may already have vCloud Air networking setup,  if so there is no need for this section as long as the Photon and test nexted ESXi VM sit on a common broadcast network this should be fine.&lt;/p&gt;
&lt;p&gt;First task would be to setup a &amp;lsquo;Virtual Private Cloud OnDemand&amp;rsquo; this you create in a location close to you for me thats Slough, UK,  this will create virtual datacenter with &amp;lsquo;default-routed-network&amp;rsquo; (192.168.109.1/24).&lt;/p&gt;
&lt;h2 id=&#34;create-photon-vapp&#34;&gt;Create Photon vApp&lt;/h2&gt;
&lt;p&gt;Surprisingly there is no Photon vApp Template available in the vCloud Air public catalog,  they do however provide the photon-1.0TP1.iso.  So first task is to navigate to vCloud Director, Catalog view and create  new personal Catalog called &amp;lsquo;Photon&amp;rsquo; then navigate to the Public Catalog \ Media tab and copy the Photon ISO to your new personal Catalog.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;While in vCloud Director navigate to My Cloud and &amp;lsquo;Build New vApp&amp;rsquo;&lt;/li&gt;
&lt;li&gt;Within the new vApp add a VM called &amp;lsquo;photon&amp;rsquo;&lt;/li&gt;
&lt;li&gt;Change its OS to &amp;lsquo;Other 3.x Linux (64-bit)&amp;rsquo;&lt;/li&gt;
&lt;li&gt;Increase memory to 2vCPU, 2GB vRAM and vHDD to 40GB&lt;/li&gt;
&lt;li&gt;Change network to &amp;lsquo;default-routed-network&amp;rsquo;, &amp;lsquo;Static - Manual&amp;rsquo; &amp;lsquo;192.168.109.100&amp;rsquo;&lt;/li&gt;
&lt;li&gt;Ensure &amp;lsquo;Organization VDC network&amp;rsquo; is selected and NOT &amp;lsquo;vApp network&amp;rsquo;&lt;/li&gt;
&lt;li&gt;Attach the Photon ISO as CDROM and power on.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;base-photon-configuration&#34;&gt;Base Photon Configuration&lt;/h2&gt;
&lt;p&gt;Open remote console to the Photon VM and it should be at boot menu.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/docker-photon.jpg&#34; alt=&#34;Photon OS&#34;&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;From boot menu select Install&lt;/li&gt;
&lt;li&gt;Accept EULA,  and defaults through disk partitioning&lt;/li&gt;
&lt;li&gt;At installation type select &amp;lsquo;3. Photon Full OS (All).&lt;/li&gt;
&lt;li&gt;Give hostname &amp;lsquo;photon.local&amp;rsquo; and give a root password (NextGEN1!)&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;configure-ssh-inbound&#34;&gt;Configure SSH Inbound&lt;/h1&gt;
&lt;p&gt;So next you might not want to use the vCloud Air console to access Photon,  you will probably want to use a SSH client and connect directly.  So to do that we would need to create a &amp;lsquo;Network Address Translation&amp;rsquo; NAT rule to link the vCloud Air externally published IP address with the internal IP address.  There are two types of NAT rule, SNAT and DNAT,  we will be looking to configure DNAT as the traffic is traveling from the Internet (the source) to a virtual machine inside vCloud Air (the destination).&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Once booted,  connect to console and logon with root and password.&lt;/li&gt;
&lt;li&gt;Use &lt;a href=&#34;http://www.virten.net/2015/04/basic-commands-for-vmware-photon-and-docker/&#34;&gt;this post&lt;/a&gt; to enable SSH and set static IP address to 192.168.109.100/24 with gateway of 192.168.109.1.&lt;/li&gt;
&lt;li&gt;If we navigate to vCloud Director view &amp;lsquo;Administration&amp;rsquo; tab,  select vDC in the left pane and choose &amp;lsquo;Edge Gateway&amp;rsquo; in the right pane.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/docker-photon-vCD_Gateway.jpg&#34; alt=&#34;vCloud Director Gateway&#34;&gt;&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Select &amp;lsquo;gateway&amp;rsquo; and select the Action for &amp;lsquo;External IP Allocations&amp;rsquo;,  note down the external IP address.&lt;/li&gt;
&lt;li&gt;Ensure &amp;lsquo;gateway&amp;rsquo; still selected and select the Action for &amp;lsquo;Edge Gateway Services&amp;rsquo;,  change to the NAT tab,  and click &amp;lsquo;Add DNAT&amp;rsquo;,  change applied to to your network add the External IP address as you noted just before,  select port 22 (SSH) for Outgoing and Translated ports,  and give the Photon IP address as the Internal IP.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/docker-photon-Air_NAT.jpg&#34; alt=&#34;vCloud Director NAT&#34;&gt;&lt;/p&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;Ensure &amp;lsquo;gateway&amp;rsquo; still selected and select the Action for &amp;lsquo;Edge Gateway Services&amp;rsquo;,  change to the firewall tab and add ingress firewall rule called SSH-In from external to specific IP 192.168.109.100 port 22.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/docker-photon-Air_FW.jpg&#34; alt=&#34;vCloud Director Firewall&#34;&gt;&lt;/p&gt;
&lt;ol start=&#34;7&#34;&gt;
&lt;li&gt;If all has gone to plan you should now be able to connect from a remote SSH client.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/docker-photon-Remote_SSH.jpg&#34; alt=&#34;SSH Remote Shell&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;enable-vcloud-air-vm-internet-access&#34;&gt;Enable vCloud Air VM Internet Access&lt;/h2&gt;
&lt;p&gt;To enable our vCloud Air VMs to access the internet we need to configure a NAT rule to handle the private to public address and also a firewall rule to allow the traffic.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Add a source NAT rule,  for Original Source enter 192.168.109.0/24 and for Translated Source select your public IP address.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/docker-photon-SNAT.png&#34; alt=&#34;Source NAT&#34;&gt;&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Add a firewall rule to allow all traffic outbound,  give it name Any-Out,  Source as &amp;lsquo;Internal&amp;rsquo; and destination &amp;lsquo;External&amp;rsquo;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/docker-photon-FW_Out.png&#34; alt=&#34;Outbound Firewall&#34;&gt;&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Your Photon VM should now be able to talk to the Internet&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.github.io/images/docker-photon-nslookup.png&#34; alt=&#34;Outbound Connectivity Test&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;docker&#34;&gt;Docker&lt;/h2&gt;
&lt;p&gt;Docker itself is included in the Photon image so with Internet connectivity you can pull things from public repositories and away you go.&lt;/p&gt;

        
      </description>
    </item>
    
  </channel>
</rss>
