<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tanzu on </title>
    <link>https://darrylcauldwell.com/tags/tanzu/</link>
    <description>Recent content in tanzu on </description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Wed, 29 Apr 2020 00:00:00 +0000</lastBuildDate><atom:link href="https://darrylcauldwell.com/tags/tanzu/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>vSphere with Kubernetes Homelab Build</title>
      <link>https://darrylcauldwell.com/post/vsphere-k8s/</link>
      <pubDate>Wed, 29 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://darrylcauldwell.com/post/vsphere-k8s/</guid>
      <description>
        
          &lt;p&gt;I&amp;rsquo;d been exploring Project Pacific during its beta in my homelab for a while. When vSphere 7 and NSX-T 3.0 went GA I took chance to rebuild lab and consolidate much of the configuration I&amp;rsquo;d been applying iteratively.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/tanzu-vsphere.png&#34; alt=&#34;vSphere with Tanzu&#34;&gt;&lt;/p&gt;
&lt;p&gt;My lab hardware specification is a constraint so I&amp;rsquo;ve had to deviate from documentation in a few areas of configuration. During stand up and power up hosts experience CPU and RAM pressure but once everything is running in steady state it is tight but just fits.&lt;/p&gt;
&lt;h2 id=&#34;single-vlan--subnet-lab-network&#34;&gt;Single VLAN / subnet Lab Network&lt;/h2&gt;
&lt;p&gt;My lab has a very simple physical network namely a single subnet (192.168.1.0/24) with DHCP enabled and which has default route to the internet.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Host&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;Allocation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ad&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;esx1&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.11&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;esx2&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.12&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;esx3&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.13&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;vcenter&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.14&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;nsx&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;edge&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.16&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;t0-uplink&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.17&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;tep-pool&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.20-24&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;workload control plane&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.30-34&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kubernetes Ingress&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.48-63&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Kubernetes Egress&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.64-79&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DHCP&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.64-253&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gateway&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;192.168.1.254&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;I look to configure subnets on the NSX overlay network using the other RFC1918 private address range. The physical router does not support dynamic routing protocol so I configure static routes for these two CIDR with next hop as tier-0 uplink IP 192.168.1.17.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;CIDR&lt;/th&gt;
&lt;th style=&#34;text-align:center&#34;&gt;IP Range&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;10.0.0.0/8&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;10.0.0.0 – 10.255.255.255&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;172.16.0.0/12&lt;/td&gt;
&lt;td style=&#34;text-align:center&#34;&gt;172.16.0.0 – 172.31.255.255&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;compute-resource-nodes&#34;&gt;Compute Resource Nodes&lt;/h2&gt;
&lt;p&gt;Lab compute resources are provided by three &lt;a href=&#34;https://ark.intel.com/content/www/us/en/ark/products/89190/intel-nuc-kit-nuc6i5syh.html&#34;&gt;Intel NUC6i5SYH&lt;/a&gt; hardware, each provides 2x 1.8Ghz CPU and 32GB RAM. I install ESXi boot partition on USB drives. To create a bootable USB for ESXi on macOS by creating a new VM in VMware Fusion with ESXi ISO attached as CD/DVD. It is only possible to connect USB to a running VM so power on VM and connect USB drive, work through ESXi installation via Fusion console.&lt;/p&gt;
&lt;p&gt;At the end of installation message to disconnect the CD/DVD and reboot VM.  I remain connected for reboot, using remote console navigate Troubleshoot menu to enable ESXi Shall and SSH, then configure networking to reflect homelab IP allocation.&lt;/p&gt;
&lt;h2 id=&#34;ensure-unique-system-uuid-and-mac-address&#34;&gt;Ensure unique System UUID and MAC address&lt;/h2&gt;
&lt;p&gt;Once ESXi has a basic network configuration I power down the VM and move USB to the Intel NUC and power on. During installation of ESXi the System UUID and MAC address are formed from the MAC address of the NIC. When  using Fusion to create multiple ESXi VMs this scenario can lead to duplicatation of System UUID and MAC address. We can configure ESXi to move to physical NIC MAC address and form new ESXi System UUID by running commands like.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;esxcli system settings advanced set -o /Net/FollowHardwareMac -i 1
sed -i &#39;s#/system/uuid.*##&#39; /etc/vmware/esx.conf
/sbin/auto-backup.sh
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;add-external-usb-nic-drivers&#34;&gt;Add external USB NIC drivers&lt;/h2&gt;
&lt;p&gt;The Intel NUC has a single onboard 1GB NIC, I add an external USB NIC to each NUC. ESXi doesn&amp;rsquo;t ship with required drivers for these external USB NIC, for labs these are provides via &lt;a href=&#34;https://flings.vmware.com/usb-network-native-driver-for-esxi&#34;&gt;VMware Flings&lt;/a&gt;. I copy the downloaded driver bundle to the /tmp folder on ESXi via an SFTP client like CyberDuck. Once the bundle is uploaded I following command to install the bundle.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;esxcli software vib install -d /tmp/ESXi700-VMKUSB-NIC-FLING-34491022-component-15873236.zip
esxcli system maintenanceMode set --enable true
esxcli system shutdown reboot --reason &amp;quot;USB NIC driver&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;suppress-warnings&#34;&gt;Suppress Warnings&lt;/h2&gt;
&lt;p&gt;When booting ESXi from USB the system logs and coredumps can not persist to local storage. I also prefer to leave SSH enabled in lab so get warnings. Both of these choices gives me warnings which I prefer to suppress.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vim-cmd hostsvc/advopt/update UserVars.SuppressShellWarning long 1
vim-cmd hostsvc/advopt/update UserVars.SuppressCoredumpWarning long 1
vim-cmd hostsvc/advopt/update Syslog.global.logHost string 127.0.0.1
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;form-vsan-cluster&#34;&gt;Form vSAN Cluster&lt;/h2&gt;
&lt;p&gt;Two of the Intel NUC I use each have SSD the third is used to provide compute resources only. The ESXi installation allows Management traffic only on VMkernel port so need to bind vSAN to VMkernel port. In my lab only two devices contribute disk to vSAN the default storage policy for will prevent any VM deployment. To configure this I run the following on each host in cluster.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;esxcli system maintenanceMode set --enable false
esxcli vsan network ip add -i vmk0
esxcli vsan policy setdefault -c cluster -p &amp;quot;((\&amp;quot;hostFailuresToTolerate\&amp;quot; i0)&amp;quot;
esxcli vsan policy setdefault -c vdisk -p &amp;quot;((\&amp;quot;hostFailuresToTolerate\&amp;quot; i0)&amp;quot;
esxcli vsan policy setdefault -c vmnamespace -p &amp;quot;((\&amp;quot;hostFailuresToTolerate\&amp;quot; i0)&amp;quot;
esxcli vsan policy setdefault -c vmswap -p &amp;quot;((\&amp;quot;hostFailuresToTolerate\&amp;quot; i0) (\&amp;quot;forceProvisioning\&amp;quot; i1))&amp;quot;
esxcli vsan policy setdefault -c vmem -p &amp;quot;((\&amp;quot;hostFailuresToTolerate\&amp;quot; i0) (\&amp;quot;forceProvisioning\&amp;quot; i1))&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;With the pre-requists in place a new vSAN cluster can be formed on first host using command like.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;esxcli vsan cluster new 
esxcli vsan cluster get | grep &amp;quot;Sub-Cluster UUID&amp;quot;
    Sub-Cluster UUID: 5291783f-77a6-c1dc-2ed8-6cfc200618b1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Add other nodes to the cluster using output of Sub-Cluster Master UUID using command like.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;esxcli vsan cluster join -u 5291783f-77a6-c1dc-2ed8-6cfc200618b1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;At this stage vSAN cluster while nodes are looking to form the quorum cannot be formed. Prior to vSAN 6.6 the cluster can discover members using multicast. Forming cluster without vCenter requires formation of unicast networking from the command line by manually building table &lt;a href=&#34;https://kb.vmware.com/s/article/2150303&#34;&gt;kb2150303&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;active-directory&#34;&gt;Active Directory&lt;/h2&gt;
&lt;p&gt;At this stage I upload Windows Server 2019 ISO to vSAN and use this to deploy a VM which acts as a environment Remote Desktop, File Store, DNS server, NTP source and Active Directory. Before proceeding create DNS A &amp;amp; PTR records for the environment.&lt;/p&gt;
&lt;h2 id=&#34;vcenter-server&#34;&gt;vCenter Server&lt;/h2&gt;
&lt;p&gt;I use the Active Directory jump server to mount the vCenter ISO and use UI installer to deploy a &amp;lsquo;Tiny&amp;rsquo; sized vCenter Server to existing Datastore. This creates a cluster with the move deployed to, at this stage I attach remaining hosts to cluster.&lt;/p&gt;
&lt;p&gt;We can then pass ownership of vSAN cluster membership back to vCenter by running following on each host.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;esxcfg-advcfg -s 0 /VSAN/IgnoreClusterMemberListupdates
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The Workload Control Plane feature requires DRS and HA are required to be enabled on clusters it manages. Ensure these are enabled and HA admission control is disabled to maintain capacity.&lt;/p&gt;
&lt;h2 id=&#34;default-vsan-storage-policy&#34;&gt;Default vSAN Storage Policy&lt;/h2&gt;
&lt;p&gt;During the setup of VSAN cluster we configured the local policies to FTT=0 as I only have 2x hosts providing disk capacity. To deploy VMs and appliances via vCenter similarly need to adjust the &amp;lsquo;vSAN Default Storage Policy&amp;rsquo; to &amp;lsquo;No data redundancy&amp;rsquo;.&lt;/p&gt;
&lt;h2 id=&#34;nsx-installation&#34;&gt;NSX Installation&lt;/h2&gt;
&lt;p&gt;Deploy NSX-T 3.0 appliance sized Small, I need to remove CPU reservation to power on.&lt;/p&gt;
&lt;h2 id=&#34;connect-nsx-and-vcenter&#34;&gt;Connect NSX and vCenter&lt;/h2&gt;
&lt;p&gt;Connect to NSX appliance navigate to System &amp;gt; Fabric &amp;gt; Compute Managers and create bindng to vCenter. In order vCenter Workload Platform services can communicate with NSX ensure Enable Trust option is checked.&lt;/p&gt;
&lt;h2 id=&#34;vsphere-distributed-switch&#34;&gt;vSphere Distributed Switch&lt;/h2&gt;
&lt;p&gt;Prior to release of vSphere 7 and NSX-T 3.0 to install NSX required the creation of a N-VDS host switch on each ESXi host. While segments created on N-VDS where visible in vSphere they were not as rich as VDS. It is a constraint that physical NIC uplinks cannot be assigned to both a VDS and N-VDS.&lt;/p&gt;
&lt;p&gt;It was possible to have hosts with two pNIC which initially get built as VDS and then migrate to N-VDS and remove VDS. When running only with N-VDS the segments show in vCenter as Opaque networks. Not many 3rd party products support Opaque networks, automation became more complex as the network could not be correctly gathered via vCenter. Many production deployments moved to hosts with four pNIC two assigned to N-VDS and two assigned to VDS to hold the VMkernel ports.&lt;/p&gt;
&lt;p&gt;With these latest product versions the VDS and N-VDS capabilities converge to singluar VDS construct for use as host switch on ESXi the N-VDS remains for Edge and non-ESXi Transport Node types. The converged VDS for ESXi improves pNIC design and also makes operational management much easier as there is a singluar place for configuring NIOC, MTU and LLDP configuration.&lt;/p&gt;
&lt;p&gt;The lab hosts have two pNIC I leave onboard to vSphere Standard Switch with VMkernel ports attached. I create a VDS Version 7 configured with 1 uplink,  NIOS enabled but without default port group. The default VDS is created with MTU 1500, to support NSX I increase this to Advanced configuration option for MTU 1700.&lt;/p&gt;
&lt;p&gt;Once VDS is created I add usb0 NIC from all cluster hosts to be assigned to Uplink1.  I do not migrate VMkernel or VM networking to VDS.&lt;/p&gt;
&lt;h2 id=&#34;create-a-tunnel-endpoint-ip-address-pool&#34;&gt;Create a Tunnel Endpoint IP Address Pool&lt;/h2&gt;
&lt;p&gt;System &amp;gt; Networking &amp;gt; IP Management &amp;gt; IP Address Pools &amp;gt; Add IP Address Pool&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:             tep-pool
IP Range:         192.168.1.20-192.168.1.24
CIDR:             192.168.1.0/24
Gateway IP:       192.168.1.254
DNS Server:       192.168.1.10
DNS Suffix:       darrylcauldwell.com
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;configure-esxi-hosts-as-transport-nodes&#34;&gt;Configure ESXi hosts as Transport Nodes&lt;/h2&gt;
&lt;p&gt;To consistently configure ESXi hosts as NSX Transport Nodes I create a Transport Node Profile.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name                           Host Transport Node Profile
Host Switch - Type             VDS
Host Switch - Mode             Standard
Host Switch - Name             vCenter \ DSwitch
Host Switch - Transport Zone   nsx-overlay-transportzone &amp;amp; nsx-vlan-transportzone
Host Switch - Uplink Profile   nsx-default-uplink-hostswitch-profile
Host Switch - IP Assignment    IP Pool - tep-pool
Host Switch - Teaming Policy   uplink1 (active)= Uplink 1
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once created using Host Transport Nodes menu select cluster and apply profile. Once completed it is possible to view the NSX components installed on all ESXi hosts using:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;esxcli software vib list | grep nsx
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;configure-nsx-edge&#34;&gt;Configure NSX Edge&lt;/h2&gt;
&lt;p&gt;The NSX Edge provides a Layer 3 routing capability the NSX Container Plugin requires at least a medium sized Edge to deploy a small loadbalancer.&lt;/p&gt;
&lt;p&gt;Deploy a medium sized Edge node&lt;/p&gt;
&lt;p&gt;System &amp;gt; Fabric &amp;gt;  Nodes &amp;gt; Edge Transport Nodes &amp;gt; Add Edge VM&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:                   nsxEdge
FQDN:                   edge
Size:                   Large
Shares:                 Normal
Memory Reservation:     0
IP Assignment:          Static
Management Interface:   VM Network
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Node Switch&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:                       nvds1
Transport Zone:             nsx-overlay-transportzone
Uplink Profile:             nsx-edge-single-nic-uplink-profile
IP Pool:                    tep-pool
DPDK Fastpath Interfaces:   VM Network

Name:                       nvds2
Transport Zone:             nsx-vlan-transportzone
Uplink Profile:             nsx-edge-single-nic-uplink-profile
DPDK Fastpath Interfaces:   VM Network
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;configure-edge-cluster&#34;&gt;Configure Edge Cluster&lt;/h2&gt;
&lt;p&gt;When Edge has fully deployed create Edge Cluster&lt;/p&gt;
&lt;p&gt;System &amp;gt; Fabric &amp;gt; Nodes &amp;gt; Edge Clusters &amp;gt; Add&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:                       edgeCluster
Transport Nodes:            nsxEdge
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;uplink-network-segment&#34;&gt;Uplink Network Segment&lt;/h2&gt;
&lt;p&gt;A network segment is required to to connect Tier0 router uplink to VLAN&lt;/p&gt;
&lt;p&gt;Networking &amp;gt; Segments &amp;gt;  Add Segment&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Segment Name:               edgeUplink
Connectivity:               None
Transport Zone:             nsx-vlan-transportzone
VLAN:                       0
Multicast:                  Disabled
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;tier0-router&#34;&gt;Tier0 Router&lt;/h2&gt;
&lt;p&gt;A logical router is required I create one like:&lt;/p&gt;
&lt;p&gt;Networking &amp;gt; Tier-0 Gateways &amp;gt; Add Gateway &amp;gt; Tier0&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:                       tier0
HA Mode:                    Active-Standby
Failover:                   non preemptive
Edge Cluster:               edgeCluster
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Add Interface&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Name:                       tier0uplink
Type:                       External
IP Address:                 192.168.1.17/24
Connected To:               edgeUplink
Edge Node:                  nsxEdge
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Once interface is added you can test it works as it will now be able to ping.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;ping 192.168.1.17 -c &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;enable-a-cluster-as-workload-control-plane-wcp&#34;&gt;Enable a cluster as Workload Control Plane (WCP)&lt;/h2&gt;
&lt;p&gt;With all of the pre-requists in place we can now use wizard to enable integrated Kubernetes.&lt;/p&gt;
&lt;p&gt;[EDIT 4th May] since initial publication I found another &lt;a href=&#34;https://www.virtuallyghetto.com/2020/04/deploying-a-minimal-vsphere-with-kubernetes-environment.html&#34;&gt;blog post&lt;/a&gt; around deploying minimal lab. By default when enabling Workload Control Plane this deploys a 3x VMs which form the Kubernetes supervisor cluster. This can be reduced to 2x by updating &lt;code&gt;/etc/vmware/wcp/wcpsvc.yaml&lt;/code&gt; on the VCSA and changing the minmasters and maxmasters properties from 3 to 2. Then restarting the wcp service &lt;code&gt;service-control --restart wcp&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Menu &amp;gt; Workload Management &amp;gt; Enable&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Select a Cluster&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If cluster does not show available check reason via GUI and or these vCenter logs for clues.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;/var/log/vmware/wcp/wcpsvc.log
/var/log/vmware/wcp/nsxd.log
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Cluster Settings&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;Cluster Size:                  Tiny
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Network&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The management network refers to the control plane needs to communicate with vCenter and NSX.  The workload network will come from the RFC1918 addressable space. Enter network details&lt;/p&gt;
&lt;p&gt;Workload Control Plane Networking&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Network:                        VM Network
Starting IP Address:            192.168.1.30
Subnet Mask:                    255.255.255.0
Gateway:                        192.168.1.254
DNS Server:                     192.168.1.10
NTP Server:                     192.168.1.10
DNS Search Domains:             darrylcauldwell.com
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Workload Network&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;vDS:                            DSwitch
Edge Cluster:                   edgeCluster
DNS Server:                     192.168.1.10
Pod CIDRs:                      10.244.0.0/21 (default)
Service CIDRS:                  10.96.0.0/24 (default)
Ingress CIDR:                   192.168.1.48/28
Egress CIDR:                    192.168.1.64/28
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Configure storage to use&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;Control Plane Node              vSAN Default Storage Policy
Ephemeral Disks                 vSAN Default Storage Policy
Image Cache                     vSAN Default Storage Policy
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;Review and confirm&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Once submitted we see in vCenter a resource pool named Namespaces and three virtual appliances named WcpAPIServerAgent each is allocated 2CPU and 8GB RAM.  An installation occurs on the three virtual appliances which,  these attempt to run before appliances deploy it is normal to see install failures during this phase.&lt;/p&gt;
&lt;p&gt;The progress messages available via UI aren&amp;rsquo;t superb its useful to SSH to VCSA and tail the log:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;tail -f /var/log/vmware/wcp/wcpsvc.logtail -f /var/log/vmware/wcp/wcpsvc.log
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;If all has goes to plan an hour and a half or so later and if all gone to plan.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://darrylcauldwell.com/images/tanzu-wcp.png&#34; alt=&#34;WCP Success&#34;&gt;&lt;/p&gt;
&lt;p&gt;From here I can begin to create and consume native Kubernetes namespaces and resources.&lt;/p&gt;

        
      </description>
    </item>
    
  </channel>
</rss>
